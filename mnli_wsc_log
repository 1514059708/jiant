09/07 05:53:48 PM: Git branch: master
09/07 05:53:48 PM: Git SHA: 883e7176a66d891d9d0238a6a08338d8f200af17
09/07 05:53:49 PM: Parsed args: 
{
  "batch_size": 24,
  "classifier": "log_reg",
  "do_target_task_training": 0,
  "dropout": 0.1,
  "dropout_embs": 0.1,
  "exp-name": "mnli_wsc",
  "input_module": "bert-base-cased",
  "local_log_path": "diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval/log.log",
  "lr": "1e-5",
  "lr_patience": 4,
  "max_epochs": 3,
  "max_vals": 10000,
  "min_lr": 0.0,
  "optimizer": "bert_adam",
  "patience": 20,
  "pretrain_tasks": "mnli,winograd-coreference",
  "pytorch_transformers_output_mode": "top",
  "random_seed": 42,
  "reload_tasks": 1,
  "remote_log_name": "my-experiment__mnli_wsc_diagnostic_eval",
  "run_dir": "diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval",
  "run_name": "mnli_wsc_diagnostic_eval",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "mnli,winograd-coreference,glue-diagnostic",
  "transfer_paradigm": "finetune",
  "write_preds": "val,test"
}
09/07 05:53:49 PM: Saved config to diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval/params.conf
09/07 05:53:49 PM: Using random seed 42
09/07 05:53:49 PM: Using GPU 0
09/07 05:53:49 PM: Loading tasks...
09/07 05:53:49 PM: Writing pre-preprocessed tasks to diagnostic_run_2/my-experiment/
09/07 05:53:49 PM: 	Creating task glue-diagnostic from scratch.
09/07 05:53:49 PM: 	Loading Tokenizer bert-base-cased
09/07 05:53:49 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/yp913/.cache/torch/pytorch_transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
09/07 05:53:50 PM: 	Finished loading diagnostic data.
09/07 05:53:50 PM: 	Finished creating score functions for diagnostic data.
09/07 05:53:50 PM: 	Task 'glue-diagnostic': |train|=1104 |val|=1104 |test|=1104
09/07 05:53:50 PM: 	Creating task mnli from scratch.
09/07 05:57:29 PM: 	Finished loading MNLI data.
09/07 05:57:34 PM: 	Task 'mnli': |train|=392702 |val|=19647 |test|=19643
09/07 05:57:34 PM: 	Creating task winograd-coreference from scratch.
09/07 05:57:38 PM: 	Task 'winograd-coreference': |train|=554 |val|=104 |test|=146
09/07 05:57:38 PM: 	Finished loading tasks: glue-diagnostic mnli winograd-coreference.
09/07 05:57:38 PM: Loading token dictionary from diagnostic_run_2/my-experiment/vocab.
09/07 05:57:38 PM: 	Loaded vocab from diagnostic_run_2/my-experiment/vocab
09/07 05:57:38 PM: 	Vocab namespace bert_cased: size 28998
09/07 05:57:38 PM: 	Vocab namespace tokens: size 25700
09/07 05:57:38 PM: 	Vocab namespace chars: size 139
09/07 05:57:38 PM: 	Finished building vocab.
09/07 05:57:38 PM: 	Task 'glue-diagnostic', split 'train': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/glue-diagnostic__train_data
09/07 05:57:38 PM: 	Task 'glue-diagnostic', split 'val': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/glue-diagnostic__val_data
09/07 05:57:38 PM: 	Task 'glue-diagnostic', split 'test': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/glue-diagnostic__test_data
09/07 05:57:38 PM: 	Task 'mnli', split 'train': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/mnli__train_data
09/07 05:57:38 PM: 	Task 'mnli', split 'val': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/mnli__val_data
09/07 05:57:38 PM: 	Task 'mnli', split 'test': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/mnli__test_data
09/07 05:57:38 PM: 	Task 'winograd-coreference', split 'train': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/winograd-coreference__train_data
09/07 05:57:38 PM: 	Task 'winograd-coreference', split 'val': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/winograd-coreference__val_data
09/07 05:57:38 PM: 	Task 'winograd-coreference', split 'test': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/winograd-coreference__test_data
09/07 05:57:38 PM: 	Finished indexing tasks
09/07 05:57:38 PM: 	Creating trimmed target-only version of glue-diagnostic train.
09/07 05:57:38 PM: 	Creating trimmed pretraining-only version of mnli train.
09/07 05:57:38 PM: 	Creating trimmed target-only version of mnli train.
09/07 05:57:38 PM: 	Creating trimmed pretraining-only version of winograd-coreference train.
09/07 05:57:38 PM: 	Creating trimmed target-only version of winograd-coreference train.
09/07 05:57:38 PM: 	  Training on mnli, winograd-coreference
09/07 05:57:38 PM: 	  Evaluating on mnli, winograd-coreference, glue-diagnostic
09/07 05:57:38 PM: 	Finished loading tasks in 229.086s
09/07 05:57:38 PM: 	 Tasks: ['glue-diagnostic', 'mnli', 'winograd-coreference']
09/07 05:57:38 PM: Building model...
09/07 05:57:38 PM: Using BERT model (bert-base-cased).
09/07 05:57:38 PM: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at diagnostic_run_2/my-experiment/pytorch_transformers_cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.d7a3af18ce3a2ab7c0f48f04dc8daff45ed9a3ed333b9e9a79d012a0dedf87a6
09/07 05:57:38 PM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

09/07 05:57:38 PM: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at diagnostic_run_2/my-experiment/pytorch_transformers_cache/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
09/07 05:57:41 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at diagnostic_run_2/my-experiment/pytorch_transformers_cache/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
09/07 05:57:41 PM: Initializing parameters
09/07 05:57:41 PM: Done initializing parameters; the following parameters are using their default initialization from their code
09/07 05:57:41 PM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/07 05:57:41 PM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/07 05:57:41 PM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/07 05:57:41 PM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/07 05:57:41 PM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/07 05:57:41 PM:    _text_field_embedder.model.pooler.dense.bias
09/07 05:57:41 PM:    _text_field_embedder.model.pooler.dense.weight
09/07 05:57:41 PM: 	Task 'mnli' params: {
  "cls_type": "log_reg",
  "d_hid": 512,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 1,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "mnli"
}
09/07 05:57:41 PM: 	Task 'glue-diagnostic' params: {
  "cls_type": "log_reg",
  "d_hid": 512,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 1,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "mnli"
}
09/07 05:57:41 PM: 	Task 'winograd-coreference' params: {
  "cls_type": "log_reg",
  "d_hid": 512,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 1,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "winograd-coreference"
}
09/07 05:57:41 PM: batch_first = True
09/07 05:57:41 PM: stateful = False
09/07 05:57:41 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/07 05:57:41 PM: CURRENTLY DEFINED PARAMETERS: 
09/07 05:57:41 PM: input_size = 1536
09/07 05:57:41 PM: hidden_size = 512
09/07 05:57:41 PM: num_layers = 1
09/07 05:57:41 PM: bidirectional = True
09/07 05:57:41 PM: batch_first = True
09/07 05:57:41 PM: Initializing parameters
09/07 05:57:41 PM: Done initializing parameters; the following parameters are using their default initialization from their code
09/07 05:57:41 PM:    _modeling_layer._module.bias_hh_l0
09/07 05:57:41 PM:    _modeling_layer._module.bias_hh_l0_reverse
09/07 05:57:41 PM:    _modeling_layer._module.bias_ih_l0
09/07 05:57:41 PM:    _modeling_layer._module.bias_ih_l0_reverse
09/07 05:57:41 PM:    _modeling_layer._module.weight_hh_l0
09/07 05:57:41 PM:    _modeling_layer._module.weight_hh_l0_reverse
09/07 05:57:41 PM:    _modeling_layer._module.weight_ih_l0
09/07 05:57:41 PM:    _modeling_layer._module.weight_ih_l0_reverse
09/07 05:57:41 PM: Name of the task is different than the classifier it should use
09/07 05:57:47 PM: Model specification:
09/07 05:57:47 PM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.1)
  )
  (mnli_mdl): SingleClassifier(
    (pooler): Pooler()
    (classifier): Classifier(
      (classifier): Linear(in_features=768, out_features=3, bias=True)
    )
  )
  (winograd-coreference_mdl): SpanClassifierModule(
    (projs): ModuleList(
      (0): Conv1d(768, 512, kernel_size=(1,), stride=(1,))
      (1): Conv1d(768, 512, kernel_size=(1,), stride=(1,))
    )
    (span_extractors): ModuleList(
      (0): SelfAttentiveSpanExtractor(
        (_global_attention): TimeDistributed(
          (_module): Linear(in_features=512, out_features=1, bias=True)
        )
      )
      (1): SelfAttentiveSpanExtractor(
        (_global_attention): TimeDistributed(
          (_module): Linear(in_features=512, out_features=1, bias=True)
        )
      )
    )
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
09/07 05:57:47 PM: Model parameters:
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Trainable parameter, count 22268928 with torch.Size([28996, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Trainable parameter, count 393216 with torch.Size([512, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:57:47 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:57:47 PM: 	mnli_mdl.classifier.classifier.weight: Trainable parameter, count 2304 with torch.Size([3, 768])
09/07 05:57:47 PM: 	mnli_mdl.classifier.classifier.bias: Trainable parameter, count 3 with torch.Size([3])
09/07 05:57:47 PM: 	winograd-coreference_mdl.projs.0.weight: Trainable parameter, count 393216 with torch.Size([512, 768, 1])
09/07 05:57:47 PM: 	winograd-coreference_mdl.projs.0.bias: Trainable parameter, count 512 with torch.Size([512])
09/07 05:57:47 PM: 	winograd-coreference_mdl.projs.1.weight: Trainable parameter, count 393216 with torch.Size([512, 768, 1])
09/07 05:57:47 PM: 	winograd-coreference_mdl.projs.1.bias: Trainable parameter, count 512 with torch.Size([512])
09/07 05:57:47 PM: 	winograd-coreference_mdl.span_extractors.0._global_attention._module.weight: Trainable parameter, count 512 with torch.Size([1, 512])
09/07 05:57:47 PM: 	winograd-coreference_mdl.span_extractors.0._global_attention._module.bias: Trainable parameter, count 1 with torch.Size([1])
09/07 05:57:47 PM: 	winograd-coreference_mdl.span_extractors.1._global_attention._module.weight: Trainable parameter, count 512 with torch.Size([1, 512])
09/07 05:57:47 PM: 	winograd-coreference_mdl.span_extractors.1._global_attention._module.bias: Trainable parameter, count 1 with torch.Size([1])
09/07 05:57:47 PM: 	winograd-coreference_mdl.classifier.classifier.weight: Trainable parameter, count 2048 with torch.Size([2, 1024])
09/07 05:57:47 PM: 	winograd-coreference_mdl.classifier.classifier.bias: Trainable parameter, count 2 with torch.Size([2])
09/07 05:57:47 PM: Total number of parameters: 109103111 (1.09103e+08)
09/07 05:57:47 PM: Number of trainable parameters: 109103111 (1.09103e+08)
09/07 05:57:47 PM: Finished building model in 8.585s
09/07 05:57:47 PM: Will run the following steps for this experiment:
Training model on tasks: mnli,winograd-coreference 
Evaluating model on tasks: mnli,winograd-coreference,glue-diagnostic 

09/07 05:57:47 PM: Training...
09/07 05:57:47 PM: patience = 20
09/07 05:57:47 PM: val_interval = 1000
09/07 05:57:47 PM: max_vals = 10000
09/07 05:57:47 PM: cuda_device = 0
09/07 05:57:47 PM: grad_norm = 5.0
09/07 05:57:47 PM: grad_clipping = None
09/07 05:57:47 PM: lr_decay = 0.99
09/07 05:57:47 PM: min_lr = 1e-07
09/07 05:57:47 PM: keep_all_checkpoints = 0
09/07 05:57:47 PM: val_data_limit = 5000
09/07 05:57:47 PM: max_epochs = 3
09/07 05:57:47 PM: dec_val_scale = 250
09/07 05:57:47 PM: training_data_fraction = 1
09/07 05:57:47 PM: type = bert_adam
09/07 05:57:47 PM: parameter_groups = None
09/07 05:57:47 PM: Number of trainable parameters: 109103111
09/07 05:57:47 PM: infer_type_and_cast = True
09/07 05:57:47 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/07 05:57:47 PM: CURRENTLY DEFINED PARAMETERS: 
09/07 05:57:47 PM: lr = 1e-5
09/07 05:57:47 PM: t_total = 50000
09/07 05:57:47 PM: warmup = 0.1
09/07 05:57:47 PM: type = reduce_on_plateau
09/07 05:57:47 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/07 05:57:47 PM: CURRENTLY DEFINED PARAMETERS: 
09/07 05:57:47 PM: mode = max
09/07 05:57:47 PM: factor = 0.5
09/07 05:57:47 PM: patience = 4
09/07 05:57:47 PM: threshold = 0.0001
09/07 05:57:47 PM: threshold_mode = abs
09/07 05:57:47 PM: verbose = True
09/07 05:57:47 PM: type = bert_adam
09/07 05:57:47 PM: parameter_groups = None
09/07 05:57:47 PM: Number of trainable parameters: 109103111
09/07 05:57:47 PM: infer_type_and_cast = True
09/07 05:57:47 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/07 05:57:47 PM: CURRENTLY DEFINED PARAMETERS: 
09/07 05:57:47 PM: lr = 1e-5
09/07 05:57:47 PM: t_total = 1000
09/07 05:57:47 PM: warmup = 0.1
09/07 05:57:47 PM: type = reduce_on_plateau
09/07 05:57:47 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/07 05:57:47 PM: CURRENTLY DEFINED PARAMETERS: 
09/07 05:57:47 PM: mode = max
09/07 05:57:47 PM: factor = 0.5
09/07 05:57:47 PM: patience = 4
09/07 05:57:47 PM: threshold = 0.0001
09/07 05:57:47 PM: threshold_mode = abs
09/07 05:57:47 PM: verbose = True
09/07 05:57:47 PM: type = bert_adam
09/07 05:57:47 PM: parameter_groups = None
09/07 05:57:47 PM: Number of trainable parameters: 109103111
09/07 05:57:47 PM: infer_type_and_cast = True
09/07 05:57:47 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/07 05:57:47 PM: CURRENTLY DEFINED PARAMETERS: 
09/07 05:57:47 PM: lr = 1e-5
09/07 05:57:47 PM: t_total = 50000
09/07 05:57:47 PM: warmup = 0.1
09/07 05:57:47 PM: type = reduce_on_plateau
09/07 05:57:47 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/07 05:57:47 PM: CURRENTLY DEFINED PARAMETERS: 
09/07 05:57:47 PM: mode = max
09/07 05:57:47 PM: factor = 0.5
09/07 05:57:47 PM: patience = 4
09/07 05:57:47 PM: threshold = 0.0001
09/07 05:57:47 PM: threshold_mode = abs
09/07 05:57:47 PM: verbose = True
09/07 05:57:47 PM: load_model=1 but there is not checkpoint.                         Starting training without restoring from a checkpoint.
09/07 05:57:47 PM: Training examples per task, before any subsampling: {'mnli': 392702, 'winograd-coreference': 554}
09/07 05:57:47 PM: Using weighting method: proportional, with normalized sample weights [0.9986 0.0014] 
09/07 05:57:47 PM: Beginning training with stopping criteria based on metric: macro_avg
09/07 05:57:57 PM: Update 31: task mnli, batch 31 (31): accuracy: 0.3492, mnli_loss: 1.1110
09/07 05:58:07 PM: Update 74: task mnli, batch 74 (74): accuracy: 0.3286, mnli_loss: 1.1150
09/07 05:58:17 PM: Update 118: task mnli, batch 118 (118): accuracy: 0.3311, mnli_loss: 1.1113
09/07 05:58:27 PM: Update 160: task mnli, batch 160 (160): accuracy: 0.3275, mnli_loss: 1.1120
09/07 05:58:37 PM: Update 199: task winograd-coreference, batch 1 (1): f1: 0.0000, acc: 0.5000, winograd-coreference_loss: 0.7031
09/07 05:58:37 PM: Update 201: task mnli, batch 200 (200): accuracy: 0.3274, mnli_loss: 1.1112
09/07 05:58:48 PM: Update 242: task mnli, batch 241 (241): accuracy: 0.3298, mnli_loss: 1.1095
09/07 05:58:58 PM: Update 283: task mnli, batch 282 (282): accuracy: 0.3373, mnli_loss: 1.1062
09/07 05:59:08 PM: Update 325: task mnli, batch 324 (324): accuracy: 0.3418, mnli_loss: 1.1045
09/07 05:59:18 PM: Update 367: task mnli, batch 366 (366): accuracy: 0.3418, mnli_loss: 1.1042
09/07 05:59:28 PM: Update 408: task mnli, batch 407 (407): accuracy: 0.3455, mnli_loss: 1.1031
09/07 05:59:38 PM: Update 438: task mnli, batch 437 (437): accuracy: 0.3485, mnli_loss: 1.1017
09/07 05:59:48 PM: Update 478: task mnli, batch 477 (477): accuracy: 0.3523, mnli_loss: 1.1004
09/07 05:59:58 PM: Update 520: task mnli, batch 519 (519): accuracy: 0.3556, mnli_loss: 1.0985
09/07 05:59:58 PM: Update 521: task winograd-coreference, batch 2 (2): f1: 0.2500, acc: 0.5385, winograd-coreference_loss: 0.6915
09/07 06:00:08 PM: Update 562: task mnli, batch 560 (560): accuracy: 0.3617, mnli_loss: 1.0962
09/07 06:00:18 PM: Update 605: task mnli, batch 603 (603): accuracy: 0.3666, mnli_loss: 1.0940
09/07 06:00:29 PM: Update 647: task mnli, batch 645 (645): accuracy: 0.3718, mnli_loss: 1.0912
09/07 06:00:39 PM: Update 688: task mnli, batch 686 (686): accuracy: 0.3766, mnli_loss: 1.0885
09/07 06:00:48 PM: Update 726: task winograd-coreference, batch 3 (3): f1: 0.1818, acc: 0.4600, winograd-coreference_loss: 0.6968
09/07 06:00:49 PM: Update 729: task mnli, batch 726 (726): accuracy: 0.3825, mnli_loss: 1.0842
09/07 06:00:59 PM: Update 769: task mnli, batch 766 (766): accuracy: 0.3895, mnli_loss: 1.0792
09/07 06:01:08 PM: Update 804: task winograd-coreference, batch 4 (4): f1: 0.3448, acc: 0.4865, winograd-coreference_loss: 0.6982
09/07 06:01:09 PM: Update 807: task mnli, batch 803 (803): accuracy: 0.3969, mnli_loss: 1.0737
09/07 06:01:19 PM: Update 839: task mnli, batch 835 (835): accuracy: 0.4031, mnli_loss: 1.0686
09/07 06:01:29 PM: Update 880: task mnli, batch 876 (876): accuracy: 0.4091, mnli_loss: 1.0640
09/07 06:01:39 PM: Update 921: task mnli, batch 917 (917): accuracy: 0.4169, mnli_loss: 1.0575
09/07 06:01:50 PM: Update 961: task mnli, batch 957 (957): accuracy: 0.4243, mnli_loss: 1.0504
09/07 06:01:59 PM: ***** Step 1000 / Validation 1 *****
09/07 06:01:59 PM: mnli: trained on 996 batches, 0.061 epochs
09/07 06:01:59 PM: winograd-coreference: trained on 4 batches, 0.167 epochs
09/07 06:01:59 PM: Validating...
09/07 06:02:00 PM: Evaluate: task mnli, batch 6 (209): accuracy: 0.6042, mnli_loss: 0.9181
09/07 06:02:10 PM: Evaluate: task mnli, batch 155 (209): accuracy: 0.5960, mnli_loss: 0.8798
09/07 06:02:13 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.3529, acc: 0.5417, winograd-coreference_loss: 0.6720
09/07 06:02:14 PM: Best result seen so far for mnli.
09/07 06:02:14 PM: Best result seen so far for winograd-coreference.
09/07 06:02:14 PM: Best result seen so far for micro.
09/07 06:02:14 PM: Best result seen so far for macro.
09/07 06:02:14 PM: Updating LR scheduler:
09/07 06:02:14 PM: 	Best result seen so far for macro_avg: 0.557
09/07 06:02:14 PM: 	# validation passes without improvement: 0
09/07 06:02:14 PM: mnli_loss: training: 1.043143 validation: 0.886336
09/07 06:02:14 PM: winograd-coreference_loss: training: 0.698185 validation: 0.695108
09/07 06:02:14 PM: macro_avg: validation: 0.556815
09/07 06:02:14 PM: micro_avg: validation: 0.592868
09/07 06:02:14 PM: mnli_accuracy: training: 0.431198 validation: 0.594400
09/07 06:02:14 PM: winograd-coreference_f1: training: 0.344828 validation: 0.390244
09/07 06:02:14 PM: winograd-coreference_acc: training: 0.486486 validation: 0.519231
09/07 06:02:14 PM: Global learning rate: 1e-05
09/07 06:02:14 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 06:02:20 PM: Update 1019: task mnli, batch 19 (1015): accuracy: 0.6250, mnli_loss: 0.8366
09/07 06:02:30 PM: Update 1059: task mnli, batch 59 (1055): accuracy: 0.6010, mnli_loss: 0.8567
09/07 06:02:40 PM: Update 1099: task mnli, batch 99 (1095): accuracy: 0.6061, mnli_loss: 0.8498
09/07 06:02:50 PM: Update 1139: task mnli, batch 139 (1135): accuracy: 0.6037, mnli_loss: 0.8577
09/07 06:03:00 PM: Update 1181: task mnli, batch 181 (1177): accuracy: 0.6054, mnli_loss: 0.8599
09/07 06:03:10 PM: Update 1222: task mnli, batch 222 (1218): accuracy: 0.6059, mnli_loss: 0.8546
09/07 06:03:21 PM: Update 1256: task mnli, batch 256 (1252): accuracy: 0.6059, mnli_loss: 0.8535
09/07 06:03:31 PM: Update 1296: task mnli, batch 296 (1292): accuracy: 0.6081, mnli_loss: 0.8532
09/07 06:03:41 PM: Update 1337: task mnli, batch 337 (1333): accuracy: 0.6083, mnli_loss: 0.8532
09/07 06:03:51 PM: Update 1376: task mnli, batch 376 (1372): accuracy: 0.6108, mnli_loss: 0.8493
09/07 06:04:01 PM: Update 1417: task mnli, batch 417 (1413): accuracy: 0.6126, mnli_loss: 0.8468
09/07 06:04:11 PM: Update 1458: task mnli, batch 458 (1454): accuracy: 0.6148, mnli_loss: 0.8440
09/07 06:04:22 PM: Update 1498: task mnli, batch 498 (1494): accuracy: 0.6171, mnli_loss: 0.8412
09/07 06:04:32 PM: Update 1539: task mnli, batch 539 (1535): accuracy: 0.6203, mnli_loss: 0.8374
09/07 06:04:42 PM: Update 1581: task mnli, batch 581 (1577): accuracy: 0.6231, mnli_loss: 0.8331
09/07 06:04:52 PM: Update 1622: task mnli, batch 622 (1618): accuracy: 0.6273, mnli_loss: 0.8273
09/07 06:05:02 PM: Update 1662: task mnli, batch 662 (1658): accuracy: 0.6289, mnli_loss: 0.8258
09/07 06:05:13 PM: Update 1694: task mnli, batch 694 (1690): accuracy: 0.6296, mnli_loss: 0.8239
09/07 06:05:23 PM: Update 1735: task mnli, batch 735 (1731): accuracy: 0.6318, mnli_loss: 0.8211
09/07 06:05:33 PM: Update 1777: task mnli, batch 777 (1773): accuracy: 0.6326, mnli_loss: 0.8188
09/07 06:05:38 PM: Update 1797: task winograd-coreference, batch 1 (5): f1: 0.1429, acc: 0.5000, winograd-coreference_loss: 0.6999
09/07 06:05:43 PM: Update 1818: task mnli, batch 817 (1813): accuracy: 0.6344, mnli_loss: 0.8173
09/07 06:05:53 PM: Update 1857: task mnli, batch 856 (1852): accuracy: 0.6350, mnli_loss: 0.8157
09/07 06:06:03 PM: Update 1896: task mnli, batch 895 (1891): accuracy: 0.6356, mnli_loss: 0.8143
09/07 06:06:13 PM: Update 1935: task mnli, batch 934 (1930): accuracy: 0.6367, mnli_loss: 0.8122
09/07 06:06:23 PM: Update 1976: task mnli, batch 975 (1971): accuracy: 0.6381, mnli_loss: 0.8100
09/07 06:06:29 PM: ***** Step 2000 / Validation 2 *****
09/07 06:06:29 PM: mnli: trained on 999 batches, 0.061 epochs
09/07 06:06:29 PM: winograd-coreference: trained on 1 batches, 0.042 epochs
09/07 06:06:29 PM: Validating...
09/07 06:06:33 PM: Evaluate: task mnli, batch 59 (209): accuracy: 0.6942, mnli_loss: 0.7177
09/07 06:06:43 PM: Evaluate: task mnli, batch 209 (209): accuracy: 0.6896, mnli_loss: 0.7248
09/07 06:06:43 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.3750, acc: 0.5833, winograd-coreference_loss: 0.6675
09/07 06:06:44 PM: Best result seen so far for mnli.
09/07 06:06:44 PM: Best result seen so far for winograd-coreference.
09/07 06:06:44 PM: Best result seen so far for micro.
09/07 06:06:44 PM: Best result seen so far for macro.
09/07 06:06:44 PM: Updating LR scheduler:
09/07 06:06:44 PM: 	Best result seen so far for macro_avg: 0.619
09/07 06:06:44 PM: 	# validation passes without improvement: 0
09/07 06:06:44 PM: mnli_loss: training: 0.809371 validation: 0.724830
09/07 06:06:44 PM: winograd-coreference_loss: training: 0.699911 validation: 0.693342
09/07 06:06:44 PM: macro_avg: validation: 0.618838
09/07 06:06:44 PM: micro_avg: validation: 0.686716
09/07 06:06:44 PM: mnli_accuracy: training: 0.638648 validation: 0.689600
09/07 06:06:44 PM: winograd-coreference_f1: training: 0.142857 validation: 0.373333
09/07 06:06:44 PM: winograd-coreference_acc: training: 0.500000 validation: 0.548077
09/07 06:06:44 PM: Global learning rate: 1e-05
09/07 06:06:44 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 06:06:53 PM: Update 2035: task mnli, batch 35 (2030): accuracy: 0.7048, mnli_loss: 0.7020
09/07 06:07:04 PM: Update 2078: task mnli, batch 78 (2073): accuracy: 0.6864, mnli_loss: 0.7398
09/07 06:07:14 PM: Update 2108: task mnli, batch 108 (2103): accuracy: 0.6834, mnli_loss: 0.7408
09/07 06:07:24 PM: Update 2149: task mnli, batch 149 (2144): accuracy: 0.6797, mnli_loss: 0.7492
09/07 06:07:34 PM: Update 2189: task mnli, batch 189 (2184): accuracy: 0.6822, mnli_loss: 0.7447
09/07 06:07:44 PM: Update 2229: task mnli, batch 229 (2224): accuracy: 0.6813, mnli_loss: 0.7457
09/07 06:07:54 PM: Update 2270: task mnli, batch 270 (2265): accuracy: 0.6853, mnli_loss: 0.7385
09/07 06:08:04 PM: Update 2311: task mnli, batch 311 (2306): accuracy: 0.6872, mnli_loss: 0.7354
09/07 06:08:15 PM: Update 2352: task mnli, batch 352 (2347): accuracy: 0.6891, mnli_loss: 0.7306
09/07 06:08:25 PM: Update 2394: task mnli, batch 394 (2389): accuracy: 0.6889, mnli_loss: 0.7312
09/07 06:08:35 PM: Update 2435: task mnli, batch 435 (2430): accuracy: 0.6860, mnli_loss: 0.7350
09/07 06:08:45 PM: Update 2479: task mnli, batch 479 (2474): accuracy: 0.6873, mnli_loss: 0.7347
09/07 06:08:55 PM: Update 2509: task mnli, batch 509 (2504): accuracy: 0.6913, mnli_loss: 0.7290
09/07 06:09:05 PM: Update 2552: task mnli, batch 552 (2547): accuracy: 0.6919, mnli_loss: 0.7278
09/07 06:09:15 PM: Update 2594: task mnli, batch 594 (2589): accuracy: 0.6930, mnli_loss: 0.7260
09/07 06:09:25 PM: Update 2635: task mnli, batch 635 (2630): accuracy: 0.6925, mnli_loss: 0.7268
09/07 06:09:36 PM: Update 2675: task mnli, batch 675 (2670): accuracy: 0.6931, mnli_loss: 0.7259
09/07 06:09:46 PM: Update 2715: task mnli, batch 715 (2710): accuracy: 0.6939, mnli_loss: 0.7246
09/07 06:09:56 PM: Update 2754: task mnli, batch 754 (2749): accuracy: 0.6935, mnli_loss: 0.7249
09/07 06:10:06 PM: Update 2795: task mnli, batch 795 (2790): accuracy: 0.6932, mnli_loss: 0.7247
09/07 06:10:16 PM: Update 2836: task mnli, batch 836 (2831): accuracy: 0.6929, mnli_loss: 0.7243
09/07 06:10:26 PM: Update 2877: task mnli, batch 877 (2872): accuracy: 0.6937, mnli_loss: 0.7216
09/07 06:10:36 PM: Update 2918: task mnli, batch 918 (2913): accuracy: 0.6933, mnli_loss: 0.7220
09/07 06:10:47 PM: Update 2950: task mnli, batch 950 (2945): accuracy: 0.6940, mnli_loss: 0.7213
09/07 06:10:57 PM: Update 2992: task mnli, batch 992 (2987): accuracy: 0.6948, mnli_loss: 0.7199
09/07 06:10:59 PM: ***** Step 3000 / Validation 3 *****
09/07 06:10:59 PM: mnli: trained on 1000 batches, 0.061 epochs
09/07 06:10:59 PM: winograd-coreference: trained on 0 batches, 0.000 epochs
09/07 06:10:59 PM: Validating...
09/07 06:11:07 PM: Evaluate: task mnli, batch 126 (209): accuracy: 0.7212, mnli_loss: 0.6699
09/07 06:11:13 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.4211, acc: 0.5417, winograd-coreference_loss: 0.6723
09/07 06:11:13 PM: Best result seen so far for mnli.
09/07 06:11:13 PM: Best result seen so far for micro.
09/07 06:11:13 PM: Updating LR scheduler:
09/07 06:11:13 PM: 	Best result seen so far for macro_avg: 0.619
09/07 06:11:13 PM: 	# validation passes without improvement: 1
09/07 06:11:13 PM: mnli_loss: training: 0.720028 validation: 0.664146
09/07 06:11:13 PM: winograd-coreference_loss: training: 0.000000 validation: 0.693163
09/07 06:11:13 PM: macro_avg: validation: 0.616908
09/07 06:11:13 PM: micro_avg: validation: 0.719828
09/07 06:11:13 PM: mnli_accuracy: training: 0.694695 validation: 0.724200
09/07 06:11:13 PM: winograd-coreference_f1: validation: 0.400000
09/07 06:11:13 PM: winograd-coreference_acc: validation: 0.509615
09/07 06:11:13 PM: Global learning rate: 1e-05
09/07 06:11:13 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 06:11:17 PM: Update 3014: task mnli, batch 14 (3009): accuracy: 0.7321, mnli_loss: 0.6526
09/07 06:11:27 PM: Update 3056: task mnli, batch 56 (3051): accuracy: 0.7299, mnli_loss: 0.6502
09/07 06:11:37 PM: Update 3096: task mnli, batch 96 (3091): accuracy: 0.7161, mnli_loss: 0.6775
09/07 06:11:47 PM: Update 3136: task mnli, batch 136 (3131): accuracy: 0.7105, mnli_loss: 0.6817
09/07 06:11:58 PM: Update 3178: task mnli, batch 178 (3173): accuracy: 0.7079, mnli_loss: 0.6822
09/07 06:12:08 PM: Update 3217: task mnli, batch 217 (3212): accuracy: 0.7043, mnli_loss: 0.6892
09/07 06:12:18 PM: Update 3257: task mnli, batch 257 (3252): accuracy: 0.7033, mnli_loss: 0.6925
09/07 06:12:28 PM: Update 3299: task mnli, batch 299 (3294): accuracy: 0.7061, mnli_loss: 0.6902
09/07 06:12:38 PM: Update 3341: task mnli, batch 341 (3336): accuracy: 0.7058, mnli_loss: 0.6901
09/07 06:12:48 PM: Update 3374: task mnli, batch 374 (3369): accuracy: 0.7048, mnli_loss: 0.6932
09/07 06:12:58 PM: Update 3415: task mnli, batch 415 (3410): accuracy: 0.7068, mnli_loss: 0.6913
09/07 06:13:08 PM: Update 3457: task mnli, batch 457 (3452): accuracy: 0.7092, mnli_loss: 0.6867
09/07 06:13:18 PM: Update 3499: task mnli, batch 499 (3494): accuracy: 0.7094, mnli_loss: 0.6850
09/07 06:13:29 PM: Update 3538: task mnli, batch 538 (3533): accuracy: 0.7111, mnli_loss: 0.6847
09/07 06:13:34 PM: Update 3559: task winograd-coreference, batch 1 (6): f1: 0.6000, acc: 0.6667, winograd-coreference_loss: 0.6822
09/07 06:13:39 PM: Update 3578: task mnli, batch 577 (3572): accuracy: 0.7129, mnli_loss: 0.6827
09/07 06:13:49 PM: Update 3619: task mnli, batch 618 (3613): accuracy: 0.7141, mnli_loss: 0.6809
09/07 06:13:59 PM: Update 3660: task mnli, batch 659 (3654): accuracy: 0.7132, mnli_loss: 0.6814
09/07 06:14:09 PM: Update 3701: task mnli, batch 700 (3695): accuracy: 0.7140, mnli_loss: 0.6797
09/07 06:14:19 PM: Update 3741: task mnli, batch 740 (3735): accuracy: 0.7155, mnli_loss: 0.6780
09/07 06:14:29 PM: Update 3769: task mnli, batch 768 (3763): accuracy: 0.7163, mnli_loss: 0.6774
09/07 06:14:39 PM: Update 3809: task mnli, batch 808 (3803): accuracy: 0.7178, mnli_loss: 0.6745
09/07 06:14:49 PM: Update 3849: task mnli, batch 848 (3843): accuracy: 0.7178, mnli_loss: 0.6747
09/07 06:14:59 PM: Update 3890: task mnli, batch 889 (3884): accuracy: 0.7194, mnli_loss: 0.6715
09/07 06:15:10 PM: Update 3930: task mnli, batch 929 (3924): accuracy: 0.7203, mnli_loss: 0.6697
09/07 06:15:10 PM: Update 3932: task winograd-coreference, batch 2 (7): f1: 0.5000, acc: 0.5000, winograd-coreference_loss: 0.7144
09/07 06:15:20 PM: Update 3971: task mnli, batch 969 (3964): accuracy: 0.7204, mnli_loss: 0.6696
09/07 06:15:27 PM: ***** Step 4000 / Validation 4 *****
09/07 06:15:27 PM: mnli: trained on 998 batches, 0.061 epochs
09/07 06:15:27 PM: winograd-coreference: trained on 2 batches, 0.083 epochs
09/07 06:15:27 PM: Validating...
09/07 06:15:30 PM: Evaluate: task mnli, batch 45 (209): accuracy: 0.7454, mnli_loss: 0.6230
09/07 06:15:40 PM: Evaluate: task mnli, batch 194 (209): accuracy: 0.7414, mnli_loss: 0.6232
09/07 06:15:41 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.4000, acc: 0.5000, winograd-coreference_loss: 0.6989
09/07 06:15:41 PM: Best result seen so far for mnli.
09/07 06:15:41 PM: Best result seen so far for micro.
09/07 06:15:41 PM: Best result seen so far for macro.
09/07 06:15:41 PM: Updating LR scheduler:
09/07 06:15:41 PM: 	Best result seen so far for macro_avg: 0.639
09/07 06:15:41 PM: 	# validation passes without improvement: 0
09/07 06:15:41 PM: mnli_loss: training: 0.669797 validation: 0.625308
09/07 06:15:41 PM: winograd-coreference_loss: training: 0.714419 validation: 0.692304
09/07 06:15:41 PM: macro_avg: validation: 0.639331
09/07 06:15:41 PM: micro_avg: validation: 0.736089
09/07 06:15:41 PM: mnli_accuracy: training: 0.720546 validation: 0.740200
09/07 06:15:41 PM: winograd-coreference_f1: training: 0.500000 validation: 0.478261
09/07 06:15:41 PM: winograd-coreference_acc: training: 0.500000 validation: 0.538462
09/07 06:15:41 PM: Global learning rate: 1e-05
09/07 06:15:41 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 06:15:50 PM: Update 4032: task mnli, batch 32 (4025): accuracy: 0.7344, mnli_loss: 0.6610
09/07 06:16:00 PM: Update 4072: task mnli, batch 72 (4065): accuracy: 0.7454, mnli_loss: 0.6340
09/07 06:16:10 PM: Update 4114: task mnli, batch 114 (4107): accuracy: 0.7463, mnli_loss: 0.6389
09/07 06:16:20 PM: Update 4156: task mnli, batch 156 (4149): accuracy: 0.7372, mnli_loss: 0.6493
09/07 06:16:30 PM: Update 4189: task mnli, batch 189 (4182): accuracy: 0.7361, mnli_loss: 0.6428
09/07 06:16:40 PM: Update 4230: task mnli, batch 230 (4223): accuracy: 0.7364, mnli_loss: 0.6396
09/07 06:16:51 PM: Update 4270: task mnli, batch 270 (4263): accuracy: 0.7324, mnli_loss: 0.6440
09/07 06:17:01 PM: Update 4312: task mnli, batch 312 (4305): accuracy: 0.7322, mnli_loss: 0.6466
09/07 06:17:11 PM: Update 4352: task mnli, batch 352 (4345): accuracy: 0.7306, mnli_loss: 0.6534
09/07 06:17:21 PM: Update 4391: task mnli, batch 391 (4384): accuracy: 0.7311, mnli_loss: 0.6528
09/07 06:17:31 PM: Update 4433: task mnli, batch 433 (4426): accuracy: 0.7294, mnli_loss: 0.6547
09/07 06:17:42 PM: Update 4475: task mnli, batch 475 (4468): accuracy: 0.7314, mnli_loss: 0.6507
09/07 06:17:52 PM: Update 4517: task mnli, batch 517 (4510): accuracy: 0.7320, mnli_loss: 0.6486
09/07 06:18:02 PM: Update 4557: task mnli, batch 557 (4550): accuracy: 0.7316, mnli_loss: 0.6490
09/07 06:18:13 PM: Update 4595: task mnli, batch 595 (4588): accuracy: 0.7330, mnli_loss: 0.6482
09/07 06:18:23 PM: Update 4635: task mnli, batch 635 (4628): accuracy: 0.7352, mnli_loss: 0.6450
09/07 06:18:33 PM: Update 4677: task mnli, batch 677 (4670): accuracy: 0.7350, mnli_loss: 0.6458
09/07 06:18:43 PM: Update 4717: task mnli, batch 717 (4710): accuracy: 0.7350, mnli_loss: 0.6457
09/07 06:18:53 PM: Update 4758: task mnli, batch 758 (4751): accuracy: 0.7350, mnli_loss: 0.6462
09/07 06:19:04 PM: Update 4800: task mnli, batch 800 (4793): accuracy: 0.7353, mnli_loss: 0.6448
09/07 06:19:14 PM: Update 4841: task mnli, batch 841 (4834): accuracy: 0.7360, mnli_loss: 0.6441
09/07 06:19:24 PM: Update 4880: task mnli, batch 880 (4873): accuracy: 0.7347, mnli_loss: 0.6443
09/07 06:19:34 PM: Update 4922: task mnli, batch 922 (4915): accuracy: 0.7349, mnli_loss: 0.6434
09/07 06:19:44 PM: Update 4964: task mnli, batch 964 (4957): accuracy: 0.7349, mnli_loss: 0.6427
09/07 06:19:53 PM: ***** Step 5000 / Validation 5 *****
09/07 06:19:53 PM: mnli: trained on 1000 batches, 0.061 epochs
09/07 06:19:53 PM: winograd-coreference: trained on 0 batches, 0.000 epochs
09/07 06:19:53 PM: Validating...
09/07 06:19:54 PM: Evaluate: task mnli, batch 18 (209): accuracy: 0.7454, mnli_loss: 0.5883
09/07 06:20:04 PM: Evaluate: task mnli, batch 168 (209): accuracy: 0.7607, mnli_loss: 0.5853
09/07 06:20:07 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.3158, acc: 0.4583, winograd-coreference_loss: 0.6974
09/07 06:20:07 PM: Best result seen so far for mnli.
09/07 06:20:07 PM: Best result seen so far for micro.
09/07 06:20:07 PM: Best result seen so far for macro.
09/07 06:20:07 PM: Updating LR scheduler:
09/07 06:20:07 PM: 	Best result seen so far for macro_avg: 0.649
09/07 06:20:07 PM: 	# validation passes without improvement: 0
09/07 06:20:07 PM: mnli_loss: training: 0.643331 validation: 0.591257
09/07 06:20:07 PM: winograd-coreference_loss: training: 0.000000 validation: 0.691729
09/07 06:20:07 PM: macro_avg: validation: 0.648531
09/07 06:20:07 PM: micro_avg: validation: 0.754114
09/07 06:20:07 PM: mnli_accuracy: training: 0.734448 validation: 0.758600
09/07 06:20:07 PM: winograd-coreference_f1: validation: 0.441860
09/07 06:20:07 PM: winograd-coreference_acc: validation: 0.538462
09/07 06:20:07 PM: Global learning rate: 1e-05
09/07 06:20:07 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 06:20:14 PM: Update 5015: task mnli, batch 15 (5008): accuracy: 0.6733, mnli_loss: 0.7548
09/07 06:20:24 PM: Update 5056: task mnli, batch 56 (5049): accuracy: 0.7268, mnli_loss: 0.6572
09/07 06:20:35 PM: Update 5095: task mnli, batch 95 (5088): accuracy: 0.7148, mnli_loss: 0.6719
09/07 06:20:45 PM: Update 5135: task mnli, batch 135 (5128): accuracy: 0.7209, mnli_loss: 0.6646
09/07 06:20:55 PM: Update 5176: task mnli, batch 176 (5169): accuracy: 0.7211, mnli_loss: 0.6629
09/07 06:21:05 PM: Update 5216: task mnli, batch 216 (5209): accuracy: 0.7206, mnli_loss: 0.6561
09/07 06:21:15 PM: Update 5257: task mnli, batch 257 (5250): accuracy: 0.7245, mnli_loss: 0.6477
09/07 06:21:25 PM: Update 5298: task mnli, batch 298 (5291): accuracy: 0.7311, mnli_loss: 0.6403
09/07 06:21:35 PM: Update 5340: task mnli, batch 340 (5333): accuracy: 0.7332, mnli_loss: 0.6372
09/07 06:21:45 PM: Update 5382: task mnli, batch 382 (5375): accuracy: 0.7354, mnli_loss: 0.6339
09/07 06:21:56 PM: Update 5424: task mnli, batch 424 (5417): accuracy: 0.7330, mnli_loss: 0.6361
09/07 06:22:06 PM: Update 5455: task mnli, batch 455 (5448): accuracy: 0.7328, mnli_loss: 0.6375
09/07 06:22:16 PM: Update 5498: task mnli, batch 498 (5491): accuracy: 0.7332, mnli_loss: 0.6354
09/07 06:22:26 PM: Update 5537: task mnli, batch 537 (5530): accuracy: 0.7338, mnli_loss: 0.6356
09/07 06:22:36 PM: Update 5576: task mnli, batch 576 (5569): accuracy: 0.7356, mnli_loss: 0.6328
09/07 06:22:46 PM: Update 5617: task mnli, batch 617 (5610): accuracy: 0.7376, mnli_loss: 0.6291
09/07 06:22:56 PM: Update 5660: task mnli, batch 660 (5653): accuracy: 0.7377, mnli_loss: 0.6297
09/07 06:23:06 PM: Update 5701: task mnli, batch 701 (5694): accuracy: 0.7391, mnli_loss: 0.6270
09/07 06:23:16 PM: Update 5742: task mnli, batch 742 (5735): accuracy: 0.7395, mnli_loss: 0.6262
09/07 06:23:26 PM: Update 5784: task mnli, batch 784 (5777): accuracy: 0.7406, mnli_loss: 0.6245
09/07 06:23:36 PM: Update 5824: task mnli, batch 824 (5817): accuracy: 0.7407, mnli_loss: 0.6240
09/07 06:23:47 PM: Update 5855: task mnli, batch 855 (5848): accuracy: 0.7400, mnli_loss: 0.6252
09/07 06:23:57 PM: Update 5898: task mnli, batch 898 (5891): accuracy: 0.7413, mnli_loss: 0.6223
09/07 06:24:07 PM: Update 5939: task mnli, batch 939 (5932): accuracy: 0.7421, mnli_loss: 0.6219
09/07 06:24:17 PM: Update 5980: task mnli, batch 980 (5973): accuracy: 0.7428, mnli_loss: 0.6219
09/07 06:24:22 PM: ***** Step 6000 / Validation 6 *****
09/07 06:24:22 PM: mnli: trained on 1000 batches, 0.061 epochs
09/07 06:24:22 PM: winograd-coreference: trained on 0 batches, 0.000 epochs
09/07 06:24:22 PM: Validating...
09/07 06:24:27 PM: Evaluate: task mnli, batch 74 (209): accuracy: 0.7770, mnli_loss: 0.5481
09/07 06:24:37 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.2353, acc: 0.4583, winograd-coreference_loss: 0.6867
09/07 06:24:37 PM: Best result seen so far for mnli.
09/07 06:24:37 PM: Best result seen so far for winograd-coreference.
09/07 06:24:37 PM: Best result seen so far for micro.
09/07 06:24:37 PM: Best result seen so far for macro.
09/07 06:24:37 PM: Updating LR scheduler:
09/07 06:24:37 PM: 	Best result seen so far for macro_avg: 0.666
09/07 06:24:37 PM: 	# validation passes without improvement: 0
09/07 06:24:37 PM: mnli_loss: training: 0.621002 validation: 0.562479
09/07 06:24:37 PM: winograd-coreference_loss: training: 0.000000 validation: 0.685829
09/07 06:24:37 PM: macro_avg: validation: 0.666046
09/07 06:24:37 PM: micro_avg: validation: 0.769984
09/07 06:24:37 PM: mnli_accuracy: training: 0.743368 validation: 0.774400
09/07 06:24:37 PM: winograd-coreference_f1: validation: 0.342857
09/07 06:24:37 PM: winograd-coreference_acc: validation: 0.557692
09/07 06:24:37 PM: Global learning rate: 1e-05
09/07 06:24:37 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 06:24:39 PM: Update 6001: task mnli, batch 1 (5994): accuracy: 0.7500, mnli_loss: 0.7541
09/07 06:24:49 PM: Update 6041: task mnli, batch 41 (6034): accuracy: 0.7337, mnli_loss: 0.6468
09/07 06:24:59 PM: Update 6079: task mnli, batch 79 (6072): accuracy: 0.7458, mnli_loss: 0.6219
09/07 06:25:09 PM: Update 6120: task mnli, batch 120 (6113): accuracy: 0.7417, mnli_loss: 0.6264
09/07 06:25:19 PM: Update 6163: task mnli, batch 163 (6156): accuracy: 0.7495, mnli_loss: 0.6113
09/07 06:25:27 PM: Update 6197: task winograd-coreference, batch 1 (8): f1: 0.2222, acc: 0.4167, winograd-coreference_loss: 0.7087
09/07 06:25:29 PM: Update 6205: task mnli, batch 204 (6197): accuracy: 0.7514, mnli_loss: 0.6052
09/07 06:25:39 PM: Update 6247: task mnli, batch 246 (6239): accuracy: 0.7561, mnli_loss: 0.5994
09/07 06:25:49 PM: Update 6278: task mnli, batch 277 (6270): accuracy: 0.7599, mnli_loss: 0.5975
09/07 06:25:59 PM: Update 6319: task mnli, batch 318 (6311): accuracy: 0.7606, mnli_loss: 0.5969
09/07 06:26:10 PM: Update 6359: task mnli, batch 358 (6351): accuracy: 0.7597, mnli_loss: 0.5953
09/07 06:26:20 PM: Update 6399: task mnli, batch 398 (6391): accuracy: 0.7600, mnli_loss: 0.5987
09/07 06:26:30 PM: Update 6439: task mnli, batch 438 (6431): accuracy: 0.7614, mnli_loss: 0.5934
09/07 06:26:40 PM: Update 6480: task mnli, batch 479 (6472): accuracy: 0.7604, mnli_loss: 0.5950
09/07 06:26:50 PM: Update 6523: task mnli, batch 522 (6515): accuracy: 0.7589, mnli_loss: 0.5972
09/07 06:27:00 PM: Update 6565: task mnli, batch 564 (6557): accuracy: 0.7587, mnli_loss: 0.5960
09/07 06:27:01 PM: Update 6569: task winograd-coreference, batch 2 (9): f1: 0.1333, acc: 0.4583, winograd-coreference_loss: 0.7496
09/07 06:27:10 PM: Update 6605: task mnli, batch 603 (6596): accuracy: 0.7586, mnli_loss: 0.5972
09/07 06:27:21 PM: Update 6646: task mnli, batch 644 (6637): accuracy: 0.7576, mnli_loss: 0.5985
09/07 06:27:31 PM: Update 6682: task mnli, batch 680 (6673): accuracy: 0.7575, mnli_loss: 0.5991
09/07 06:27:42 PM: Update 6722: task mnli, batch 720 (6713): accuracy: 0.7577, mnli_loss: 0.5992
09/07 06:27:52 PM: Update 6763: task mnli, batch 761 (6754): accuracy: 0.7576, mnli_loss: 0.5985
09/07 06:28:02 PM: Update 6803: task mnli, batch 801 (6794): accuracy: 0.7569, mnli_loss: 0.5989
09/07 06:28:12 PM: Update 6844: task mnli, batch 842 (6835): accuracy: 0.7560, mnli_loss: 0.5993
09/07 06:28:22 PM: Update 6885: task mnli, batch 883 (6876): accuracy: 0.7568, mnli_loss: 0.5973
09/07 06:28:32 PM: Update 6926: task mnli, batch 924 (6917): accuracy: 0.7574, mnli_loss: 0.5968
09/07 06:28:43 PM: Update 6967: task mnli, batch 965 (6958): accuracy: 0.7579, mnli_loss: 0.5962
09/07 06:28:50 PM: ***** Step 7000 / Validation 7 *****
09/07 06:28:50 PM: mnli: trained on 998 batches, 0.061 epochs
09/07 06:28:50 PM: winograd-coreference: trained on 2 batches, 0.083 epochs
09/07 06:28:50 PM: Validating...
09/07 06:28:53 PM: Evaluate: task mnli, batch 35 (209): accuracy: 0.7798, mnli_loss: 0.5339
09/07 06:29:03 PM: Evaluate: task mnli, batch 185 (209): accuracy: 0.7773, mnli_loss: 0.5426
09/07 06:29:04 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.3333, acc: 0.5000, winograd-coreference_loss: 0.6886
09/07 06:29:04 PM: Best result seen so far for mnli.
09/07 06:29:04 PM: Best result seen so far for micro.
09/07 06:29:04 PM: Updating LR scheduler:
09/07 06:29:04 PM: 	Best result seen so far for macro_avg: 0.666
09/07 06:29:04 PM: 	# validation passes without improvement: 1
09/07 06:29:04 PM: mnli_loss: training: 0.594798 validation: 0.548104
09/07 06:29:04 PM: winograd-coreference_loss: training: 0.749600 validation: 0.689530
09/07 06:29:04 PM: macro_avg: validation: 0.652223
09/07 06:29:04 PM: micro_avg: validation: 0.770572
09/07 06:29:04 PM: mnli_accuracy: training: 0.758439 validation: 0.775600
09/07 06:29:04 PM: winograd-coreference_f1: training: 0.133333 validation: 0.423529
09/07 06:29:04 PM: winograd-coreference_acc: training: 0.458333 validation: 0.528846
09/07 06:29:04 PM: Global learning rate: 1e-05
09/07 06:29:04 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 06:29:13 PM: Update 7031: task mnli, batch 31 (7022): accuracy: 0.7581, mnli_loss: 0.5941
09/07 06:29:23 PM: Update 7069: task mnli, batch 69 (7060): accuracy: 0.7421, mnli_loss: 0.6111
09/07 06:29:33 PM: Update 7102: task mnli, batch 102 (7093): accuracy: 0.7471, mnli_loss: 0.6026
09/07 06:29:43 PM: Update 7145: task mnli, batch 145 (7136): accuracy: 0.7503, mnli_loss: 0.6039
09/07 06:29:53 PM: Update 7187: task mnli, batch 187 (7178): accuracy: 0.7511, mnli_loss: 0.6013
09/07 06:30:04 PM: Update 7229: task mnli, batch 229 (7220): accuracy: 0.7560, mnli_loss: 0.5940
09/07 06:30:14 PM: Update 7268: task mnli, batch 268 (7259): accuracy: 0.7561, mnli_loss: 0.5961
09/07 06:30:24 PM: Update 7308: task mnli, batch 308 (7299): accuracy: 0.7561, mnli_loss: 0.5945
09/07 06:30:34 PM: Update 7350: task mnli, batch 350 (7341): accuracy: 0.7588, mnli_loss: 0.5876
09/07 06:30:44 PM: Update 7392: task mnli, batch 392 (7383): accuracy: 0.7602, mnli_loss: 0.5848
09/07 06:30:54 PM: Update 7431: task mnli, batch 431 (7422): accuracy: 0.7578, mnli_loss: 0.5899
09/07 06:31:05 PM: Update 7472: task mnli, batch 472 (7463): accuracy: 0.7571, mnli_loss: 0.5912
09/07 06:31:15 PM: Update 7513: task mnli, batch 513 (7504): accuracy: 0.7565, mnli_loss: 0.5917
09/07 06:31:25 PM: Update 7541: task mnli, batch 541 (7532): accuracy: 0.7571, mnli_loss: 0.5911
09/07 06:31:35 PM: Update 7583: task mnli, batch 583 (7574): accuracy: 0.7584, mnli_loss: 0.5888
09/07 06:31:45 PM: Update 7623: task mnli, batch 623 (7614): accuracy: 0.7589, mnli_loss: 0.5893
09/07 06:31:55 PM: Update 7663: task mnli, batch 663 (7654): accuracy: 0.7604, mnli_loss: 0.5858
09/07 06:32:05 PM: Update 7703: task mnli, batch 703 (7694): accuracy: 0.7610, mnli_loss: 0.5869
09/07 06:32:15 PM: Update 7744: task mnli, batch 744 (7735): accuracy: 0.7624, mnli_loss: 0.5846
09/07 06:32:26 PM: Update 7784: task mnli, batch 784 (7775): accuracy: 0.7626, mnli_loss: 0.5849
09/07 06:32:36 PM: Update 7824: task mnli, batch 824 (7815): accuracy: 0.7636, mnli_loss: 0.5838
09/07 06:32:46 PM: Update 7865: task mnli, batch 865 (7856): accuracy: 0.7633, mnli_loss: 0.5834
09/07 06:32:56 PM: Update 7907: task mnli, batch 907 (7898): accuracy: 0.7637, mnli_loss: 0.5822
09/07 06:33:06 PM: Update 7940: task mnli, batch 940 (7931): accuracy: 0.7642, mnli_loss: 0.5818
09/07 06:33:16 PM: Update 7981: task mnli, batch 981 (7972): accuracy: 0.7638, mnli_loss: 0.5820
09/07 06:33:21 PM: ***** Step 8000 / Validation 8 *****
09/07 06:33:21 PM: mnli: trained on 1000 batches, 0.061 epochs
09/07 06:33:21 PM: winograd-coreference: trained on 0 batches, 0.000 epochs
09/07 06:33:21 PM: Validating...
09/07 06:33:26 PM: Evaluate: task mnli, batch 82 (209): accuracy: 0.7927, mnli_loss: 0.5173
09/07 06:33:35 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.1429, acc: 0.5000, winograd-coreference_loss: 0.6834
09/07 06:33:35 PM: Best result seen so far for mnli.
09/07 06:33:35 PM: Best result seen so far for micro.
09/07 06:33:35 PM: Updating LR scheduler:
09/07 06:33:35 PM: 	Best result seen so far for macro_avg: 0.666
09/07 06:33:35 PM: 	# validation passes without improvement: 2
09/07 06:33:35 PM: mnli_loss: training: 0.582371 validation: 0.539484
09/07 06:33:35 PM: winograd-coreference_loss: training: 0.000000 validation: 0.684947
09/07 06:33:35 PM: macro_avg: validation: 0.640400
09/07 06:33:35 PM: micro_avg: validation: 0.775078
09/07 06:33:35 PM: mnli_accuracy: training: 0.763889 validation: 0.780800
09/07 06:33:35 PM: winograd-coreference_f1: validation: 0.235294
09/07 06:33:35 PM: winograd-coreference_acc: validation: 0.500000
09/07 06:33:35 PM: Global learning rate: 1e-05
09/07 06:33:35 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 06:33:36 PM: Update 8002: task mnli, batch 2 (7993): accuracy: 0.8333, mnli_loss: 0.4543
09/07 06:33:46 PM: Update 8043: task mnli, batch 43 (8034): accuracy: 0.7703, mnli_loss: 0.5767
09/07 06:33:56 PM: Update 8084: task mnli, batch 84 (8075): accuracy: 0.7664, mnli_loss: 0.5801
09/07 06:34:07 PM: Update 8126: task mnli, batch 126 (8117): accuracy: 0.7649, mnli_loss: 0.5808
09/07 06:34:17 PM: Update 8167: task mnli, batch 167 (8158): accuracy: 0.7730, mnli_loss: 0.5719
09/07 06:34:27 PM: Update 8208: task mnli, batch 208 (8199): accuracy: 0.7716, mnli_loss: 0.5756
09/07 06:34:33 PM: Update 8235: task winograd-coreference, batch 1 (10): f1: 0.1000, acc: 0.2500, winograd-coreference_loss: 0.7462
09/07 06:34:37 PM: Update 8250: task mnli, batch 249 (8240): accuracy: 0.7702, mnli_loss: 0.5764
09/07 06:34:47 PM: Update 8291: task mnli, batch 290 (8281): accuracy: 0.7685, mnli_loss: 0.5789
09/07 06:34:57 PM: Update 8329: task mnli, batch 328 (8319): accuracy: 0.7683, mnli_loss: 0.5782
09/07 06:35:07 PM: Update 8358: task mnli, batch 357 (8348): accuracy: 0.7665, mnli_loss: 0.5799
09/07 06:35:17 PM: Update 8400: task mnli, batch 399 (8390): accuracy: 0.7660, mnli_loss: 0.5798
09/07 06:35:28 PM: Update 8441: task mnli, batch 440 (8431): accuracy: 0.7661, mnli_loss: 0.5789
09/07 06:35:38 PM: Update 8480: task mnli, batch 479 (8470): accuracy: 0.7651, mnli_loss: 0.5812
09/07 06:35:48 PM: Update 8521: task mnli, batch 520 (8511): accuracy: 0.7663, mnli_loss: 0.5774
09/07 06:35:58 PM: Update 8563: task mnli, batch 562 (8553): accuracy: 0.7671, mnli_loss: 0.5758
09/07 06:36:08 PM: Update 8605: task mnli, batch 604 (8595): accuracy: 0.7654, mnli_loss: 0.5781
09/07 06:36:18 PM: Update 8644: task mnli, batch 643 (8634): accuracy: 0.7651, mnli_loss: 0.5785
09/07 06:36:28 PM: Update 8685: task mnli, batch 684 (8675): accuracy: 0.7668, mnli_loss: 0.5755
09/07 06:36:38 PM: Update 8726: task mnli, batch 725 (8716): accuracy: 0.7682, mnli_loss: 0.5716
09/07 06:36:48 PM: Update 8767: task mnli, batch 766 (8757): accuracy: 0.7683, mnli_loss: 0.5713
09/07 06:36:58 PM: Update 8799: task mnli, batch 798 (8789): accuracy: 0.7681, mnli_loss: 0.5718
09/07 06:37:08 PM: Update 8839: task mnli, batch 838 (8829): accuracy: 0.7684, mnli_loss: 0.5705
09/07 06:37:19 PM: Update 8879: task mnli, batch 878 (8869): accuracy: 0.7666, mnli_loss: 0.5745
09/07 06:37:29 PM: Update 8919: task mnli, batch 918 (8909): accuracy: 0.7664, mnli_loss: 0.5755
09/07 06:37:39 PM: Update 8961: task mnli, batch 960 (8951): accuracy: 0.7674, mnli_loss: 0.5739
09/07 06:37:49 PM: ***** Step 9000 / Validation 9 *****
09/07 06:37:49 PM: mnli: trained on 999 batches, 0.061 epochs
09/07 06:37:49 PM: winograd-coreference: trained on 1 batches, 0.042 epochs
09/07 06:37:49 PM: Validating...
09/07 06:37:49 PM: Evaluate: task mnli, batch 7 (209): accuracy: 0.7798, mnli_loss: 0.4878
09/07 06:37:59 PM: Evaluate: task mnli, batch 157 (209): accuracy: 0.7893, mnli_loss: 0.5299
09/07 06:38:04 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.5000, acc: 0.3333, winograd-coreference_loss: 0.8507
09/07 06:38:04 PM: Best result seen so far for mnli.
09/07 06:38:04 PM: Best result seen so far for micro.
09/07 06:38:04 PM: Updating LR scheduler:
09/07 06:38:04 PM: 	Best result seen so far for macro_avg: 0.666
09/07 06:38:04 PM: 	# validation passes without improvement: 3
09/07 06:38:04 PM: mnli_loss: training: 0.573540 validation: 0.537351
09/07 06:38:04 PM: winograd-coreference_loss: training: 0.746234 validation: 0.800223
09/07 06:38:04 PM: macro_avg: validation: 0.576392
09/07 06:38:04 PM: micro_avg: validation: 0.778801
09/07 06:38:04 PM: mnli_accuracy: training: 0.767362 validation: 0.787400
09/07 06:38:04 PM: winograd-coreference_f1: training: 0.100000 validation: 0.535211
09/07 06:38:04 PM: winograd-coreference_acc: training: 0.250000 validation: 0.365385
09/07 06:38:04 PM: Global learning rate: 1e-05
09/07 06:38:04 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 06:38:09 PM: Update 9018: task mnli, batch 18 (9008): accuracy: 0.7546, mnli_loss: 0.5905
09/07 06:38:19 PM: Update 9061: task mnli, batch 61 (9051): accuracy: 0.7671, mnli_loss: 0.5691
09/07 06:38:30 PM: Update 9102: task mnli, batch 102 (9092): accuracy: 0.7647, mnli_loss: 0.5721
09/07 06:38:40 PM: Update 9142: task mnli, batch 142 (9132): accuracy: 0.7644, mnli_loss: 0.5675
09/07 06:38:50 PM: Update 9183: task mnli, batch 183 (9173): accuracy: 0.7698, mnli_loss: 0.5646
09/07 06:39:00 PM: Update 9214: task mnli, batch 214 (9204): accuracy: 0.7664, mnli_loss: 0.5682
09/07 06:39:10 PM: Update 9255: task mnli, batch 255 (9245): accuracy: 0.7669, mnli_loss: 0.5714
09/07 06:39:20 PM: Update 9297: task mnli, batch 297 (9287): accuracy: 0.7673, mnli_loss: 0.5710
09/07 06:39:30 PM: Update 9338: task mnli, batch 338 (9328): accuracy: 0.7669, mnli_loss: 0.5724
09/07 06:39:41 PM: Update 9380: task mnli, batch 380 (9370): accuracy: 0.7689, mnli_loss: 0.5667
09/07 06:39:51 PM: Update 9420: task mnli, batch 420 (9410): accuracy: 0.7715, mnli_loss: 0.5609
09/07 06:40:01 PM: Update 9460: task mnli, batch 460 (9450): accuracy: 0.7725, mnli_loss: 0.5582
09/07 06:40:11 PM: Update 9499: task mnli, batch 499 (9489): accuracy: 0.7730, mnli_loss: 0.5593
09/07 06:40:21 PM: Update 9541: task mnli, batch 541 (9531): accuracy: 0.7719, mnli_loss: 0.5613
09/07 06:40:31 PM: Update 9581: task mnli, batch 581 (9571): accuracy: 0.7711, mnli_loss: 0.5633
09/07 06:40:41 PM: Update 9613: task winograd-coreference, batch 1 (11): f1: 0.7027, acc: 0.5417, winograd-coreference_loss: 0.7252
09/07 06:40:41 PM: Update 9614: task mnli, batch 613 (9603): accuracy: 0.7717, mnli_loss: 0.5633
09/07 06:40:51 PM: Update 9653: task mnli, batch 652 (9642): accuracy: 0.7700, mnli_loss: 0.5651
09/07 06:41:01 PM: Update 9695: task mnli, batch 694 (9684): accuracy: 0.7689, mnli_loss: 0.5662
09/07 06:41:11 PM: Update 9736: task mnli, batch 735 (9725): accuracy: 0.7687, mnli_loss: 0.5675
09/07 06:41:22 PM: Update 9779: task mnli, batch 778 (9768): accuracy: 0.7687, mnli_loss: 0.5669
09/07 06:41:32 PM: Update 9819: task mnli, batch 818 (9808): accuracy: 0.7696, mnli_loss: 0.5654
09/07 06:41:42 PM: Update 9860: task mnli, batch 859 (9849): accuracy: 0.7694, mnli_loss: 0.5657
09/07 06:41:52 PM: Update 9901: task mnli, batch 900 (9890): accuracy: 0.7705, mnli_loss: 0.5632
09/07 06:42:02 PM: Update 9943: task mnli, batch 942 (9932): accuracy: 0.7708, mnli_loss: 0.5628
09/07 06:42:12 PM: Update 9984: task mnli, batch 983 (9973): accuracy: 0.7710, mnli_loss: 0.5622
09/07 06:42:16 PM: ***** Step 10000 / Validation 10 *****
09/07 06:42:16 PM: mnli: trained on 999 batches, 0.061 epochs
09/07 06:42:16 PM: winograd-coreference: trained on 1 batches, 0.042 epochs
09/07 06:42:16 PM: Validating...
09/07 06:42:22 PM: Evaluate: task mnli, batch 91 (209): accuracy: 0.7880, mnli_loss: 0.5384
09/07 06:42:30 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.5000, acc: 0.5000, winograd-coreference_loss: 0.6953
09/07 06:42:30 PM: Updating LR scheduler:
09/07 06:42:30 PM: 	Best result seen so far for macro_avg: 0.666
09/07 06:42:30 PM: 	# validation passes without improvement: 4
09/07 06:42:30 PM: mnli_loss: training: 0.561620 validation: 0.559086
09/07 06:42:30 PM: winograd-coreference_loss: training: 0.725186 validation: 0.695818
09/07 06:42:30 PM: macro_avg: validation: 0.634792
09/07 06:42:30 PM: micro_avg: validation: 0.773315
09/07 06:42:30 PM: mnli_accuracy: training: 0.771369 validation: 0.779200
09/07 06:42:30 PM: winograd-coreference_f1: training: 0.702703 validation: 0.513761
09/07 06:42:30 PM: winograd-coreference_acc: training: 0.541667 validation: 0.490385
09/07 06:42:30 PM: Global learning rate: 1e-05
09/07 06:42:30 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 06:42:32 PM: Update 10005: task mnli, batch 5 (9994): accuracy: 0.7917, mnli_loss: 0.6350
09/07 06:42:42 PM: Update 10036: task mnli, batch 36 (10025): accuracy: 0.7675, mnli_loss: 0.5932
09/07 06:42:53 PM: Update 10078: task mnli, batch 78 (10067): accuracy: 0.7881, mnli_loss: 0.5414
09/07 06:43:03 PM: Update 10118: task mnli, batch 118 (10107): accuracy: 0.7858, mnli_loss: 0.5488
09/07 06:43:13 PM: Update 10158: task mnli, batch 158 (10147): accuracy: 0.7799, mnli_loss: 0.5521
09/07 06:43:23 PM: Update 10199: task mnli, batch 199 (10188): accuracy: 0.7802, mnli_loss: 0.5493
09/07 06:43:33 PM: Update 10241: task mnli, batch 241 (10230): accuracy: 0.7848, mnli_loss: 0.5426
09/07 06:43:43 PM: Update 10282: task winograd-coreference, batch 1 (12): f1: 0.4167, acc: 0.4167, winograd-coreference_loss: 0.7121
09/07 06:43:44 PM: Update 10284: task mnli, batch 283 (10272): accuracy: 0.7879, mnli_loss: 0.5330
09/07 06:43:54 PM: Update 10326: task mnli, batch 325 (10314): accuracy: 0.7861, mnli_loss: 0.5317
09/07 06:44:04 PM: Update 10364: task mnli, batch 363 (10352): accuracy: 0.7854, mnli_loss: 0.5324
09/07 06:44:14 PM: Update 10404: task mnli, batch 403 (10392): accuracy: 0.7860, mnli_loss: 0.5332
09/07 06:44:26 PM: Update 10438: task mnli, batch 437 (10426): accuracy: 0.7844, mnli_loss: 0.5350
09/07 06:44:36 PM: Update 10478: task mnli, batch 477 (10466): accuracy: 0.7827, mnli_loss: 0.5384
09/07 06:44:47 PM: Update 10517: task mnli, batch 516 (10505): accuracy: 0.7830, mnli_loss: 0.5368
09/07 06:44:57 PM: Update 10557: task mnli, batch 556 (10545): accuracy: 0.7828, mnli_loss: 0.5348
09/07 06:45:07 PM: Update 10597: task mnli, batch 596 (10585): accuracy: 0.7818, mnli_loss: 0.5368
09/07 06:45:17 PM: Update 10638: task mnli, batch 637 (10626): accuracy: 0.7810, mnli_loss: 0.5386
09/07 06:45:27 PM: Update 10679: task mnli, batch 678 (10667): accuracy: 0.7827, mnli_loss: 0.5357
09/07 06:45:37 PM: Update 10721: task mnli, batch 720 (10709): accuracy: 0.7828, mnli_loss: 0.5341
09/07 06:45:41 PM: Update 10738: task winograd-coreference, batch 2 (13): f1: 0.5902, acc: 0.4792, winograd-coreference_loss: 0.7337
09/07 06:45:48 PM: Update 10763: task mnli, batch 761 (10750): accuracy: 0.7819, mnli_loss: 0.5354
09/07 06:45:52 PM: Update 10781: task winograd-coreference, batch 3 (14): f1: 0.5714, acc: 0.4583, winograd-coreference_loss: 0.7472
09/07 06:45:58 PM: Update 10803: task mnli, batch 800 (10789): accuracy: 0.7817, mnli_loss: 0.5367
09/07 06:46:08 PM: Update 10844: task mnli, batch 841 (10830): accuracy: 0.7818, mnli_loss: 0.5365
09/07 06:46:18 PM: Update 10874: task mnli, batch 871 (10860): accuracy: 0.7821, mnli_loss: 0.5364
09/07 06:46:28 PM: Update 10914: task mnli, batch 911 (10900): accuracy: 0.7818, mnli_loss: 0.5374
09/07 06:46:38 PM: Update 10956: task mnli, batch 953 (10942): accuracy: 0.7815, mnli_loss: 0.5384
09/07 06:46:48 PM: Update 10996: task mnli, batch 993 (10982): accuracy: 0.7814, mnli_loss: 0.5371
09/07 06:46:49 PM: ***** Step 11000 / Validation 11 *****
09/07 06:46:49 PM: mnli: trained on 997 batches, 0.061 epochs
09/07 06:46:49 PM: winograd-coreference: trained on 3 batches, 0.125 epochs
09/07 06:46:49 PM: Validating...
09/07 06:46:58 PM: Evaluate: task mnli, batch 135 (209): accuracy: 0.7935, mnli_loss: 0.5162
09/07 06:47:03 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.2222, acc: 0.7083, winograd-coreference_loss: 0.6428
09/07 06:47:04 PM: Best result seen so far for mnli.
09/07 06:47:04 PM: Best result seen so far for winograd-coreference.
09/07 06:47:04 PM: Best result seen so far for micro.
09/07 06:47:04 PM: Best result seen so far for macro.
09/07 06:47:04 PM: Updating LR scheduler:
09/07 06:47:04 PM: 	Best result seen so far for macro_avg: 0.714
09/07 06:47:04 PM: 	# validation passes without improvement: 0
09/07 06:47:04 PM: mnli_loss: training: 0.536911 validation: 0.525648
09/07 06:47:04 PM: winograd-coreference_loss: training: 0.747242 validation: 0.675758
09/07 06:47:04 PM: macro_avg: validation: 0.713608
09/07 06:47:04 PM: micro_avg: validation: 0.789381
09/07 06:47:04 PM: mnli_accuracy: training: 0.781501 validation: 0.792600
09/07 06:47:04 PM: winograd-coreference_f1: training: 0.571429 validation: 0.050000
09/07 06:47:04 PM: winograd-coreference_acc: training: 0.458333 validation: 0.634615
09/07 06:47:04 PM: Global learning rate: 1e-05
09/07 06:47:04 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 06:47:09 PM: Update 11016: task mnli, batch 16 (11002): accuracy: 0.7526, mnli_loss: 0.5948
09/07 06:47:19 PM: Update 11057: task mnli, batch 57 (11043): accuracy: 0.7844, mnli_loss: 0.5241
09/07 06:47:29 PM: Update 11097: task mnli, batch 97 (11083): accuracy: 0.7899, mnli_loss: 0.5071
09/07 06:47:39 PM: Update 11139: task mnli, batch 139 (11125): accuracy: 0.7920, mnli_loss: 0.5097
09/07 06:47:49 PM: Update 11178: task mnli, batch 178 (11164): accuracy: 0.7893, mnli_loss: 0.5163
09/07 06:48:00 PM: Update 11217: task mnli, batch 217 (11203): accuracy: 0.7838, mnli_loss: 0.5258
09/07 06:48:10 PM: Update 11256: task mnli, batch 256 (11242): accuracy: 0.7832, mnli_loss: 0.5295
09/07 06:48:20 PM: Update 11286: task mnli, batch 286 (11272): accuracy: 0.7853, mnli_loss: 0.5268
09/07 06:48:24 PM: Update 11303: task winograd-coreference, batch 1 (15): f1: 0.0000, acc: 0.6250, winograd-coreference_loss: 0.6686
09/07 06:48:30 PM: Update 11327: task mnli, batch 326 (11312): accuracy: 0.7821, mnli_loss: 0.5334
09/07 06:48:40 PM: Update 11368: task mnli, batch 366 (11352): accuracy: 0.7817, mnli_loss: 0.5331
09/07 06:48:51 PM: Update 11408: task mnli, batch 406 (11392): accuracy: 0.7828, mnli_loss: 0.5324
09/07 06:49:01 PM: Update 11449: task mnli, batch 447 (11433): accuracy: 0.7826, mnli_loss: 0.5338
09/07 06:49:11 PM: Update 11488: task mnli, batch 486 (11472): accuracy: 0.7814, mnli_loss: 0.5348
09/07 06:49:20 PM: Update 11526: task winograd-coreference, batch 3 (17): f1: 0.1463, acc: 0.5139, winograd-coreference_loss: 0.7030
09/07 06:49:21 PM: Update 11528: task mnli, batch 525 (11511): accuracy: 0.7824, mnli_loss: 0.5339
09/07 06:49:31 PM: Update 11566: task mnli, batch 563 (11549): accuracy: 0.7811, mnli_loss: 0.5352
09/07 06:49:41 PM: Update 11605: task mnli, batch 602 (11588): accuracy: 0.7803, mnli_loss: 0.5360
09/07 06:49:51 PM: Update 11646: task mnli, batch 643 (11629): accuracy: 0.7807, mnli_loss: 0.5354
09/07 06:50:01 PM: Update 11689: task mnli, batch 686 (11672): accuracy: 0.7823, mnli_loss: 0.5337
09/07 06:50:11 PM: Update 11720: task mnli, batch 717 (11703): accuracy: 0.7818, mnli_loss: 0.5348
09/07 06:50:21 PM: Update 11761: task mnli, batch 758 (11744): accuracy: 0.7813, mnli_loss: 0.5371
09/07 06:50:31 PM: Update 11799: task mnli, batch 796 (11782): accuracy: 0.7815, mnli_loss: 0.5371
09/07 06:50:42 PM: Update 11842: task mnli, batch 839 (11825): accuracy: 0.7816, mnli_loss: 0.5365
09/07 06:50:52 PM: Update 11882: task mnli, batch 879 (11865): accuracy: 0.7821, mnli_loss: 0.5363
09/07 06:51:02 PM: Update 11923: task mnli, batch 920 (11906): accuracy: 0.7820, mnli_loss: 0.5357
09/07 06:51:12 PM: Update 11964: task mnli, batch 961 (11947): accuracy: 0.7820, mnli_loss: 0.5355
09/07 06:51:21 PM: ***** Step 12000 / Validation 12 *****
09/07 06:51:21 PM: mnli: trained on 997 batches, 0.061 epochs
09/07 06:51:21 PM: winograd-coreference: trained on 3 batches, 0.125 epochs
09/07 06:51:21 PM: Validating...
09/07 06:51:22 PM: Evaluate: task mnli, batch 15 (209): accuracy: 0.7889, mnli_loss: 0.5028
09/07 06:51:32 PM: Evaluate: task mnli, batch 164 (209): accuracy: 0.7952, mnli_loss: 0.5208
09/07 06:51:35 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.0000, acc: 0.6250, winograd-coreference_loss: 0.6678
09/07 06:51:35 PM: Updating LR scheduler:
09/07 06:51:35 PM: 	Best result seen so far for macro_avg: 0.714
09/07 06:51:35 PM: 	# validation passes without improvement: 1
09/07 06:51:35 PM: mnli_loss: training: 0.536364 validation: 0.525780
09/07 06:51:35 PM: winograd-coreference_loss: training: 0.703022 validation: 0.676412
09/07 06:51:35 PM: macro_avg: validation: 0.712208
09/07 06:51:35 PM: micro_avg: validation: 0.786638
09/07 06:51:35 PM: mnli_accuracy: training: 0.781490 validation: 0.789800
09/07 06:51:35 PM: winograd-coreference_f1: training: 0.146341 validation: 0.095238
09/07 06:51:35 PM: winograd-coreference_acc: training: 0.513889 validation: 0.634615
09/07 06:51:35 PM: Global learning rate: 1e-05
09/07 06:51:35 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 06:51:42 PM: Update 12023: task mnli, batch 23 (12006): accuracy: 0.7953, mnli_loss: 0.5121
09/07 06:51:52 PM: Update 12064: task mnli, batch 64 (12047): accuracy: 0.7871, mnli_loss: 0.5123
09/07 06:52:03 PM: Update 12103: task mnli, batch 103 (12086): accuracy: 0.7836, mnli_loss: 0.5248
09/07 06:52:13 PM: Update 12132: task mnli, batch 132 (12115): accuracy: 0.7864, mnli_loss: 0.5228
09/07 06:52:23 PM: Update 12173: task mnli, batch 173 (12156): accuracy: 0.7886, mnli_loss: 0.5215
09/07 06:52:33 PM: Update 12214: task mnli, batch 214 (12197): accuracy: 0.7865, mnli_loss: 0.5248
09/07 06:52:43 PM: Update 12255: task mnli, batch 255 (12238): accuracy: 0.7899, mnli_loss: 0.5205
09/07 06:52:43 PM: Update 12257: task winograd-coreference, batch 1 (18): f1: 0.0000, acc: 0.4583, winograd-coreference_loss: 0.7612
09/07 06:52:53 PM: Update 12297: task mnli, batch 296 (12279): accuracy: 0.7904, mnli_loss: 0.5244
09/07 06:53:03 PM: Update 12336: task mnli, batch 335 (12318): accuracy: 0.7897, mnli_loss: 0.5250
09/07 06:53:13 PM: Update 12375: task mnli, batch 374 (12357): accuracy: 0.7911, mnli_loss: 0.5213
09/07 06:53:23 PM: Update 12417: task mnli, batch 416 (12399): accuracy: 0.7893, mnli_loss: 0.5251
09/07 06:53:34 PM: Update 12458: task mnli, batch 457 (12440): accuracy: 0.7873, mnli_loss: 0.5285
09/07 06:53:35 PM: Update 12464: task winograd-coreference, batch 2 (19): f1: 0.3500, acc: 0.4583, winograd-coreference_loss: 0.7316
09/07 06:53:44 PM: Update 12500: task mnli, batch 498 (12481): accuracy: 0.7845, mnli_loss: 0.5332
09/07 06:53:54 PM: Update 12532: task mnli, batch 530 (12513): accuracy: 0.7845, mnli_loss: 0.5328
09/07 06:54:04 PM: Update 12572: task mnli, batch 570 (12553): accuracy: 0.7867, mnli_loss: 0.5290
09/07 06:54:14 PM: Update 12614: task mnli, batch 612 (12595): accuracy: 0.7863, mnli_loss: 0.5300
09/07 06:54:24 PM: Update 12656: task mnli, batch 654 (12637): accuracy: 0.7870, mnli_loss: 0.5296
09/07 06:54:34 PM: Update 12697: task mnli, batch 695 (12678): accuracy: 0.7879, mnli_loss: 0.5285
09/07 06:54:45 PM: Update 12738: task mnli, batch 736 (12719): accuracy: 0.7878, mnli_loss: 0.5285
09/07 06:54:55 PM: Update 12779: task mnli, batch 777 (12760): accuracy: 0.7884, mnli_loss: 0.5267
09/07 06:55:05 PM: Update 12818: task mnli, batch 816 (12799): accuracy: 0.7890, mnli_loss: 0.5247
09/07 06:55:15 PM: Update 12858: task mnli, batch 856 (12839): accuracy: 0.7894, mnli_loss: 0.5238
09/07 06:55:25 PM: Update 12899: task mnli, batch 897 (12880): accuracy: 0.7896, mnli_loss: 0.5243
09/07 06:55:35 PM: Update 12939: task mnli, batch 937 (12920): accuracy: 0.7896, mnli_loss: 0.5241
09/07 06:55:45 PM: Update 12970: task mnli, batch 968 (12951): accuracy: 0.7893, mnli_loss: 0.5250
09/07 06:55:53 PM: ***** Step 13000 / Validation 13 *****
09/07 06:55:53 PM: mnli: trained on 998 batches, 0.061 epochs
09/07 06:55:53 PM: winograd-coreference: trained on 2 batches, 0.083 epochs
09/07 06:55:53 PM: Validating...
09/07 06:55:55 PM: Evaluate: task mnli, batch 33 (209): accuracy: 0.8043, mnli_loss: 0.4805
09/07 06:56:05 PM: Evaluate: task mnli, batch 182 (209): accuracy: 0.8013, mnli_loss: 0.4927
09/07 06:56:07 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.5455, acc: 0.5833, winograd-coreference_loss: 0.7166
09/07 06:56:08 PM: Best result seen so far for mnli.
09/07 06:56:08 PM: Best result seen so far for micro.
09/07 06:56:08 PM: Updating LR scheduler:
09/07 06:56:08 PM: 	Best result seen so far for macro_avg: 0.714
09/07 06:56:08 PM: 	# validation passes without improvement: 2
09/07 06:56:08 PM: mnli_loss: training: 0.525261 validation: 0.499370
09/07 06:56:08 PM: winograd-coreference_loss: training: 0.731626 validation: 0.698938
09/07 06:56:08 PM: macro_avg: validation: 0.692369
09/07 06:56:08 PM: micro_avg: validation: 0.793887
09/07 06:56:08 PM: mnli_accuracy: training: 0.789201 validation: 0.798200
09/07 06:56:08 PM: winograd-coreference_f1: training: 0.350000 validation: 0.547368
09/07 06:56:08 PM: winograd-coreference_acc: training: 0.458333 validation: 0.586538
09/07 06:56:08 PM: Global learning rate: 1e-05
09/07 06:56:08 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 06:56:15 PM: Update 13029: task mnli, batch 29 (13010): accuracy: 0.7716, mnli_loss: 0.5399
09/07 06:56:26 PM: Update 13070: task mnli, batch 70 (13051): accuracy: 0.7804, mnli_loss: 0.5541
09/07 06:56:36 PM: Update 13110: task mnli, batch 110 (13091): accuracy: 0.7833, mnli_loss: 0.5443
09/07 06:56:46 PM: Update 13153: task mnli, batch 153 (13134): accuracy: 0.7827, mnli_loss: 0.5414
09/07 06:56:56 PM: Update 13194: task mnli, batch 194 (13175): accuracy: 0.7833, mnli_loss: 0.5389
09/07 06:57:00 PM: Update 13210: task winograd-coreference, batch 1 (20): f1: 0.1000, acc: 0.2500, winograd-coreference_loss: 0.7430
09/07 06:57:06 PM: Update 13235: task mnli, batch 234 (13215): accuracy: 0.7829, mnli_loss: 0.5395
09/07 06:57:16 PM: Update 13275: task mnli, batch 274 (13255): accuracy: 0.7824, mnli_loss: 0.5446
09/07 06:57:26 PM: Update 13316: task mnli, batch 315 (13296): accuracy: 0.7832, mnli_loss: 0.5436
09/07 06:57:37 PM: Update 13357: task mnli, batch 356 (13337): accuracy: 0.7825, mnli_loss: 0.5442
09/07 06:57:47 PM: Update 13389: task mnli, batch 388 (13369): accuracy: 0.7851, mnli_loss: 0.5385
09/07 06:57:57 PM: Update 13431: task mnli, batch 430 (13411): accuracy: 0.7848, mnli_loss: 0.5387
09/07 06:58:07 PM: Update 13471: task mnli, batch 470 (13451): accuracy: 0.7852, mnli_loss: 0.5355
09/07 06:58:17 PM: Update 13512: task mnli, batch 511 (13492): accuracy: 0.7873, mnli_loss: 0.5313
09/07 06:58:27 PM: Update 13553: task mnli, batch 552 (13533): accuracy: 0.7879, mnli_loss: 0.5320
09/07 06:58:37 PM: Update 13593: task mnli, batch 592 (13573): accuracy: 0.7868, mnli_loss: 0.5334
09/07 06:58:47 PM: Update 13633: task mnli, batch 632 (13613): accuracy: 0.7873, mnli_loss: 0.5326
09/07 06:58:57 PM: Update 13675: task mnli, batch 674 (13655): accuracy: 0.7875, mnli_loss: 0.5311
09/07 06:59:07 PM: Update 13717: task mnli, batch 716 (13697): accuracy: 0.7881, mnli_loss: 0.5295
09/07 06:59:18 PM: Update 13757: task mnli, batch 756 (13737): accuracy: 0.7878, mnli_loss: 0.5293
09/07 06:59:28 PM: Update 13788: task mnli, batch 787 (13768): accuracy: 0.7877, mnli_loss: 0.5293
09/07 06:59:38 PM: Update 13828: task mnli, batch 827 (13808): accuracy: 0.7872, mnli_loss: 0.5306
09/07 06:59:48 PM: Update 13868: task mnli, batch 867 (13848): accuracy: 0.7866, mnli_loss: 0.5315
09/07 06:59:58 PM: Update 13908: task mnli, batch 907 (13888): accuracy: 0.7859, mnli_loss: 0.5333
09/07 07:00:08 PM: Update 13949: task mnli, batch 948 (13929): accuracy: 0.7861, mnli_loss: 0.5329
09/07 07:00:18 PM: Update 13989: task mnli, batch 988 (13969): accuracy: 0.7863, mnli_loss: 0.5325
09/07 07:00:21 PM: ***** Step 14000 / Validation 14 *****
09/07 07:00:21 PM: mnli: trained on 999 batches, 0.061 epochs
09/07 07:00:21 PM: winograd-coreference: trained on 1 batches, 0.042 epochs
09/07 07:00:21 PM: Validating...
09/07 07:00:28 PM: Evaluate: task mnli, batch 103 (209): accuracy: 0.8030, mnli_loss: 0.5080
09/07 07:00:35 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.0000, acc: 0.6667, winograd-coreference_loss: 0.6349
09/07 07:00:36 PM: Best result seen so far for mnli.
09/07 07:00:36 PM: Best result seen so far for micro.
09/07 07:00:36 PM: Best result seen so far for macro.
09/07 07:00:36 PM: Updating LR scheduler:
09/07 07:00:36 PM: 	Best result seen so far for macro_avg: 0.717
09/07 07:00:36 PM: 	# validation passes without improvement: 0
09/07 07:00:36 PM: mnli_loss: training: 0.532346 validation: 0.510677
09/07 07:00:36 PM: winograd-coreference_loss: training: 0.743036 validation: 0.674461
09/07 07:00:36 PM: macro_avg: validation: 0.717408
09/07 07:00:36 PM: micro_avg: validation: 0.796826
09/07 07:00:36 PM: mnli_accuracy: training: 0.786477 validation: 0.800200
09/07 07:00:36 PM: winograd-coreference_f1: training: 0.100000 validation: 0.000000
09/07 07:00:36 PM: winograd-coreference_acc: training: 0.250000 validation: 0.634615
09/07 07:00:36 PM: Global learning rate: 1e-05
09/07 07:00:36 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 07:00:38 PM: Update 14007: task mnli, batch 7 (13987): accuracy: 0.8095, mnli_loss: 0.5336
09/07 07:00:48 PM: Update 14048: task mnli, batch 48 (14028): accuracy: 0.7969, mnli_loss: 0.5494
09/07 07:00:59 PM: Update 14090: task mnli, batch 90 (14070): accuracy: 0.7986, mnli_loss: 0.5333
09/07 07:01:09 PM: Update 14131: task mnli, batch 131 (14111): accuracy: 0.7971, mnli_loss: 0.5306
09/07 07:01:19 PM: Update 14172: task mnli, batch 172 (14152): accuracy: 0.7982, mnli_loss: 0.5265
09/07 07:01:29 PM: Update 14201: task mnli, batch 201 (14181): accuracy: 0.7990, mnli_loss: 0.5240
09/07 07:01:39 PM: Update 14242: task mnli, batch 242 (14222): accuracy: 0.7919, mnli_loss: 0.5366
09/07 07:01:49 PM: Update 14284: task mnli, batch 284 (14264): accuracy: 0.7907, mnli_loss: 0.5341
09/07 07:01:59 PM: Update 14324: task mnli, batch 324 (14304): accuracy: 0.7918, mnli_loss: 0.5312
09/07 07:02:09 PM: Update 14364: task mnli, batch 364 (14344): accuracy: 0.7953, mnli_loss: 0.5222
09/07 07:02:19 PM: Update 14407: task mnli, batch 407 (14387): accuracy: 0.7940, mnli_loss: 0.5236
09/07 07:02:29 PM: Update 14448: task mnli, batch 448 (14428): accuracy: 0.7954, mnli_loss: 0.5225
09/07 07:02:40 PM: Update 14489: task mnli, batch 489 (14469): accuracy: 0.7954, mnli_loss: 0.5226
09/07 07:02:50 PM: Update 14529: task mnli, batch 529 (14509): accuracy: 0.7952, mnli_loss: 0.5210
09/07 07:03:00 PM: Update 14569: task mnli, batch 569 (14549): accuracy: 0.7951, mnli_loss: 0.5195
09/07 07:03:10 PM: Update 14610: task mnli, batch 610 (14590): accuracy: 0.7952, mnli_loss: 0.5175
09/07 07:03:17 PM: Update 14629: task winograd-coreference, batch 1 (21): f1: 0.2000, acc: 0.6667, winograd-coreference_loss: 0.6874
09/07 07:03:20 PM: Update 14640: task mnli, batch 639 (14619): accuracy: 0.7949, mnli_loss: 0.5182
09/07 07:03:30 PM: Update 14681: task mnli, batch 680 (14660): accuracy: 0.7942, mnli_loss: 0.5177
09/07 07:03:41 PM: Update 14722: task mnli, batch 721 (14701): accuracy: 0.7941, mnli_loss: 0.5175
09/07 07:03:51 PM: Update 14762: task mnli, batch 761 (14741): accuracy: 0.7944, mnli_loss: 0.5183
09/07 07:04:01 PM: Update 14803: task mnli, batch 802 (14782): accuracy: 0.7939, mnli_loss: 0.5189
09/07 07:04:11 PM: Update 14845: task mnli, batch 844 (14824): accuracy: 0.7941, mnli_loss: 0.5169
09/07 07:04:21 PM: Update 14884: task mnli, batch 883 (14863): accuracy: 0.7943, mnli_loss: 0.5160
09/07 07:04:31 PM: Update 14925: task mnli, batch 924 (14904): accuracy: 0.7943, mnli_loss: 0.5169
09/07 07:04:41 PM: Update 14966: task mnli, batch 965 (14945): accuracy: 0.7948, mnli_loss: 0.5157
09/07 07:04:50 PM: ***** Step 15000 / Validation 15 *****
09/07 07:04:50 PM: mnli: trained on 999 batches, 0.061 epochs
09/07 07:04:50 PM: winograd-coreference: trained on 1 batches, 0.042 epochs
09/07 07:04:50 PM: Validating...
09/07 07:04:51 PM: Evaluate: task mnli, batch 25 (209): accuracy: 0.8017, mnli_loss: 0.4792
09/07 07:05:01 PM: Evaluate: task mnli, batch 176 (209): accuracy: 0.7985, mnli_loss: 0.5019
09/07 07:05:04 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.0000, acc: 0.6667, winograd-coreference_loss: 0.6239
09/07 07:05:04 PM: Updating LR scheduler:
09/07 07:05:04 PM: 	Best result seen so far for macro_avg: 0.717
09/07 07:05:04 PM: 	# validation passes without improvement: 1
09/07 07:05:04 PM: mnli_loss: training: 0.517472 validation: 0.506920
09/07 07:05:04 PM: winograd-coreference_loss: training: 0.687447 validation: 0.681002
09/07 07:05:04 PM: macro_avg: validation: 0.715608
09/07 07:05:04 PM: micro_avg: validation: 0.793299
09/07 07:05:04 PM: mnli_accuracy: training: 0.793990 validation: 0.796600
09/07 07:05:04 PM: winograd-coreference_f1: training: 0.200000 validation: 0.000000
09/07 07:05:04 PM: winograd-coreference_acc: training: 0.666667 validation: 0.634615
09/07 07:05:04 PM: Global learning rate: 1e-05
09/07 07:05:04 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 07:05:12 PM: Update 15028: task mnli, batch 28 (15007): accuracy: 0.8006, mnli_loss: 0.5115
09/07 07:05:22 PM: Update 15060: task mnli, batch 60 (15039): accuracy: 0.7849, mnli_loss: 0.5460
09/07 07:05:32 PM: Update 15100: task mnli, batch 100 (15079): accuracy: 0.7901, mnli_loss: 0.5311
09/07 07:05:42 PM: Update 15141: task mnli, batch 141 (15120): accuracy: 0.7897, mnli_loss: 0.5313
09/07 07:05:52 PM: Update 15182: task mnli, batch 182 (15161): accuracy: 0.7851, mnli_loss: 0.5348
09/07 07:06:02 PM: Update 15223: task mnli, batch 223 (15202): accuracy: 0.7871, mnli_loss: 0.5327
09/07 07:06:12 PM: Update 15263: task mnli, batch 263 (15242): accuracy: 0.7859, mnli_loss: 0.5338
09/07 07:06:22 PM: Update 15303: task mnli, batch 303 (15282): accuracy: 0.7855, mnli_loss: 0.5317
09/07 07:06:32 PM: Update 15343: task mnli, batch 343 (15322): accuracy: 0.7856, mnli_loss: 0.5306
09/07 07:06:43 PM: Update 15384: task mnli, batch 384 (15363): accuracy: 0.7868, mnli_loss: 0.5277
09/07 07:06:53 PM: Update 15425: task mnli, batch 425 (15404): accuracy: 0.7886, mnli_loss: 0.5259
09/07 07:07:03 PM: Update 15456: task mnli, batch 456 (15435): accuracy: 0.7880, mnli_loss: 0.5276
09/07 07:07:13 PM: Update 15498: task mnli, batch 498 (15477): accuracy: 0.7864, mnli_loss: 0.5302
09/07 07:07:23 PM: Update 15539: task mnli, batch 539 (15518): accuracy: 0.7873, mnli_loss: 0.5253
09/07 07:07:33 PM: Update 15580: task mnli, batch 580 (15559): accuracy: 0.7888, mnli_loss: 0.5230
09/07 07:07:43 PM: Update 15621: task mnli, batch 621 (15600): accuracy: 0.7881, mnli_loss: 0.5234
09/07 07:07:53 PM: Update 15662: task mnli, batch 662 (15641): accuracy: 0.7881, mnli_loss: 0.5222
09/07 07:08:04 PM: Update 15703: task mnli, batch 703 (15682): accuracy: 0.7897, mnli_loss: 0.5210
09/07 07:08:14 PM: Update 15743: task mnli, batch 743 (15722): accuracy: 0.7893, mnli_loss: 0.5213
09/07 07:08:24 PM: Update 15783: task mnli, batch 783 (15762): accuracy: 0.7894, mnli_loss: 0.5221
09/07 07:08:34 PM: Update 15823: task mnli, batch 823 (15802): accuracy: 0.7899, mnli_loss: 0.5211
09/07 07:08:44 PM: Update 15862: task mnli, batch 862 (15841): accuracy: 0.7891, mnli_loss: 0.5221
09/07 07:08:54 PM: Update 15896: task mnli, batch 896 (15875): accuracy: 0.7898, mnli_loss: 0.5210
09/07 07:09:04 PM: Update 15936: task mnli, batch 936 (15915): accuracy: 0.7906, mnli_loss: 0.5198
09/07 07:09:14 PM: Update 15976: task mnli, batch 976 (15955): accuracy: 0.7918, mnli_loss: 0.5177
09/07 07:09:20 PM: ***** Step 16000 / Validation 16 *****
09/07 07:09:20 PM: mnli: trained on 1000 batches, 0.061 epochs
09/07 07:09:20 PM: winograd-coreference: trained on 0 batches, 0.000 epochs
09/07 07:09:20 PM: Validating...
09/07 07:09:24 PM: Evaluate: task mnli, batch 60 (209): accuracy: 0.8069, mnli_loss: 0.4855
09/07 07:09:34 PM: Evaluate: task mnli, batch 195 (209): accuracy: 0.8064, mnli_loss: 0.4914
09/07 07:09:35 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.0000, acc: 0.6667, winograd-coreference_loss: 0.6232
09/07 07:09:36 PM: Best result seen so far for mnli.
09/07 07:09:36 PM: Best result seen so far for micro.
09/07 07:09:36 PM: Best result seen so far for macro.
09/07 07:09:36 PM: Updating LR scheduler:
09/07 07:09:36 PM: 	Best result seen so far for macro_avg: 0.720
09/07 07:09:36 PM: 	# validation passes without improvement: 0
09/07 07:09:36 PM: mnli_loss: training: 0.517894 validation: 0.496115
09/07 07:09:36 PM: winograd-coreference_loss: training: 0.000000 validation: 0.684941
09/07 07:09:36 PM: macro_avg: validation: 0.719808
09/07 07:09:36 PM: micro_avg: validation: 0.801528
09/07 07:09:36 PM: mnli_accuracy: training: 0.791583 validation: 0.805000
09/07 07:09:36 PM: winograd-coreference_f1: validation: 0.000000
09/07 07:09:36 PM: winograd-coreference_acc: validation: 0.634615
09/07 07:09:36 PM: Global learning rate: 1e-05
09/07 07:09:36 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 07:09:45 PM: Update 16029: task mnli, batch 29 (16008): accuracy: 0.7830, mnli_loss: 0.5315
09/07 07:09:55 PM: Update 16069: task mnli, batch 69 (16048): accuracy: 0.7929, mnli_loss: 0.5262
09/07 07:10:05 PM: Update 16111: task mnli, batch 111 (16090): accuracy: 0.7969, mnli_loss: 0.5132
09/07 07:10:15 PM: Update 16151: task mnli, batch 151 (16130): accuracy: 0.7942, mnli_loss: 0.5169
09/07 07:10:16 PM: Update 16158: task winograd-coreference, batch 1 (22): f1: 0.0000, acc: 0.4583, winograd-coreference_loss: 0.7667
09/07 07:10:25 PM: Update 16192: task mnli, batch 191 (16170): accuracy: 0.7912, mnli_loss: 0.5269
09/07 07:10:35 PM: Update 16234: task mnli, batch 233 (16212): accuracy: 0.7906, mnli_loss: 0.5268
09/07 07:10:45 PM: Update 16275: task mnli, batch 274 (16253): accuracy: 0.7927, mnli_loss: 0.5234
09/07 07:10:56 PM: Update 16315: task mnli, batch 314 (16293): accuracy: 0.7899, mnli_loss: 0.5266
09/07 07:11:06 PM: Update 16355: task mnli, batch 354 (16333): accuracy: 0.7891, mnli_loss: 0.5264
09/07 07:11:16 PM: Update 16396: task mnli, batch 395 (16374): accuracy: 0.7881, mnli_loss: 0.5239
09/07 07:11:26 PM: Update 16427: task mnli, batch 426 (16405): accuracy: 0.7891, mnli_loss: 0.5234
09/07 07:11:36 PM: Update 16467: task mnli, batch 466 (16445): accuracy: 0.7909, mnli_loss: 0.5216
09/07 07:11:46 PM: Update 16508: task mnli, batch 507 (16486): accuracy: 0.7926, mnli_loss: 0.5211
09/07 07:11:56 PM: Update 16549: task mnli, batch 548 (16527): accuracy: 0.7940, mnli_loss: 0.5183
09/07 07:12:06 PM: Update 16592: task mnli, batch 591 (16570): accuracy: 0.7951, mnli_loss: 0.5160
09/07 07:12:16 PM: Update 16633: task mnli, batch 632 (16611): accuracy: 0.7970, mnli_loss: 0.5130
09/07 07:12:26 PM: Update 16674: task mnli, batch 673 (16652): accuracy: 0.7968, mnli_loss: 0.5132
09/07 07:12:37 PM: Update 16714: task mnli, batch 713 (16692): accuracy: 0.7972, mnli_loss: 0.5122
09/07 07:12:47 PM: Update 16755: task mnli, batch 754 (16733): accuracy: 0.7976, mnli_loss: 0.5103
09/07 07:12:57 PM: Update 16794: task mnli, batch 793 (16772): accuracy: 0.7968, mnli_loss: 0.5107
09/07 07:13:07 PM: Update 16826: task mnli, batch 825 (16804): accuracy: 0.7970, mnli_loss: 0.5099
09/07 07:13:17 PM: Update 16869: task mnli, batch 868 (16847): accuracy: 0.7973, mnli_loss: 0.5089
09/07 07:13:27 PM: Update 16910: task mnli, batch 909 (16888): accuracy: 0.7972, mnli_loss: 0.5081
09/07 07:13:31 PM: Update 16925: task winograd-coreference, batch 2 (23): f1: 0.0625, acc: 0.3750, winograd-coreference_loss: 0.7614
09/07 07:13:37 PM: Update 16952: task mnli, batch 950 (16929): accuracy: 0.7970, mnli_loss: 0.5078
09/07 07:13:48 PM: Update 16992: task mnli, batch 990 (16969): accuracy: 0.7964, mnli_loss: 0.5078
09/07 07:13:50 PM: ***** Step 17000 / Validation 17 *****
09/07 07:13:50 PM: mnli: trained on 998 batches, 0.061 epochs
09/07 07:13:50 PM: winograd-coreference: trained on 2 batches, 0.083 epochs
09/07 07:13:50 PM: Validating...
09/07 07:13:58 PM: Evaluate: task mnli, batch 119 (209): accuracy: 0.8036, mnli_loss: 0.5012
09/07 07:14:04 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.5000, acc: 0.5000, winograd-coreference_loss: 0.7146
09/07 07:14:04 PM: Best result seen so far for mnli.
09/07 07:14:04 PM: Updating LR scheduler:
09/07 07:14:04 PM: 	Best result seen so far for macro_avg: 0.720
09/07 07:14:04 PM: 	# validation passes without improvement: 1
09/07 07:14:04 PM: mnli_loss: training: 0.507729 validation: 0.504387
09/07 07:14:04 PM: winograd-coreference_loss: training: 0.761385 validation: 0.703491
09/07 07:14:04 PM: macro_avg: validation: 0.628562
09/07 07:14:04 PM: micro_avg: validation: 0.798002
09/07 07:14:04 PM: mnli_accuracy: training: 0.796372 validation: 0.805200
09/07 07:14:04 PM: winograd-coreference_f1: training: 0.062500 validation: 0.477064
09/07 07:14:04 PM: winograd-coreference_acc: training: 0.375000 validation: 0.451923
09/07 07:14:04 PM: Global learning rate: 1e-05
09/07 07:14:04 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 07:14:08 PM: Update 17012: task mnli, batch 12 (16989): accuracy: 0.7639, mnli_loss: 0.5323
09/07 07:14:18 PM: Update 17053: task mnli, batch 53 (17030): accuracy: 0.7925, mnli_loss: 0.5042
09/07 07:14:28 PM: Update 17094: task mnli, batch 94 (17071): accuracy: 0.7992, mnli_loss: 0.4897
09/07 07:14:38 PM: Update 17133: task mnli, batch 133 (17110): accuracy: 0.7967, mnli_loss: 0.4956
09/07 07:14:40 PM: Update 17142: task winograd-coreference, batch 1 (24): f1: 0.2105, acc: 0.3750, winograd-coreference_loss: 0.6985
09/07 07:14:48 PM: Update 17175: task mnli, batch 174 (17151): accuracy: 0.8005, mnli_loss: 0.4893
09/07 07:14:58 PM: Update 17216: task mnli, batch 215 (17192): accuracy: 0.8021, mnli_loss: 0.4880
09/07 07:15:08 PM: Update 17249: task mnli, batch 248 (17225): accuracy: 0.8011, mnli_loss: 0.4866
09/07 07:15:19 PM: Update 17291: task mnli, batch 290 (17267): accuracy: 0.8032, mnli_loss: 0.4854
09/07 07:15:29 PM: Update 17332: task mnli, batch 331 (17308): accuracy: 0.8003, mnli_loss: 0.4885
09/07 07:15:39 PM: Update 17373: task mnli, batch 372 (17349): accuracy: 0.7992, mnli_loss: 0.4902
09/07 07:15:49 PM: Update 17414: task mnli, batch 413 (17390): accuracy: 0.7974, mnli_loss: 0.4938
09/07 07:15:59 PM: Update 17454: task mnli, batch 453 (17430): accuracy: 0.7977, mnli_loss: 0.4937
09/07 07:16:10 PM: Update 17495: task mnli, batch 494 (17471): accuracy: 0.7987, mnli_loss: 0.4943
09/07 07:16:10 PM: Update 17497: task winograd-coreference, batch 2 (25): f1: 0.1905, acc: 0.3462, winograd-coreference_loss: 0.7185
09/07 07:16:20 PM: Update 17537: task mnli, batch 535 (17512): accuracy: 0.7964, mnli_loss: 0.4960
09/07 07:16:30 PM: Update 17579: task mnli, batch 577 (17554): accuracy: 0.7970, mnli_loss: 0.4943
09/07 07:16:40 PM: Update 17619: task mnli, batch 617 (17594): accuracy: 0.7974, mnli_loss: 0.4956
09/07 07:16:52 PM: Update 17653: task mnli, batch 651 (17628): accuracy: 0.7972, mnli_loss: 0.4953
09/07 07:17:02 PM: Update 17694: task mnli, batch 692 (17669): accuracy: 0.7974, mnli_loss: 0.4951
09/07 07:17:12 PM: Update 17734: task mnli, batch 732 (17709): accuracy: 0.7980, mnli_loss: 0.4963
09/07 07:17:22 PM: Update 17774: task mnli, batch 772 (17749): accuracy: 0.7977, mnli_loss: 0.4963
09/07 07:17:32 PM: Update 17815: task mnli, batch 813 (17790): accuracy: 0.7968, mnli_loss: 0.4981
09/07 07:17:42 PM: Update 17856: task mnli, batch 854 (17831): accuracy: 0.7980, mnli_loss: 0.4970
09/07 07:17:53 PM: Update 17897: task mnli, batch 895 (17872): accuracy: 0.7993, mnli_loss: 0.4953
09/07 07:18:00 PM: Update 17925: task winograd-coreference, batch 3 (26): f1: 0.4074, acc: 0.3600, winograd-coreference_loss: 0.7490
09/07 07:18:03 PM: Update 17937: task mnli, batch 934 (17911): accuracy: 0.7993, mnli_loss: 0.4949
09/07 07:18:13 PM: Update 17979: task mnli, batch 976 (17953): accuracy: 0.8002, mnli_loss: 0.4935
09/07 07:18:14 PM: Update 17986: task winograd-coreference, batch 4 (27): f1: 0.4474, acc: 0.4324, winograd-coreference_loss: 0.7275
09/07 07:18:18 PM: ***** Step 18000 / Validation 18 *****
09/07 07:18:18 PM: mnli: trained on 996 batches, 0.061 epochs
09/07 07:18:18 PM: winograd-coreference: trained on 4 batches, 0.167 epochs
09/07 07:18:18 PM: Validating...
09/07 07:18:23 PM: Evaluate: task mnli, batch 70 (209): accuracy: 0.8190, mnli_loss: 0.4651
09/07 07:18:32 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.2667, acc: 0.5417, winograd-coreference_loss: 0.6682
09/07 07:18:32 PM: Best result seen so far for mnli.
09/07 07:18:32 PM: Best result seen so far for micro.
09/07 07:18:32 PM: Updating LR scheduler:
09/07 07:18:32 PM: 	Best result seen so far for macro_avg: 0.720
09/07 07:18:32 PM: 	# validation passes without improvement: 2
09/07 07:18:32 PM: mnli_loss: training: 0.493439 validation: 0.485241
09/07 07:18:32 PM: winograd-coreference_loss: training: 0.727535 validation: 0.679755
09/07 07:18:32 PM: macro_avg: validation: 0.709485
09/07 07:18:32 PM: micro_avg: validation: 0.808973
09/07 07:18:32 PM: mnli_accuracy: training: 0.800067 validation: 0.813200
09/07 07:18:32 PM: winograd-coreference_f1: training: 0.447368 validation: 0.327869
09/07 07:18:32 PM: winograd-coreference_acc: training: 0.432432 validation: 0.605769
09/07 07:18:32 PM: Global learning rate: 1e-05
09/07 07:18:32 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 07:18:34 PM: Update 18001: task mnli, batch 1 (17974): accuracy: 0.7917, mnli_loss: 0.5942
09/07 07:18:44 PM: Update 18043: task mnli, batch 43 (18016): accuracy: 0.8043, mnli_loss: 0.4947
09/07 07:18:54 PM: Update 18074: task mnli, batch 74 (18047): accuracy: 0.8077, mnli_loss: 0.4895
09/07 07:19:04 PM: Update 18115: task mnli, batch 115 (18088): accuracy: 0.7998, mnli_loss: 0.5075
09/07 07:19:14 PM: Update 18157: task mnli, batch 157 (18130): accuracy: 0.7973, mnli_loss: 0.5026
09/07 07:19:24 PM: Update 18197: task mnli, batch 197 (18170): accuracy: 0.8008, mnli_loss: 0.4995
09/07 07:19:34 PM: Update 18237: task mnli, batch 237 (18210): accuracy: 0.8026, mnli_loss: 0.4976
09/07 07:19:45 PM: Update 18280: task mnli, batch 280 (18253): accuracy: 0.8045, mnli_loss: 0.4937
09/07 07:19:55 PM: Update 18320: task mnli, batch 320 (18293): accuracy: 0.8046, mnli_loss: 0.4924
09/07 07:20:05 PM: Update 18361: task mnli, batch 361 (18334): accuracy: 0.8053, mnli_loss: 0.4914
09/07 07:20:15 PM: Update 18402: task mnli, batch 402 (18375): accuracy: 0.8055, mnli_loss: 0.4919
09/07 07:20:25 PM: Update 18443: task mnli, batch 443 (18416): accuracy: 0.8063, mnli_loss: 0.4874
09/07 07:20:35 PM: Update 18484: task mnli, batch 484 (18457): accuracy: 0.8041, mnli_loss: 0.4906
09/07 07:20:46 PM: Update 18516: task mnli, batch 516 (18489): accuracy: 0.8049, mnli_loss: 0.4907
09/07 07:20:56 PM: Update 18557: task mnli, batch 557 (18530): accuracy: 0.8043, mnli_loss: 0.4925
09/07 07:21:06 PM: Update 18599: task mnli, batch 599 (18572): accuracy: 0.8054, mnli_loss: 0.4900
09/07 07:21:16 PM: Update 18642: task mnli, batch 642 (18615): accuracy: 0.8074, mnli_loss: 0.4858
09/07 07:21:26 PM: Update 18682: task mnli, batch 682 (18655): accuracy: 0.8078, mnli_loss: 0.4849
09/07 07:21:36 PM: Update 18721: task mnli, batch 721 (18694): accuracy: 0.8078, mnli_loss: 0.4852
09/07 07:21:46 PM: Update 18762: task mnli, batch 762 (18735): accuracy: 0.8082, mnli_loss: 0.4830
09/07 07:21:57 PM: Update 18802: task mnli, batch 802 (18775): accuracy: 0.8078, mnli_loss: 0.4830
09/07 07:22:07 PM: Update 18843: task mnli, batch 843 (18816): accuracy: 0.8083, mnli_loss: 0.4830
09/07 07:22:17 PM: Update 18885: task mnli, batch 885 (18858): accuracy: 0.8086, mnli_loss: 0.4828
09/07 07:22:27 PM: Update 18917: task mnli, batch 917 (18890): accuracy: 0.8081, mnli_loss: 0.4834
09/07 07:22:37 PM: Update 18957: task mnli, batch 957 (18930): accuracy: 0.8076, mnli_loss: 0.4842
09/07 07:22:47 PM: Update 18997: task mnli, batch 997 (18970): accuracy: 0.8084, mnli_loss: 0.4836
09/07 07:22:48 PM: ***** Step 19000 / Validation 19 *****
09/07 07:22:48 PM: mnli: trained on 1000 batches, 0.061 epochs
09/07 07:22:48 PM: winograd-coreference: trained on 0 batches, 0.000 epochs
09/07 07:22:48 PM: Validating...
09/07 07:22:57 PM: Evaluate: task mnli, batch 138 (209): accuracy: 0.8164, mnli_loss: 0.4675
09/07 07:23:02 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.1429, acc: 0.5000, winograd-coreference_loss: 0.6628
09/07 07:23:02 PM: Best result seen so far for mnli.
09/07 07:23:02 PM: Best result seen so far for micro.
09/07 07:23:02 PM: Updating LR scheduler:
09/07 07:23:02 PM: 	Best result seen so far for macro_avg: 0.720
09/07 07:23:02 PM: 	# validation passes without improvement: 3
09/07 07:23:02 PM: mnli_loss: training: 0.483575 validation: 0.479321
09/07 07:23:02 PM: winograd-coreference_loss: training: 0.000000 validation: 0.679440
09/07 07:23:02 PM: macro_avg: validation: 0.696862
09/07 07:23:02 PM: micro_avg: validation: 0.811912
09/07 07:23:02 PM: mnli_accuracy: training: 0.808433 validation: 0.816800
09/07 07:23:02 PM: winograd-coreference_f1: validation: 0.241379
09/07 07:23:02 PM: winograd-coreference_acc: validation: 0.576923
09/07 07:23:02 PM: Global learning rate: 1e-05
09/07 07:23:02 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 07:23:08 PM: Update 19016: task mnli, batch 16 (18989): accuracy: 0.8255, mnli_loss: 0.4849
09/07 07:23:18 PM: Update 19058: task mnli, batch 58 (19031): accuracy: 0.8204, mnli_loss: 0.4669
09/07 07:23:28 PM: Update 19098: task mnli, batch 98 (19071): accuracy: 0.8193, mnli_loss: 0.4598
09/07 07:23:38 PM: Update 19139: task mnli, batch 139 (19112): accuracy: 0.8189, mnli_loss: 0.4539
09/07 07:23:48 PM: Update 19180: task mnli, batch 180 (19153): accuracy: 0.8167, mnli_loss: 0.4615
09/07 07:23:58 PM: Update 19221: task mnli, batch 221 (19194): accuracy: 0.8130, mnli_loss: 0.4685
09/07 07:24:08 PM: Update 19263: task mnli, batch 263 (19236): accuracy: 0.8135, mnli_loss: 0.4693
09/07 07:24:18 PM: Update 19303: task mnli, batch 303 (19276): accuracy: 0.8106, mnli_loss: 0.4745
09/07 07:24:28 PM: Update 19334: task mnli, batch 334 (19307): accuracy: 0.8082, mnli_loss: 0.4769
09/07 07:24:39 PM: Update 19373: task mnli, batch 373 (19346): accuracy: 0.8088, mnli_loss: 0.4751
09/07 07:24:49 PM: Update 19415: task mnli, batch 415 (19388): accuracy: 0.8108, mnli_loss: 0.4699
09/07 07:24:59 PM: Update 19458: task mnli, batch 458 (19431): accuracy: 0.8128, mnli_loss: 0.4678
09/07 07:25:09 PM: Update 19497: task mnli, batch 497 (19470): accuracy: 0.8119, mnli_loss: 0.4702
09/07 07:25:19 PM: Update 19538: task mnli, batch 538 (19511): accuracy: 0.8105, mnli_loss: 0.4717
09/07 07:25:30 PM: Update 19579: task mnli, batch 579 (19552): accuracy: 0.8119, mnli_loss: 0.4689
09/07 07:25:40 PM: Update 19618: task mnli, batch 618 (19591): accuracy: 0.8105, mnli_loss: 0.4726
09/07 07:25:50 PM: Update 19660: task mnli, batch 660 (19633): accuracy: 0.8104, mnli_loss: 0.4725
09/07 07:26:00 PM: Update 19702: task mnli, batch 702 (19675): accuracy: 0.8107, mnli_loss: 0.4714
09/07 07:26:06 PM: Update 19726: task winograd-coreference, batch 1 (28): f1: 0.2500, acc: 0.5000, winograd-coreference_loss: 0.6760
09/07 07:26:12 PM: Update 19741: task mnli, batch 740 (19713): accuracy: 0.8119, mnli_loss: 0.4703
09/07 07:26:22 PM: Update 19780: task mnli, batch 779 (19752): accuracy: 0.8119, mnli_loss: 0.4708
09/07 07:26:32 PM: Update 19821: task mnli, batch 820 (19793): accuracy: 0.8123, mnli_loss: 0.4702
09/07 07:26:42 PM: Update 19862: task mnli, batch 861 (19834): accuracy: 0.8135, mnli_loss: 0.4693
09/07 07:26:52 PM: Update 19901: task mnli, batch 900 (19873): accuracy: 0.8137, mnli_loss: 0.4696
09/07 07:27:02 PM: Update 19940: task mnli, batch 939 (19912): accuracy: 0.8137, mnli_loss: 0.4703
09/07 07:27:12 PM: Update 19983: task mnli, batch 982 (19955): accuracy: 0.8133, mnli_loss: 0.4708
09/07 07:27:16 PM: ***** Step 20000 / Validation 20 *****
09/07 07:27:16 PM: mnli: trained on 999 batches, 0.061 epochs
09/07 07:27:16 PM: winograd-coreference: trained on 1 batches, 0.042 epochs
09/07 07:27:16 PM: Validating...
09/07 07:27:22 PM: Evaluate: task mnli, batch 88 (209): accuracy: 0.8120, mnli_loss: 0.4723
09/07 07:27:31 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.5385, acc: 0.5000, winograd-coreference_loss: 0.7314
09/07 07:27:32 PM: Updating LR scheduler:
09/07 07:27:32 PM: 	Best result seen so far for macro_avg: 0.720
09/07 07:27:32 PM: 	# validation passes without improvement: 4
09/07 07:27:32 PM: mnli_loss: training: 0.471867 validation: 0.490368
09/07 07:27:32 PM: winograd-coreference_loss: training: 0.676032 validation: 0.710192
09/07 07:27:32 PM: macro_avg: validation: 0.639877
09/07 07:27:32 PM: micro_avg: validation: 0.801724
09/07 07:27:32 PM: mnli_accuracy: training: 0.812604 validation: 0.808600
09/07 07:27:32 PM: winograd-coreference_f1: training: 0.250000 validation: 0.513274
09/07 07:27:32 PM: winograd-coreference_acc: training: 0.500000 validation: 0.471154
09/07 07:27:32 PM: Global learning rate: 1e-05
09/07 07:27:32 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 07:27:33 PM: Update 20001: task mnli, batch 1 (19973): accuracy: 0.7500, mnli_loss: 0.7981
09/07 07:27:43 PM: Update 20043: task mnli, batch 43 (20015): accuracy: 0.8043, mnli_loss: 0.5119
09/07 07:27:53 PM: Update 20084: task mnli, batch 84 (20056): accuracy: 0.8160, mnli_loss: 0.4769
09/07 07:28:04 PM: Update 20126: task mnli, batch 126 (20098): accuracy: 0.8171, mnli_loss: 0.4692
09/07 07:28:14 PM: Update 20159: task mnli, batch 159 (20131): accuracy: 0.8180, mnli_loss: 0.4629
09/07 07:28:24 PM: Update 20199: task mnli, batch 199 (20171): accuracy: 0.8175, mnli_loss: 0.4585
09/07 07:28:34 PM: Update 20240: task mnli, batch 240 (20212): accuracy: 0.8225, mnli_loss: 0.4490
09/07 07:28:42 PM: Update 20274: task winograd-coreference, batch 1 (29): f1: 0.3478, acc: 0.3750, winograd-coreference_loss: 0.7020
09/07 07:28:44 PM: Update 20280: task mnli, batch 279 (20251): accuracy: 0.8219, mnli_loss: 0.4504
09/07 07:28:54 PM: Update 20320: task mnli, batch 319 (20291): accuracy: 0.8224, mnli_loss: 0.4499
09/07 07:29:04 PM: Update 20361: task mnli, batch 360 (20332): accuracy: 0.8226, mnli_loss: 0.4519
09/07 07:29:15 PM: Update 20402: task mnli, batch 401 (20373): accuracy: 0.8231, mnli_loss: 0.4500
09/07 07:29:25 PM: Update 20443: task mnli, batch 442 (20414): accuracy: 0.8224, mnli_loss: 0.4511
09/07 07:29:35 PM: Update 20485: task mnli, batch 484 (20456): accuracy: 0.8236, mnli_loss: 0.4502
09/07 07:29:45 PM: Update 20525: task mnli, batch 524 (20496): accuracy: 0.8232, mnli_loss: 0.4491
09/07 07:29:55 PM: Update 20562: task mnli, batch 561 (20533): accuracy: 0.8228, mnli_loss: 0.4503
09/07 07:30:05 PM: Update 20593: task mnli, batch 592 (20564): accuracy: 0.8229, mnli_loss: 0.4515
09/07 07:30:16 PM: Update 20634: task mnli, batch 633 (20605): accuracy: 0.8230, mnli_loss: 0.4518
09/07 07:30:26 PM: Update 20674: task mnli, batch 673 (20645): accuracy: 0.8226, mnli_loss: 0.4519
09/07 07:30:36 PM: Update 20716: task mnli, batch 715 (20687): accuracy: 0.8222, mnli_loss: 0.4526
09/07 07:30:46 PM: Update 20755: task mnli, batch 754 (20726): accuracy: 0.8228, mnli_loss: 0.4513
09/07 07:30:56 PM: Update 20795: task mnli, batch 794 (20766): accuracy: 0.8237, mnli_loss: 0.4483
09/07 07:31:06 PM: Update 20836: task mnli, batch 835 (20807): accuracy: 0.8239, mnli_loss: 0.4478
09/07 07:31:16 PM: Update 20876: task mnli, batch 875 (20847): accuracy: 0.8236, mnli_loss: 0.4480
09/07 07:31:27 PM: Update 20917: task mnli, batch 916 (20888): accuracy: 0.8242, mnli_loss: 0.4478
09/07 07:31:37 PM: Update 20958: task mnli, batch 957 (20929): accuracy: 0.8250, mnli_loss: 0.4465
09/07 07:31:48 PM: Update 20993: task mnli, batch 992 (20964): accuracy: 0.8246, mnli_loss: 0.4477
09/07 07:31:50 PM: ***** Step 21000 / Validation 21 *****
09/07 07:31:50 PM: mnli: trained on 999 batches, 0.061 epochs
09/07 07:31:50 PM: winograd-coreference: trained on 1 batches, 0.042 epochs
09/07 07:31:50 PM: Validating...
09/07 07:31:58 PM: Evaluate: task mnli, batch 122 (209): accuracy: 0.8087, mnli_loss: 0.4877
09/07 07:32:04 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.3529, acc: 0.5417, winograd-coreference_loss: 0.6840
09/07 07:32:04 PM: Updating LR scheduler:
09/07 07:32:04 PM: 	Best result seen so far for macro_avg: 0.720
09/07 07:32:04 PM: 	# validation passes without improvement: 0
09/07 07:32:04 PM: mnli_loss: training: 0.447934 validation: 0.493629
09/07 07:32:04 PM: winograd-coreference_loss: training: 0.702034 validation: 0.683974
09/07 07:32:04 PM: macro_avg: validation: 0.698369
09/07 07:32:04 PM: micro_avg: validation: 0.805643
09/07 07:32:04 PM: mnli_accuracy: training: 0.824232 validation: 0.810200
09/07 07:32:04 PM: winograd-coreference_f1: training: 0.347826 validation: 0.426667
09/07 07:32:04 PM: winograd-coreference_acc: training: 0.375000 validation: 0.586538
09/07 07:32:04 PM: Global learning rate: 5e-06
09/07 07:32:04 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 07:32:08 PM: Update 21013: task mnli, batch 13 (20984): accuracy: 0.7981, mnli_loss: 0.4654
09/07 07:32:18 PM: Update 21055: task mnli, batch 55 (21026): accuracy: 0.8152, mnli_loss: 0.4425
09/07 07:32:28 PM: Update 21094: task mnli, batch 94 (21065): accuracy: 0.8191, mnli_loss: 0.4419
09/07 07:32:39 PM: Update 21134: task mnli, batch 134 (21105): accuracy: 0.8190, mnli_loss: 0.4482
09/07 07:32:46 PM: Update 21163: task winograd-coreference, batch 1 (30): f1: 0.4211, acc: 0.5417, winograd-coreference_loss: 0.6846
09/07 07:32:49 PM: Update 21176: task mnli, batch 175 (21146): accuracy: 0.8231, mnli_loss: 0.4375
09/07 07:32:59 PM: Update 21215: task mnli, batch 214 (21185): accuracy: 0.8279, mnli_loss: 0.4313
09/07 07:33:09 PM: Update 21257: task mnli, batch 256 (21227): accuracy: 0.8252, mnli_loss: 0.4335
09/07 07:33:19 PM: Update 21299: task mnli, batch 298 (21269): accuracy: 0.8247, mnli_loss: 0.4351
09/07 07:33:29 PM: Update 21338: task mnli, batch 337 (21308): accuracy: 0.8222, mnli_loss: 0.4394
09/07 07:33:39 PM: Update 21377: task mnli, batch 376 (21347): accuracy: 0.8198, mnli_loss: 0.4444
09/07 07:33:51 PM: Update 21411: task mnli, batch 410 (21381): accuracy: 0.8202, mnli_loss: 0.4414
09/07 07:34:01 PM: Update 21453: task mnli, batch 452 (21423): accuracy: 0.8231, mnli_loss: 0.4376
09/07 07:34:11 PM: Update 21496: task mnli, batch 495 (21466): accuracy: 0.8250, mnli_loss: 0.4373
09/07 07:34:22 PM: Update 21537: task mnli, batch 536 (21507): accuracy: 0.8257, mnli_loss: 0.4350
09/07 07:34:32 PM: Update 21577: task mnli, batch 576 (21547): accuracy: 0.8252, mnli_loss: 0.4361
09/07 07:34:42 PM: Update 21618: task mnli, batch 617 (21588): accuracy: 0.8251, mnli_loss: 0.4359
09/07 07:34:52 PM: Update 21658: task mnli, batch 657 (21628): accuracy: 0.8243, mnli_loss: 0.4361
09/07 07:35:02 PM: Update 21697: task mnli, batch 696 (21667): accuracy: 0.8246, mnli_loss: 0.4370
09/07 07:35:12 PM: Update 21734: task winograd-coreference, batch 2 (31): f1: 0.2581, acc: 0.5208, winograd-coreference_loss: 0.6850
09/07 07:35:12 PM: Update 21737: task mnli, batch 735 (21706): accuracy: 0.8251, mnli_loss: 0.4372
09/07 07:35:22 PM: Update 21778: task mnli, batch 776 (21747): accuracy: 0.8264, mnli_loss: 0.4353
09/07 07:35:32 PM: Update 21817: task mnli, batch 815 (21786): accuracy: 0.8271, mnli_loss: 0.4348
09/07 07:35:43 PM: Update 21848: task mnli, batch 846 (21817): accuracy: 0.8270, mnli_loss: 0.4351
09/07 07:35:53 PM: Update 21889: task mnli, batch 887 (21858): accuracy: 0.8268, mnli_loss: 0.4356
09/07 07:35:56 PM: Update 21901: task winograd-coreference, batch 3 (32): f1: 0.2273, acc: 0.5278, winograd-coreference_loss: 0.6869
09/07 07:36:03 PM: Update 21930: task mnli, batch 927 (21898): accuracy: 0.8270, mnli_loss: 0.4353
09/07 07:36:13 PM: Update 21971: task mnli, batch 968 (21939): accuracy: 0.8271, mnli_loss: 0.4342
09/07 07:36:20 PM: ***** Step 22000 / Validation 22 *****
09/07 07:36:20 PM: mnli: trained on 997 batches, 0.061 epochs
09/07 07:36:20 PM: winograd-coreference: trained on 3 batches, 0.125 epochs
09/07 07:36:20 PM: Validating...
09/07 07:36:23 PM: Evaluate: task mnli, batch 41 (209): accuracy: 0.8222, mnli_loss: 0.4683
09/07 07:36:33 PM: Evaluate: task mnli, batch 192 (209): accuracy: 0.8218, mnli_loss: 0.4716
09/07 07:36:34 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.1429, acc: 0.5000, winograd-coreference_loss: 0.6590
09/07 07:36:35 PM: Best result seen so far for mnli.
09/07 07:36:35 PM: Best result seen so far for micro.
09/07 07:36:35 PM: Updating LR scheduler:
09/07 07:36:35 PM: 	Best result seen so far for macro_avg: 0.720
09/07 07:36:35 PM: 	# validation passes without improvement: 1
09/07 07:36:35 PM: mnli_loss: training: 0.434206 validation: 0.478595
09/07 07:36:35 PM: winograd-coreference_loss: training: 0.686928 validation: 0.677985
09/07 07:36:35 PM: macro_avg: validation: 0.698062
09/07 07:36:35 PM: micro_avg: validation: 0.814263
09/07 07:36:35 PM: mnli_accuracy: training: 0.827367 validation: 0.819200
09/07 07:36:35 PM: winograd-coreference_f1: training: 0.227273 validation: 0.214286
09/07 07:36:35 PM: winograd-coreference_acc: training: 0.527778 validation: 0.576923
09/07 07:36:35 PM: Global learning rate: 5e-06
09/07 07:36:35 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 07:36:43 PM: Update 22031: task mnli, batch 31 (21999): accuracy: 0.8589, mnli_loss: 0.3653
09/07 07:36:53 PM: Update 22072: task mnli, batch 72 (22040): accuracy: 0.8484, mnli_loss: 0.3902
09/07 07:37:04 PM: Update 22114: task mnli, batch 114 (22082): accuracy: 0.8428, mnli_loss: 0.3992
09/07 07:37:14 PM: Update 22155: task mnli, batch 155 (22123): accuracy: 0.8419, mnli_loss: 0.3991
09/07 07:37:24 PM: Update 22195: task mnli, batch 195 (22163): accuracy: 0.8382, mnli_loss: 0.4140
09/07 07:37:34 PM: Update 22235: task mnli, batch 235 (22203): accuracy: 0.8395, mnli_loss: 0.4124
09/07 07:37:44 PM: Update 22267: task mnli, batch 267 (22235): accuracy: 0.8408, mnli_loss: 0.4111
09/07 07:37:54 PM: Update 22307: task mnli, batch 307 (22275): accuracy: 0.8420, mnli_loss: 0.4099
09/07 07:38:04 PM: Update 22348: task mnli, batch 348 (22316): accuracy: 0.8408, mnli_loss: 0.4124
09/07 07:38:15 PM: Update 22388: task mnli, batch 388 (22356): accuracy: 0.8399, mnli_loss: 0.4128
09/07 07:38:25 PM: Update 22430: task mnli, batch 430 (22398): accuracy: 0.8394, mnli_loss: 0.4146
09/07 07:38:35 PM: Update 22469: task mnli, batch 469 (22437): accuracy: 0.8386, mnli_loss: 0.4170
09/07 07:38:45 PM: Update 22510: task mnli, batch 510 (22478): accuracy: 0.8380, mnli_loss: 0.4187
09/07 07:38:55 PM: Update 22553: task mnli, batch 553 (22521): accuracy: 0.8390, mnli_loss: 0.4181
09/07 07:39:05 PM: Update 22596: task mnli, batch 596 (22564): accuracy: 0.8394, mnli_loss: 0.4164
09/07 07:39:15 PM: Update 22635: task mnli, batch 635 (22603): accuracy: 0.8407, mnli_loss: 0.4131
09/07 07:39:22 PM: Update 22662: task winograd-coreference, batch 1 (33): f1: 0.4348, acc: 0.4583, winograd-coreference_loss: 0.7037
09/07 07:39:25 PM: Update 22667: task mnli, batch 666 (22634): accuracy: 0.8400, mnli_loss: 0.4140
09/07 07:39:36 PM: Update 22708: task mnli, batch 707 (22675): accuracy: 0.8394, mnli_loss: 0.4151
09/07 07:39:46 PM: Update 22749: task mnli, batch 748 (22716): accuracy: 0.8403, mnli_loss: 0.4139
09/07 07:39:56 PM: Update 22790: task mnli, batch 789 (22757): accuracy: 0.8397, mnli_loss: 0.4147
09/07 07:40:06 PM: Update 22833: task mnli, batch 832 (22800): accuracy: 0.8406, mnli_loss: 0.4139
09/07 07:40:16 PM: Update 22873: task mnli, batch 872 (22840): accuracy: 0.8404, mnli_loss: 0.4143
09/07 07:40:26 PM: Update 22913: task mnli, batch 912 (22880): accuracy: 0.8404, mnli_loss: 0.4146
09/07 07:40:36 PM: Update 22955: task mnli, batch 954 (22922): accuracy: 0.8405, mnli_loss: 0.4149
09/07 07:40:47 PM: Update 22996: task mnli, batch 995 (22963): accuracy: 0.8405, mnli_loss: 0.4144
09/07 07:40:48 PM: ***** Step 23000 / Validation 23 *****
09/07 07:40:48 PM: mnli: trained on 999 batches, 0.061 epochs
09/07 07:40:48 PM: winograd-coreference: trained on 1 batches, 0.042 epochs
09/07 07:40:48 PM: Validating...
09/07 07:40:57 PM: Evaluate: task mnli, batch 134 (209): accuracy: 0.8175, mnli_loss: 0.4823
09/07 07:41:02 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.5000, acc: 0.5000, winograd-coreference_loss: 0.6982
09/07 07:41:02 PM: Updating LR scheduler:
09/07 07:41:02 PM: 	Best result seen so far for macro_avg: 0.720
09/07 07:41:02 PM: 	# validation passes without improvement: 2
09/07 07:41:02 PM: mnli_loss: training: 0.414153 validation: 0.490306
09/07 07:41:02 PM: winograd-coreference_loss: training: 0.703678 validation: 0.693547
09/07 07:41:02 PM: macro_avg: validation: 0.687546
09/07 07:41:02 PM: micro_avg: validation: 0.812108
09/07 07:41:02 PM: mnli_accuracy: training: 0.840526 validation: 0.817400
09/07 07:41:02 PM: winograd-coreference_f1: training: 0.434783 validation: 0.500000
09/07 07:41:02 PM: winograd-coreference_acc: training: 0.458333 validation: 0.557692
09/07 07:41:02 PM: Global learning rate: 5e-06
09/07 07:41:02 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 07:41:07 PM: Update 23017: task mnli, batch 17 (22984): accuracy: 0.8260, mnli_loss: 0.4588
09/07 07:41:17 PM: Update 23058: task mnli, batch 58 (23025): accuracy: 0.8312, mnli_loss: 0.4499
09/07 07:41:27 PM: Update 23085: task mnli, batch 85 (23052): accuracy: 0.8317, mnli_loss: 0.4499
09/07 07:41:37 PM: Update 23127: task mnli, batch 127 (23094): accuracy: 0.8438, mnli_loss: 0.4240
09/07 07:41:42 PM: Update 23146: task winograd-coreference, batch 1 (34): f1: 0.4286, acc: 0.3333, winograd-coreference_loss: 0.7449
09/07 07:41:47 PM: Update 23166: task mnli, batch 165 (23132): accuracy: 0.8413, mnli_loss: 0.4210
09/07 07:41:58 PM: Update 23208: task mnli, batch 207 (23174): accuracy: 0.8433, mnli_loss: 0.4123
09/07 07:42:08 PM: Update 23249: task mnli, batch 248 (23215): accuracy: 0.8434, mnli_loss: 0.4095
09/07 07:42:18 PM: Update 23289: task mnli, batch 288 (23255): accuracy: 0.8399, mnli_loss: 0.4136
09/07 07:42:28 PM: Update 23328: task mnli, batch 327 (23294): accuracy: 0.8409, mnli_loss: 0.4114
09/07 07:42:38 PM: Update 23369: task mnli, batch 368 (23335): accuracy: 0.8402, mnli_loss: 0.4104
09/07 07:42:48 PM: Update 23412: task mnli, batch 411 (23378): accuracy: 0.8418, mnli_loss: 0.4071
09/07 07:42:58 PM: Update 23452: task mnli, batch 451 (23418): accuracy: 0.8429, mnli_loss: 0.4062
09/07 07:43:08 PM: Update 23493: task mnli, batch 492 (23459): accuracy: 0.8422, mnli_loss: 0.4076
09/07 07:43:18 PM: Update 23524: task mnli, batch 523 (23490): accuracy: 0.8420, mnli_loss: 0.4069
09/07 07:43:28 PM: Update 23563: task mnli, batch 562 (23529): accuracy: 0.8420, mnli_loss: 0.4063
09/07 07:43:39 PM: Update 23604: task mnli, batch 603 (23570): accuracy: 0.8415, mnli_loss: 0.4065
09/07 07:43:49 PM: Update 23644: task mnli, batch 643 (23610): accuracy: 0.8404, mnli_loss: 0.4093
09/07 07:43:59 PM: Update 23685: task mnli, batch 684 (23651): accuracy: 0.8410, mnli_loss: 0.4075
09/07 07:44:09 PM: Update 23726: task mnli, batch 725 (23692): accuracy: 0.8411, mnli_loss: 0.4070
09/07 07:44:19 PM: Update 23771: task mnli, batch 770 (23737): accuracy: 0.8414, mnli_loss: 0.4068
09/07 07:44:29 PM: Update 23813: task mnli, batch 812 (23779): accuracy: 0.8417, mnli_loss: 0.4051
09/07 07:44:39 PM: Update 23853: task mnli, batch 852 (23819): accuracy: 0.8420, mnli_loss: 0.4048
09/07 07:44:50 PM: Update 23894: task mnli, batch 893 (23860): accuracy: 0.8417, mnli_loss: 0.4066
09/07 07:45:00 PM: Update 23923: task mnli, batch 922 (23889): accuracy: 0.8413, mnli_loss: 0.4068
09/07 07:45:10 PM: Update 23965: task mnli, batch 964 (23931): accuracy: 0.8408, mnli_loss: 0.4074
09/07 07:45:18 PM: ***** Step 24000 / Validation 24 *****
09/07 07:45:18 PM: mnli: trained on 999 batches, 0.061 epochs
09/07 07:45:18 PM: winograd-coreference: trained on 1 batches, 0.042 epochs
09/07 07:45:18 PM: Validating...
09/07 07:45:20 PM: Evaluate: task mnli, batch 24 (209): accuracy: 0.8299, mnli_loss: 0.4635
09/07 07:45:30 PM: Evaluate: task mnli, batch 175 (209): accuracy: 0.8167, mnli_loss: 0.4863
09/07 07:45:32 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.3158, acc: 0.4583, winograd-coreference_loss: 0.6833
09/07 07:45:32 PM: Updating LR scheduler:
09/07 07:45:32 PM: 	Best result seen so far for macro_avg: 0.720
09/07 07:45:32 PM: 	# validation passes without improvement: 3
09/07 07:45:32 PM: mnli_loss: training: 0.408030 validation: 0.496325
09/07 07:45:32 PM: winograd-coreference_loss: training: 0.744949 validation: 0.686402
09/07 07:45:32 PM: macro_avg: validation: 0.684146
09/07 07:45:32 PM: micro_avg: validation: 0.805447
09/07 07:45:32 PM: mnli_accuracy: training: 0.840514 validation: 0.810600
09/07 07:45:32 PM: winograd-coreference_f1: training: 0.428571 validation: 0.425000
09/07 07:45:32 PM: winograd-coreference_acc: training: 0.333333 validation: 0.557692
09/07 07:45:32 PM: Global learning rate: 5e-06
09/07 07:45:32 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 07:45:40 PM: Update 24027: task mnli, batch 27 (23993): accuracy: 0.8488, mnli_loss: 0.4007
09/07 07:45:50 PM: Update 24067: task mnli, batch 67 (24033): accuracy: 0.8439, mnli_loss: 0.4121
09/07 07:46:00 PM: Update 24109: task mnli, batch 109 (24075): accuracy: 0.8425, mnli_loss: 0.4055
09/07 07:46:10 PM: Update 24150: task mnli, batch 150 (24116): accuracy: 0.8439, mnli_loss: 0.4033
09/07 07:46:21 PM: Update 24192: task mnli, batch 192 (24158): accuracy: 0.8435, mnli_loss: 0.4055
09/07 07:46:31 PM: Update 24231: task mnli, batch 231 (24197): accuracy: 0.8456, mnli_loss: 0.4040
09/07 07:46:41 PM: Update 24270: task mnli, batch 270 (24236): accuracy: 0.8437, mnli_loss: 0.4055
09/07 07:46:51 PM: Update 24311: task mnli, batch 311 (24277): accuracy: 0.8475, mnli_loss: 0.3976
09/07 07:47:01 PM: Update 24344: task mnli, batch 344 (24310): accuracy: 0.8492, mnli_loss: 0.3957
09/07 07:47:11 PM: Update 24385: task mnli, batch 385 (24351): accuracy: 0.8460, mnli_loss: 0.4025
09/07 07:47:21 PM: Update 24426: task mnli, batch 426 (24392): accuracy: 0.8463, mnli_loss: 0.4014
09/07 07:47:31 PM: Update 24466: task mnli, batch 466 (24432): accuracy: 0.8467, mnli_loss: 0.4007
09/07 07:47:41 PM: Update 24507: task mnli, batch 507 (24473): accuracy: 0.8467, mnli_loss: 0.4003
09/07 07:47:51 PM: Update 24547: task mnli, batch 547 (24513): accuracy: 0.8441, mnli_loss: 0.4035
09/07 07:48:01 PM: Update 24588: task mnli, batch 588 (24554): accuracy: 0.8424, mnli_loss: 0.4067
09/07 07:48:12 PM: Update 24630: task mnli, batch 630 (24596): accuracy: 0.8451, mnli_loss: 0.4023
09/07 07:48:22 PM: Update 24670: task mnli, batch 670 (24636): accuracy: 0.8438, mnli_loss: 0.4067
09/07 07:48:32 PM: Update 24711: task mnli, batch 711 (24677): accuracy: 0.8441, mnli_loss: 0.4070
09/07 07:48:45 PM: Update 24751: task mnli, batch 751 (24717): accuracy: 0.8445, mnli_loss: 0.4069
09/07 07:48:55 PM: Update 24791: task mnli, batch 791 (24757): accuracy: 0.8439, mnli_loss: 0.4075
09/07 07:49:06 PM: Update 24832: task mnli, batch 832 (24798): accuracy: 0.8447, mnli_loss: 0.4051
09/07 07:49:16 PM: Update 24873: task mnli, batch 873 (24839): accuracy: 0.8452, mnli_loss: 0.4045
09/07 07:49:26 PM: Update 24914: task mnli, batch 914 (24880): accuracy: 0.8445, mnli_loss: 0.4057
09/07 07:49:36 PM: Update 24956: task mnli, batch 956 (24922): accuracy: 0.8444, mnli_loss: 0.4058
09/07 07:49:46 PM: Update 24997: task mnli, batch 997 (24963): accuracy: 0.8448, mnli_loss: 0.4054
09/07 07:49:47 PM: ***** Step 25000 / Validation 25 *****
09/07 07:49:47 PM: mnli: trained on 1000 batches, 0.061 epochs
09/07 07:49:47 PM: winograd-coreference: trained on 0 batches, 0.000 epochs
09/07 07:49:47 PM: Validating...
09/07 07:49:56 PM: Evaluate: task mnli, batch 144 (209): accuracy: 0.8113, mnli_loss: 0.4951
09/07 07:50:00 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.3333, acc: 0.5000, winograd-coreference_loss: 0.6837
09/07 07:50:01 PM: Updating LR scheduler:
09/07 07:50:01 PM: 	Best result seen so far for macro_avg: 0.720
09/07 07:50:01 PM: 	# validation passes without improvement: 4
09/07 07:50:01 PM: mnli_loss: training: 0.405346 validation: 0.498602
09/07 07:50:01 PM: winograd-coreference_loss: training: 0.000000 validation: 0.686310
09/07 07:50:01 PM: macro_avg: validation: 0.694362
09/07 07:50:01 PM: micro_avg: validation: 0.807014
09/07 07:50:01 PM: mnli_accuracy: training: 0.844813 validation: 0.811800
09/07 07:50:01 PM: winograd-coreference_f1: validation: 0.435897
09/07 07:50:01 PM: winograd-coreference_acc: validation: 0.576923
09/07 07:50:01 PM: Global learning rate: 5e-06
09/07 07:50:01 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 07:50:06 PM: Update 25018: task mnli, batch 18 (24984): accuracy: 0.8218, mnli_loss: 0.4348
09/07 07:50:16 PM: Update 25059: task mnli, batch 59 (25025): accuracy: 0.8249, mnli_loss: 0.4314
09/07 07:50:27 PM: Update 25099: task mnli, batch 99 (25065): accuracy: 0.8333, mnli_loss: 0.4223
09/07 07:50:37 PM: Update 25139: task mnli, batch 139 (25105): accuracy: 0.8369, mnli_loss: 0.4120
09/07 07:50:47 PM: Update 25172: task mnli, batch 172 (25138): accuracy: 0.8408, mnli_loss: 0.4102
09/07 07:50:57 PM: Update 25213: task mnli, batch 213 (25179): accuracy: 0.8407, mnli_loss: 0.4125
09/07 07:51:07 PM: Update 25253: task mnli, batch 253 (25219): accuracy: 0.8371, mnli_loss: 0.4149
09/07 07:51:17 PM: Update 25294: task mnli, batch 294 (25260): accuracy: 0.8422, mnli_loss: 0.4034
09/07 07:51:27 PM: Update 25336: task mnli, batch 336 (25302): accuracy: 0.8412, mnli_loss: 0.4052
09/07 07:51:38 PM: Update 25378: task mnli, batch 378 (25344): accuracy: 0.8416, mnli_loss: 0.4020
09/07 07:51:48 PM: Update 25419: task mnli, batch 419 (25385): accuracy: 0.8428, mnli_loss: 0.4027
09/07 07:51:58 PM: Update 25459: task mnli, batch 459 (25425): accuracy: 0.8418, mnli_loss: 0.4076
09/07 07:52:08 PM: Update 25498: task mnli, batch 498 (25464): accuracy: 0.8399, mnli_loss: 0.4101
09/07 07:52:18 PM: Update 25538: task mnli, batch 538 (25504): accuracy: 0.8388, mnli_loss: 0.4121
09/07 07:52:28 PM: Update 25580: task mnli, batch 580 (25546): accuracy: 0.8391, mnli_loss: 0.4104
09/07 07:52:38 PM: Update 25612: task mnli, batch 612 (25578): accuracy: 0.8389, mnli_loss: 0.4125
09/07 07:52:48 PM: Update 25654: task mnli, batch 654 (25620): accuracy: 0.8396, mnli_loss: 0.4128
09/07 07:52:59 PM: Update 25694: task mnli, batch 694 (25660): accuracy: 0.8398, mnli_loss: 0.4138
09/07 07:53:09 PM: Update 25735: task mnli, batch 735 (25701): accuracy: 0.8399, mnli_loss: 0.4137
09/07 07:53:19 PM: Update 25776: task mnli, batch 776 (25742): accuracy: 0.8400, mnli_loss: 0.4128
09/07 07:53:29 PM: Update 25815: task mnli, batch 815 (25781): accuracy: 0.8401, mnli_loss: 0.4122
09/07 07:53:39 PM: Update 25856: task mnli, batch 856 (25822): accuracy: 0.8414, mnli_loss: 0.4101
09/07 07:53:49 PM: Update 25896: task mnli, batch 896 (25862): accuracy: 0.8413, mnli_loss: 0.4102
09/07 07:53:59 PM: Update 25938: task mnli, batch 938 (25904): accuracy: 0.8412, mnli_loss: 0.4095
09/07 07:54:10 PM: Update 25979: task mnli, batch 979 (25945): accuracy: 0.8411, mnli_loss: 0.4097
09/07 07:54:15 PM: ***** Step 26000 / Validation 26 *****
09/07 07:54:15 PM: mnli: trained on 1000 batches, 0.061 epochs
09/07 07:54:15 PM: winograd-coreference: trained on 0 batches, 0.000 epochs
09/07 07:54:15 PM: Validating...
09/07 07:54:20 PM: Evaluate: task mnli, batch 71 (209): accuracy: 0.8251, mnli_loss: 0.4724
09/07 07:54:29 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.3158, acc: 0.4583, winograd-coreference_loss: 0.6864
09/07 07:54:29 PM: Updating LR scheduler:
09/07 07:54:29 PM: 	Best result seen so far for macro_avg: 0.720
09/07 07:54:29 PM: 	# validation passes without improvement: 0
09/07 07:54:29 PM: mnli_loss: training: 0.410309 validation: 0.490600
09/07 07:54:29 PM: winograd-coreference_loss: training: 0.000000 validation: 0.686934
09/07 07:54:29 PM: macro_avg: validation: 0.696962
09/07 07:54:29 PM: micro_avg: validation: 0.812108
09/07 07:54:29 PM: mnli_accuracy: training: 0.840769 validation: 0.817000
09/07 07:54:29 PM: winograd-coreference_f1: validation: 0.450000
09/07 07:54:29 PM: winograd-coreference_acc: validation: 0.576923
09/07 07:54:29 PM: Global learning rate: 2.5e-06
09/07 07:54:29 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 07:54:30 PM: Update 26001: task mnli, batch 1 (25967): accuracy: 0.8333, mnli_loss: 0.6492
09/07 07:54:40 PM: Update 26035: task mnli, batch 35 (26001): accuracy: 0.8305, mnli_loss: 0.4297
09/07 07:54:47 PM: Update 26066: task winograd-coreference, batch 1 (35): f1: 0.4615, acc: 0.4167, winograd-coreference_loss: 0.7236
09/07 07:54:50 PM: Update 26077: task mnli, batch 76 (26042): accuracy: 0.8431, mnli_loss: 0.4019
09/07 07:55:01 PM: Update 26116: task mnli, batch 115 (26081): accuracy: 0.8347, mnli_loss: 0.4162
09/07 07:55:11 PM: Update 26157: task mnli, batch 156 (26122): accuracy: 0.8378, mnli_loss: 0.4091
09/07 07:55:21 PM: Update 26199: task mnli, batch 198 (26164): accuracy: 0.8417, mnli_loss: 0.4062
09/07 07:55:31 PM: Update 26241: task mnli, batch 240 (26206): accuracy: 0.8432, mnli_loss: 0.4043
09/07 07:55:41 PM: Update 26282: task mnli, batch 281 (26247): accuracy: 0.8416, mnli_loss: 0.4060
09/07 07:55:51 PM: Update 26322: task mnli, batch 321 (26287): accuracy: 0.8415, mnli_loss: 0.4060
09/07 07:56:01 PM: Update 26363: task mnli, batch 362 (26328): accuracy: 0.8423, mnli_loss: 0.4046
09/07 07:56:11 PM: Update 26403: task mnli, batch 402 (26368): accuracy: 0.8413, mnli_loss: 0.4056
09/07 07:56:21 PM: Update 26436: task mnli, batch 435 (26401): accuracy: 0.8432, mnli_loss: 0.4023
09/07 07:56:32 PM: Update 26476: task mnli, batch 475 (26441): accuracy: 0.8443, mnli_loss: 0.4004
09/07 07:56:42 PM: Update 26516: task mnli, batch 515 (26481): accuracy: 0.8449, mnli_loss: 0.4010
09/07 07:56:52 PM: Update 26558: task mnli, batch 557 (26523): accuracy: 0.8470, mnli_loss: 0.3958
09/07 07:57:02 PM: Update 26599: task mnli, batch 598 (26564): accuracy: 0.8484, mnli_loss: 0.3933
09/07 07:57:05 PM: Update 26612: task winograd-coreference, batch 2 (36): f1: 0.5965, acc: 0.5208, winograd-coreference_loss: 0.6968
09/07 07:57:12 PM: Update 26641: task mnli, batch 639 (26605): accuracy: 0.8491, mnli_loss: 0.3923
09/07 07:57:22 PM: Update 26682: task mnli, batch 680 (26646): accuracy: 0.8493, mnli_loss: 0.3925
09/07 07:57:33 PM: Update 26723: task mnli, batch 721 (26687): accuracy: 0.8490, mnli_loss: 0.3918
09/07 07:57:43 PM: Update 26764: task mnli, batch 762 (26728): accuracy: 0.8495, mnli_loss: 0.3910
09/07 07:57:53 PM: Update 26804: task mnli, batch 802 (26768): accuracy: 0.8494, mnli_loss: 0.3919
09/07 07:57:56 PM: Update 26818: task winograd-coreference, batch 3 (37): f1: 0.5682, acc: 0.4722, winograd-coreference_loss: 0.7099
09/07 07:58:04 PM: Update 26839: task mnli, batch 836 (26802): accuracy: 0.8498, mnli_loss: 0.3911
09/07 07:58:15 PM: Update 26878: task mnli, batch 875 (26841): accuracy: 0.8496, mnli_loss: 0.3916
09/07 07:58:25 PM: Update 26919: task mnli, batch 916 (26882): accuracy: 0.8498, mnli_loss: 0.3914
09/07 07:58:28 PM: Update 26931: task winograd-coreference, batch 4 (38): f1: 0.5556, acc: 0.5000, winograd-coreference_loss: 0.7077
09/07 07:58:35 PM: Update 26960: task mnli, batch 956 (26922): accuracy: 0.8494, mnli_loss: 0.3921
09/07 07:58:45 PM: ***** Step 27000 / Validation 27 *****
09/07 07:58:45 PM: mnli: trained on 996 batches, 0.061 epochs
09/07 07:58:45 PM: winograd-coreference: trained on 4 batches, 0.167 epochs
09/07 07:58:45 PM: Validating...
09/07 07:58:45 PM: Evaluate: task mnli, batch 4 (209): accuracy: 0.7708, mnli_loss: 0.5068
09/07 07:58:55 PM: Evaluate: task mnli, batch 158 (209): accuracy: 0.8220, mnli_loss: 0.4855
09/07 07:58:58 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.4348, acc: 0.4583, winograd-coreference_loss: 0.6930
09/07 07:58:59 PM: Updating LR scheduler:
09/07 07:58:59 PM: 	Best result seen so far for macro_avg: 0.720
09/07 07:58:59 PM: 	# validation passes without improvement: 1
09/07 07:58:59 PM: mnli_loss: training: 0.392123 validation: 0.495102
09/07 07:58:59 PM: winograd-coreference_loss: training: 0.707691 validation: 0.690900
09/07 07:58:59 PM: macro_avg: validation: 0.678731
09/07 07:58:59 PM: micro_avg: validation: 0.813284
09/07 07:58:59 PM: mnli_accuracy: training: 0.849288 validation: 0.819000
09/07 07:58:59 PM: winograd-coreference_f1: training: 0.555556 validation: 0.466667
09/07 07:58:59 PM: winograd-coreference_acc: training: 0.500000 validation: 0.538462
09/07 07:58:59 PM: Global learning rate: 2.5e-06
09/07 07:58:59 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 07:59:05 PM: Update 27023: task mnli, batch 23 (26985): accuracy: 0.8370, mnli_loss: 0.3839
09/07 07:59:15 PM: Update 27065: task mnli, batch 65 (27027): accuracy: 0.8378, mnli_loss: 0.3905
09/07 07:59:24 PM: Update 27097: task winograd-coreference, batch 1 (39): f1: 0.2857, acc: 0.3750, winograd-coreference_loss: 0.7519
09/07 07:59:26 PM: Update 27106: task mnli, batch 105 (27067): accuracy: 0.8472, mnli_loss: 0.3777
09/07 07:59:36 PM: Update 27147: task mnli, batch 146 (27108): accuracy: 0.8582, mnli_loss: 0.3667
09/07 07:59:46 PM: Update 27188: task mnli, batch 187 (27149): accuracy: 0.8605, mnli_loss: 0.3622
09/07 07:59:56 PM: Update 27228: task mnli, batch 227 (27189): accuracy: 0.8594, mnli_loss: 0.3629
09/07 08:00:06 PM: Update 27260: task mnli, batch 259 (27221): accuracy: 0.8574, mnli_loss: 0.3664
09/07 08:00:16 PM: Update 27302: task mnli, batch 301 (27263): accuracy: 0.8571, mnli_loss: 0.3655
09/07 08:00:26 PM: Update 27343: task mnli, batch 342 (27304): accuracy: 0.8555, mnli_loss: 0.3678
09/07 08:00:36 PM: Update 27385: task mnli, batch 384 (27346): accuracy: 0.8550, mnli_loss: 0.3693
09/07 08:00:47 PM: Update 27428: task mnli, batch 427 (27389): accuracy: 0.8552, mnli_loss: 0.3663
09/07 08:00:57 PM: Update 27468: task mnli, batch 467 (27429): accuracy: 0.8529, mnli_loss: 0.3710
09/07 08:01:07 PM: Update 27508: task mnli, batch 507 (27469): accuracy: 0.8509, mnli_loss: 0.3759
09/07 08:01:17 PM: Update 27548: task mnli, batch 547 (27509): accuracy: 0.8513, mnli_loss: 0.3769
09/07 08:01:27 PM: Update 27588: task mnli, batch 587 (27549): accuracy: 0.8515, mnli_loss: 0.3772
09/07 08:01:37 PM: Update 27629: task mnli, batch 628 (27590): accuracy: 0.8527, mnli_loss: 0.3770
09/07 08:01:48 PM: Update 27669: task mnli, batch 668 (27630): accuracy: 0.8527, mnli_loss: 0.3759
09/07 08:01:58 PM: Update 27700: task mnli, batch 699 (27661): accuracy: 0.8524, mnli_loss: 0.3761
09/07 08:02:08 PM: Update 27741: task mnli, batch 740 (27702): accuracy: 0.8519, mnli_loss: 0.3770
09/07 08:02:18 PM: Update 27781: task mnli, batch 780 (27742): accuracy: 0.8522, mnli_loss: 0.3763
09/07 08:02:28 PM: Update 27823: task mnli, batch 822 (27784): accuracy: 0.8514, mnli_loss: 0.3783
09/07 08:02:38 PM: Update 27864: task mnli, batch 863 (27825): accuracy: 0.8511, mnli_loss: 0.3797
09/07 08:02:49 PM: Update 27904: task mnli, batch 903 (27865): accuracy: 0.8516, mnli_loss: 0.3798
09/07 08:02:57 PM: Update 27938: task winograd-coreference, batch 2 (40): f1: 0.2439, acc: 0.3542, winograd-coreference_loss: 0.7476
09/07 08:02:59 PM: Update 27945: task mnli, batch 943 (27905): accuracy: 0.8515, mnli_loss: 0.3798
09/07 08:03:09 PM: Update 27987: task mnli, batch 985 (27947): accuracy: 0.8510, mnli_loss: 0.3803
09/07 08:03:12 PM: ***** Step 28000 / Validation 28 *****
09/07 08:03:12 PM: mnli: trained on 998 batches, 0.061 epochs
09/07 08:03:12 PM: winograd-coreference: trained on 2 batches, 0.083 epochs
09/07 08:03:12 PM: Validating...
09/07 08:03:19 PM: Evaluate: task mnli, batch 106 (209): accuracy: 0.8176, mnli_loss: 0.4970
09/07 08:03:26 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.2500, acc: 0.5000, winograd-coreference_loss: 0.6671
09/07 08:03:26 PM: Updating LR scheduler:
09/07 08:03:26 PM: 	Best result seen so far for macro_avg: 0.720
09/07 08:03:26 PM: 	# validation passes without improvement: 2
09/07 08:03:26 PM: mnli_loss: training: 0.380844 validation: 0.509966
09/07 08:03:26 PM: winograd-coreference_loss: training: 0.747614 validation: 0.680237
09/07 08:03:26 PM: macro_avg: validation: 0.681538
09/07 08:03:26 PM: micro_avg: validation: 0.809561
09/07 08:03:26 PM: mnli_accuracy: training: 0.850810 validation: 0.815000
09/07 08:03:26 PM: winograd-coreference_f1: training: 0.243902 validation: 0.318841
09/07 08:03:26 PM: winograd-coreference_acc: training: 0.354167 validation: 0.548077
09/07 08:03:26 PM: Global learning rate: 2.5e-06
09/07 08:03:26 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 08:03:29 PM: Update 28008: task mnli, batch 8 (27968): accuracy: 0.8750, mnli_loss: 0.3231
09/07 08:03:39 PM: Update 28050: task mnli, batch 50 (28010): accuracy: 0.8492, mnli_loss: 0.3886
09/07 08:03:49 PM: Update 28091: task mnli, batch 91 (28051): accuracy: 0.8507, mnli_loss: 0.3846
09/07 08:03:59 PM: Update 28123: task mnli, batch 123 (28083): accuracy: 0.8444, mnli_loss: 0.4054
09/07 08:04:10 PM: Update 28163: task mnli, batch 163 (28123): accuracy: 0.8463, mnli_loss: 0.4066
09/07 08:04:20 PM: Update 28204: task mnli, batch 204 (28164): accuracy: 0.8462, mnli_loss: 0.4054
09/07 08:04:30 PM: Update 28243: task mnli, batch 243 (28203): accuracy: 0.8479, mnli_loss: 0.3994
09/07 08:04:40 PM: Update 28284: task mnli, batch 284 (28244): accuracy: 0.8486, mnli_loss: 0.3945
09/07 08:04:50 PM: Update 28326: task mnli, batch 326 (28286): accuracy: 0.8488, mnli_loss: 0.3933
09/07 08:05:00 PM: Update 28366: task mnli, batch 366 (28326): accuracy: 0.8518, mnli_loss: 0.3900
09/07 08:05:10 PM: Update 28407: task mnli, batch 407 (28367): accuracy: 0.8522, mnli_loss: 0.3894
09/07 08:05:20 PM: Update 28450: task mnli, batch 450 (28410): accuracy: 0.8520, mnli_loss: 0.3899
09/07 08:05:31 PM: Update 28490: task mnli, batch 490 (28450): accuracy: 0.8525, mnli_loss: 0.3896
09/07 08:05:41 PM: Update 28520: task mnli, batch 520 (28480): accuracy: 0.8525, mnli_loss: 0.3893
09/07 08:05:51 PM: Update 28562: task mnli, batch 562 (28522): accuracy: 0.8533, mnli_loss: 0.3856
09/07 08:06:01 PM: Update 28602: task mnli, batch 602 (28562): accuracy: 0.8528, mnli_loss: 0.3856
09/07 08:06:11 PM: Update 28645: task mnli, batch 645 (28605): accuracy: 0.8535, mnli_loss: 0.3853
09/07 08:06:21 PM: Update 28687: task mnli, batch 687 (28647): accuracy: 0.8545, mnli_loss: 0.3833
09/07 08:06:31 PM: Update 28727: task mnli, batch 727 (28687): accuracy: 0.8546, mnli_loss: 0.3830
09/07 08:06:42 PM: Update 28768: task mnli, batch 768 (28728): accuracy: 0.8531, mnli_loss: 0.3846
09/07 08:06:52 PM: Update 28809: task mnli, batch 809 (28769): accuracy: 0.8537, mnli_loss: 0.3843
09/07 08:06:56 PM: Update 28825: task winograd-coreference, batch 1 (41): f1: 0.1250, acc: 0.4167, winograd-coreference_loss: 0.7256
09/07 08:07:02 PM: Update 28849: task mnli, batch 848 (28808): accuracy: 0.8535, mnli_loss: 0.3849
09/07 08:07:12 PM: Update 28889: task mnli, batch 888 (28848): accuracy: 0.8533, mnli_loss: 0.3852
09/07 08:07:24 PM: Update 28928: task mnli, batch 927 (28887): accuracy: 0.8533, mnli_loss: 0.3864
09/07 08:07:34 PM: Update 28967: task mnli, batch 966 (28926): accuracy: 0.8535, mnli_loss: 0.3863
09/07 08:07:43 PM: ***** Step 29000 / Validation 29 *****
09/07 08:07:43 PM: mnli: trained on 999 batches, 0.061 epochs
09/07 08:07:43 PM: winograd-coreference: trained on 1 batches, 0.042 epochs
09/07 08:07:43 PM: Validating...
09/07 08:07:44 PM: Evaluate: task mnli, batch 27 (209): accuracy: 0.8225, mnli_loss: 0.4703
09/07 08:07:54 PM: Evaluate: task mnli, batch 176 (209): accuracy: 0.8182, mnli_loss: 0.4980
09/07 08:07:57 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.1333, acc: 0.4583, winograd-coreference_loss: 0.6661
09/07 08:07:57 PM: Updating LR scheduler:
09/07 08:07:57 PM: 	Best result seen so far for macro_avg: 0.720
09/07 08:07:57 PM: 	# validation passes without improvement: 3
09/07 08:07:57 PM: mnli_loss: training: 0.384395 validation: 0.507933
09/07 08:07:57 PM: winograd-coreference_loss: training: 0.725554 validation: 0.680964
09/07 08:07:57 PM: macro_avg: validation: 0.676531
09/07 08:07:57 PM: micro_avg: validation: 0.808973
09/07 08:07:57 PM: mnli_accuracy: training: 0.853707 validation: 0.814600
09/07 08:07:57 PM: winograd-coreference_f1: training: 0.125000 validation: 0.294118
09/07 08:07:57 PM: winograd-coreference_acc: training: 0.416667 validation: 0.538462
09/07 08:07:57 PM: Global learning rate: 2.5e-06
09/07 08:07:57 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 08:08:04 PM: Update 29027: task mnli, batch 27 (28986): accuracy: 0.8426, mnli_loss: 0.3977
09/07 08:08:15 PM: Update 29069: task mnli, batch 69 (29028): accuracy: 0.8647, mnli_loss: 0.3517
09/07 08:08:25 PM: Update 29110: task mnli, batch 110 (29069): accuracy: 0.8534, mnli_loss: 0.3778
09/07 08:08:35 PM: Update 29149: task mnli, batch 149 (29108): accuracy: 0.8579, mnli_loss: 0.3738
09/07 08:08:45 PM: Update 29190: task mnli, batch 190 (29149): accuracy: 0.8625, mnli_loss: 0.3655
09/07 08:08:55 PM: Update 29229: task mnli, batch 229 (29188): accuracy: 0.8601, mnli_loss: 0.3712
09/07 08:09:05 PM: Update 29270: task mnli, batch 270 (29229): accuracy: 0.8573, mnli_loss: 0.3744
09/07 08:09:15 PM: Update 29311: task mnli, batch 311 (29270): accuracy: 0.8591, mnli_loss: 0.3720
09/07 08:09:26 PM: Update 29345: task mnli, batch 345 (29304): accuracy: 0.8590, mnli_loss: 0.3737
09/07 08:09:36 PM: Update 29386: task mnli, batch 386 (29345): accuracy: 0.8571, mnli_loss: 0.3764
09/07 08:09:46 PM: Update 29426: task mnli, batch 426 (29385): accuracy: 0.8548, mnli_loss: 0.3790
09/07 08:09:56 PM: Update 29468: task mnli, batch 468 (29427): accuracy: 0.8548, mnli_loss: 0.3798
09/07 08:10:06 PM: Update 29508: task mnli, batch 508 (29467): accuracy: 0.8542, mnli_loss: 0.3818
09/07 08:10:16 PM: Update 29549: task mnli, batch 549 (29508): accuracy: 0.8527, mnli_loss: 0.3850
09/07 08:10:26 PM: Update 29590: task mnli, batch 590 (29549): accuracy: 0.8525, mnli_loss: 0.3874
09/07 08:10:36 PM: Update 29630: task mnli, batch 630 (29589): accuracy: 0.8533, mnli_loss: 0.3863
09/07 08:10:46 PM: Update 29669: task mnli, batch 669 (29628): accuracy: 0.8511, mnli_loss: 0.3901
09/07 08:10:56 PM: Update 29710: task mnli, batch 710 (29669): accuracy: 0.8505, mnli_loss: 0.3901
09/07 08:11:06 PM: Update 29751: task mnli, batch 751 (29710): accuracy: 0.8514, mnli_loss: 0.3883
09/07 08:11:17 PM: Update 29784: task mnli, batch 784 (29743): accuracy: 0.8514, mnli_loss: 0.3886
09/07 08:11:27 PM: Update 29825: task mnli, batch 825 (29784): accuracy: 0.8514, mnli_loss: 0.3897
09/07 08:11:37 PM: Update 29865: task mnli, batch 865 (29824): accuracy: 0.8508, mnli_loss: 0.3900
09/07 08:11:47 PM: Update 29908: task mnli, batch 908 (29867): accuracy: 0.8508, mnli_loss: 0.3893
09/07 08:11:57 PM: Update 29948: task mnli, batch 948 (29907): accuracy: 0.8505, mnli_loss: 0.3894
09/07 08:12:07 PM: Update 29988: task mnli, batch 988 (29947): accuracy: 0.8507, mnli_loss: 0.3888
09/07 08:12:10 PM: ***** Step 30000 / Validation 30 *****
09/07 08:12:10 PM: mnli: trained on 1000 batches, 0.061 epochs
09/07 08:12:10 PM: winograd-coreference: trained on 0 batches, 0.000 epochs
09/07 08:12:10 PM: Validating...
09/07 08:12:17 PM: Evaluate: task mnli, batch 103 (209): accuracy: 0.8155, mnli_loss: 0.4936
09/07 08:12:25 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.1333, acc: 0.4583, winograd-coreference_loss: 0.6654
09/07 08:12:25 PM: Updating LR scheduler:
09/07 08:12:25 PM: 	Best result seen so far for macro_avg: 0.720
09/07 08:12:25 PM: 	# validation passes without improvement: 4
09/07 08:12:25 PM: mnli_loss: training: 0.388692 validation: 0.509373
09/07 08:12:25 PM: winograd-coreference_loss: training: 0.000000 validation: 0.680961
09/07 08:12:25 PM: macro_avg: validation: 0.675631
09/07 08:12:25 PM: micro_avg: validation: 0.807210
09/07 08:12:25 PM: mnli_accuracy: training: 0.850525 validation: 0.812800
09/07 08:12:25 PM: winograd-coreference_f1: validation: 0.294118
09/07 08:12:25 PM: winograd-coreference_acc: validation: 0.538462
09/07 08:12:25 PM: Global learning rate: 2.5e-06
09/07 08:12:25 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 08:12:27 PM: Update 30007: task mnli, batch 7 (29966): accuracy: 0.8512, mnli_loss: 0.3586
09/07 08:12:37 PM: Update 30046: task mnli, batch 46 (30005): accuracy: 0.8560, mnli_loss: 0.3665
09/07 08:12:48 PM: Update 30088: task mnli, batch 88 (30047): accuracy: 0.8584, mnli_loss: 0.3696
09/07 08:12:58 PM: Update 30127: task mnli, batch 127 (30086): accuracy: 0.8537, mnli_loss: 0.3748
09/07 08:13:08 PM: Update 30169: task mnli, batch 169 (30128): accuracy: 0.8602, mnli_loss: 0.3601
09/07 08:13:18 PM: Update 30196: task mnli, batch 196 (30155): accuracy: 0.8578, mnli_loss: 0.3698
09/07 08:13:28 PM: Update 30236: task mnli, batch 236 (30195): accuracy: 0.8545, mnli_loss: 0.3773
09/07 08:13:30 PM: Update 30245: task winograd-coreference, batch 1 (42): f1: 0.4211, acc: 0.5417, winograd-coreference_loss: 0.7012
09/07 08:13:38 PM: Update 30275: task mnli, batch 274 (30233): accuracy: 0.8544, mnli_loss: 0.3788
09/07 08:13:49 PM: Update 30316: task mnli, batch 315 (30274): accuracy: 0.8545, mnli_loss: 0.3790
09/07 08:13:59 PM: Update 30357: task mnli, batch 356 (30315): accuracy: 0.8537, mnli_loss: 0.3809
09/07 08:14:09 PM: Update 30398: task mnli, batch 397 (30356): accuracy: 0.8528, mnli_loss: 0.3826
09/07 08:14:19 PM: Update 30438: task mnli, batch 437 (30396): accuracy: 0.8517, mnli_loss: 0.3844
09/07 08:14:29 PM: Update 30478: task mnli, batch 477 (30436): accuracy: 0.8517, mnli_loss: 0.3841
09/07 08:14:39 PM: Update 30517: task mnli, batch 516 (30475): accuracy: 0.8513, mnli_loss: 0.3856
09/07 08:14:50 PM: Update 30558: task mnli, batch 557 (30516): accuracy: 0.8513, mnli_loss: 0.3870
09/07 08:15:00 PM: Update 30596: task mnli, batch 595 (30554): accuracy: 0.8510, mnli_loss: 0.3890
09/07 08:15:10 PM: Update 30627: task mnli, batch 626 (30585): accuracy: 0.8511, mnli_loss: 0.3883
09/07 08:15:20 PM: Update 30670: task mnli, batch 669 (30628): accuracy: 0.8514, mnli_loss: 0.3881
09/07 08:15:30 PM: Update 30711: task mnli, batch 710 (30669): accuracy: 0.8515, mnli_loss: 0.3881
09/07 08:15:41 PM: Update 30752: task mnli, batch 751 (30710): accuracy: 0.8517, mnli_loss: 0.3878
09/07 08:15:51 PM: Update 30791: task mnli, batch 790 (30749): accuracy: 0.8512, mnli_loss: 0.3892
09/07 08:16:01 PM: Update 30833: task mnli, batch 832 (30791): accuracy: 0.8513, mnli_loss: 0.3886
09/07 08:16:11 PM: Update 30873: task mnli, batch 872 (30831): accuracy: 0.8509, mnli_loss: 0.3885
09/07 08:16:21 PM: Update 30912: task mnli, batch 911 (30870): accuracy: 0.8517, mnli_loss: 0.3862
09/07 08:16:32 PM: Update 30952: task mnli, batch 951 (30910): accuracy: 0.8519, mnli_loss: 0.3855
09/07 08:16:42 PM: Update 30994: task mnli, batch 993 (30952): accuracy: 0.8522, mnli_loss: 0.3846
09/07 08:16:43 PM: ***** Step 31000 / Validation 31 *****
09/07 08:16:43 PM: mnli: trained on 999 batches, 0.061 epochs
09/07 08:16:43 PM: winograd-coreference: trained on 1 batches, 0.042 epochs
09/07 08:16:43 PM: Validating...
09/07 08:16:52 PM: Evaluate: task mnli, batch 124 (209): accuracy: 0.8169, mnli_loss: 0.4992
09/07 08:16:58 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.2500, acc: 0.5000, winograd-coreference_loss: 0.6685
09/07 08:16:58 PM: Updating LR scheduler:
09/07 08:16:58 PM: 	Best result seen so far for macro_avg: 0.720
09/07 08:16:58 PM: 	# validation passes without improvement: 0
09/07 08:16:58 PM: mnli_loss: training: 0.384481 validation: 0.503246
09/07 08:16:58 PM: winograd-coreference_loss: training: 0.701194 validation: 0.682812
09/07 08:16:58 PM: macro_avg: validation: 0.677131
09/07 08:16:58 PM: micro_avg: validation: 0.810149
09/07 08:16:58 PM: mnli_accuracy: training: 0.852129 validation: 0.815800
09/07 08:16:58 PM: winograd-coreference_f1: training: 0.421053 validation: 0.314286
09/07 08:16:58 PM: winograd-coreference_acc: training: 0.541667 validation: 0.538462
09/07 08:16:58 PM: Global learning rate: 1.25e-06
09/07 08:16:58 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 08:17:02 PM: Update 31012: task mnli, batch 12 (30970): accuracy: 0.8542, mnli_loss: 0.3952
09/07 08:17:12 PM: Update 31043: task mnli, batch 43 (31001): accuracy: 0.8555, mnli_loss: 0.3650
09/07 08:17:22 PM: Update 31083: task mnli, batch 83 (31041): accuracy: 0.8503, mnli_loss: 0.3646
09/07 08:17:32 PM: Update 31122: task mnli, batch 122 (31080): accuracy: 0.8449, mnli_loss: 0.3813
09/07 08:17:42 PM: Update 31160: task mnli, batch 160 (31118): accuracy: 0.8463, mnli_loss: 0.3808
09/07 08:17:53 PM: Update 31201: task mnli, batch 201 (31159): accuracy: 0.8459, mnli_loss: 0.3888
09/07 08:18:03 PM: Update 31241: task mnli, batch 241 (31199): accuracy: 0.8412, mnli_loss: 0.4021
09/07 08:18:13 PM: Update 31284: task mnli, batch 284 (31242): accuracy: 0.8452, mnli_loss: 0.3933
09/07 08:18:23 PM: Update 31328: task mnli, batch 328 (31286): accuracy: 0.8470, mnli_loss: 0.3936
09/07 08:18:33 PM: Update 31368: task mnli, batch 368 (31326): accuracy: 0.8488, mnli_loss: 0.3919
09/07 08:18:44 PM: Update 31410: task mnli, batch 410 (31368): accuracy: 0.8493, mnli_loss: 0.3882
09/07 08:18:53 PM: Update 31440: task winograd-coreference, batch 1 (43): f1: 0.5000, acc: 0.5000, winograd-coreference_loss: 0.6869
09/07 08:18:54 PM: Update 31442: task mnli, batch 441 (31399): accuracy: 0.8492, mnli_loss: 0.3889
09/07 08:19:04 PM: Update 31483: task mnli, batch 482 (31440): accuracy: 0.8489, mnli_loss: 0.3891
09/07 08:19:14 PM: Update 31524: task mnli, batch 523 (31481): accuracy: 0.8489, mnli_loss: 0.3879
09/07 08:19:24 PM: Update 31564: task mnli, batch 563 (31521): accuracy: 0.8492, mnli_loss: 0.3903
09/07 08:19:35 PM: Update 31603: task mnli, batch 602 (31560): accuracy: 0.8488, mnli_loss: 0.3912
09/07 08:19:45 PM: Update 31643: task mnli, batch 642 (31600): accuracy: 0.8493, mnli_loss: 0.3894
09/07 08:19:55 PM: Update 31681: task mnli, batch 680 (31638): accuracy: 0.8495, mnli_loss: 0.3895
09/07 08:20:05 PM: Update 31722: task mnli, batch 721 (31679): accuracy: 0.8496, mnli_loss: 0.3907
09/07 08:20:15 PM: Update 31762: task mnli, batch 761 (31719): accuracy: 0.8502, mnli_loss: 0.3893
09/07 08:20:25 PM: Update 31803: task mnli, batch 802 (31760): accuracy: 0.8505, mnli_loss: 0.3890
09/07 08:20:36 PM: Update 31844: task mnli, batch 843 (31801): accuracy: 0.8511, mnli_loss: 0.3885
09/07 08:20:46 PM: Update 31872: task mnli, batch 871 (31829): accuracy: 0.8517, mnli_loss: 0.3877
09/07 08:20:56 PM: Update 31913: task mnli, batch 912 (31870): accuracy: 0.8517, mnli_loss: 0.3879
09/07 08:21:06 PM: Update 31953: task mnli, batch 952 (31910): accuracy: 0.8512, mnli_loss: 0.3880
09/07 08:21:17 PM: Update 31994: task mnli, batch 993 (31951): accuracy: 0.8518, mnli_loss: 0.3865
09/07 08:21:18 PM: ***** Step 32000 / Validation 32 *****
09/07 08:21:18 PM: mnli: trained on 999 batches, 0.061 epochs
09/07 08:21:18 PM: winograd-coreference: trained on 1 batches, 0.042 epochs
09/07 08:21:18 PM: Validating...
09/07 08:21:27 PM: Evaluate: task mnli, batch 126 (209): accuracy: 0.8191, mnli_loss: 0.4987
09/07 08:21:33 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.2500, acc: 0.5000, winograd-coreference_loss: 0.6696
09/07 08:21:33 PM: Updating LR scheduler:
09/07 08:21:33 PM: 	Best result seen so far for macro_avg: 0.720
09/07 08:21:33 PM: 	# validation passes without improvement: 1
09/07 08:21:33 PM: mnli_loss: training: 0.386807 validation: 0.506337
09/07 08:21:33 PM: winograd-coreference_loss: training: 0.686943 validation: 0.682865
09/07 08:21:33 PM: macro_avg: validation: 0.677931
09/07 08:21:33 PM: micro_avg: validation: 0.811716
09/07 08:21:33 PM: mnli_accuracy: training: 0.851620 validation: 0.817400
09/07 08:21:33 PM: winograd-coreference_f1: training: 0.500000 validation: 0.314286
09/07 08:21:33 PM: winograd-coreference_acc: training: 0.500000 validation: 0.538462
09/07 08:21:33 PM: Global learning rate: 1.25e-06
09/07 08:21:33 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 08:21:37 PM: Update 32011: task mnli, batch 11 (31968): accuracy: 0.7955, mnli_loss: 0.5002
09/07 08:21:47 PM: Update 32052: task mnli, batch 52 (32009): accuracy: 0.8446, mnli_loss: 0.3860
09/07 08:21:57 PM: Update 32094: task mnli, batch 94 (32051): accuracy: 0.8440, mnli_loss: 0.3883
09/07 08:22:07 PM: Update 32135: task mnli, batch 135 (32092): accuracy: 0.8494, mnli_loss: 0.3839
09/07 08:22:17 PM: Update 32175: task mnli, batch 175 (32132): accuracy: 0.8507, mnli_loss: 0.3832
09/07 08:22:21 PM: Update 32188: task winograd-coreference, batch 1 (44): f1: 0.4286, acc: 0.6667, winograd-coreference_loss: 0.6515
09/07 08:22:28 PM: Update 32216: task mnli, batch 215 (32172): accuracy: 0.8490, mnli_loss: 0.3883
09/07 08:22:38 PM: Update 32259: task mnli, batch 258 (32215): accuracy: 0.8500, mnli_loss: 0.3875
09/07 08:22:44 PM: Update 32274: task winograd-coreference, batch 2 (45): f1: 0.4118, acc: 0.5833, winograd-coreference_loss: 0.6731
09/07 08:22:48 PM: Update 32290: task mnli, batch 288 (32245): accuracy: 0.8479, mnli_loss: 0.3930
09/07 08:22:58 PM: Update 32332: task mnli, batch 330 (32287): accuracy: 0.8491, mnli_loss: 0.3904
09/07 08:23:08 PM: Update 32373: task mnli, batch 371 (32328): accuracy: 0.8490, mnli_loss: 0.3890
09/07 08:23:18 PM: Update 32411: task mnli, batch 409 (32366): accuracy: 0.8501, mnli_loss: 0.3867
09/07 08:23:29 PM: Update 32448: task mnli, batch 446 (32403): accuracy: 0.8513, mnli_loss: 0.3870
09/07 08:23:39 PM: Update 32484: task mnli, batch 482 (32439): accuracy: 0.8504, mnli_loss: 0.3899
09/07 08:23:42 PM: Update 32495: task winograd-coreference, batch 3 (46): f1: 0.3404, acc: 0.5694, winograd-coreference_loss: 0.6789
09/07 08:23:49 PM: Update 32522: task mnli, batch 519 (32476): accuracy: 0.8516, mnli_loss: 0.3882
09/07 08:23:59 PM: Update 32559: task mnli, batch 556 (32513): accuracy: 0.8524, mnli_loss: 0.3852
09/07 08:24:09 PM: Update 32597: task mnli, batch 594 (32551): accuracy: 0.8528, mnli_loss: 0.3851
09/07 08:24:20 PM: Update 32634: task mnli, batch 631 (32588): accuracy: 0.8528, mnli_loss: 0.3847
09/07 08:24:30 PM: Update 32672: task mnli, batch 669 (32626): accuracy: 0.8521, mnli_loss: 0.3859
09/07 08:24:40 PM: Update 32708: task mnli, batch 705 (32662): accuracy: 0.8504, mnli_loss: 0.3886
09/07 08:24:50 PM: Update 32746: task mnli, batch 743 (32700): accuracy: 0.8504, mnli_loss: 0.3888
09/07 08:25:00 PM: Update 32783: task mnli, batch 780 (32737): accuracy: 0.8500, mnli_loss: 0.3884
09/07 08:25:10 PM: Update 32811: task mnli, batch 808 (32765): accuracy: 0.8495, mnli_loss: 0.3891
09/07 08:25:20 PM: Update 32848: task mnli, batch 845 (32802): accuracy: 0.8497, mnli_loss: 0.3885
09/07 08:25:31 PM: Update 32883: task mnli, batch 880 (32837): accuracy: 0.8493, mnli_loss: 0.3889
09/07 08:25:41 PM: Update 32921: task mnli, batch 918 (32875): accuracy: 0.8500, mnli_loss: 0.3880
09/07 08:25:47 PM: Update 32943: task winograd-coreference, batch 4 (47): f1: 0.3939, acc: 0.5833, winograd-coreference_loss: 0.6759
09/07 08:25:51 PM: Update 32959: task mnli, batch 955 (32912): accuracy: 0.8509, mnli_loss: 0.3871
09/07 08:26:01 PM: Update 32996: task mnli, batch 992 (32949): accuracy: 0.8506, mnli_loss: 0.3865
09/07 08:26:02 PM: ***** Step 33000 / Validation 33 *****
09/07 08:26:02 PM: mnli: trained on 996 batches, 0.061 epochs
09/07 08:26:02 PM: winograd-coreference: trained on 4 batches, 0.167 epochs
09/07 08:26:02 PM: Validating...
09/07 08:26:11 PM: Evaluate: task mnli, batch 138 (209): accuracy: 0.8204, mnli_loss: 0.4894
09/07 08:26:16 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.2500, acc: 0.5000, winograd-coreference_loss: 0.6662
09/07 08:26:16 PM: Best result seen so far for mnli.
09/07 08:26:16 PM: Best result seen so far for micro.
09/07 08:26:16 PM: Updating LR scheduler:
09/07 08:26:16 PM: 	Best result seen so far for macro_avg: 0.720
09/07 08:26:16 PM: 	# validation passes without improvement: 2
09/07 08:26:16 PM: mnli_loss: training: 0.386027 validation: 0.498256
09/07 08:26:16 PM: winograd-coreference_loss: training: 0.675880 validation: 0.681044
09/07 08:26:16 PM: macro_avg: validation: 0.679331
09/07 08:26:16 PM: micro_avg: validation: 0.814459
09/07 08:26:16 PM: mnli_accuracy: training: 0.850825 validation: 0.820200
09/07 08:26:16 PM: winograd-coreference_f1: training: 0.393939 validation: 0.314286
09/07 08:26:16 PM: winograd-coreference_acc: training: 0.583333 validation: 0.538462
09/07 08:26:16 PM: Global learning rate: 1.25e-06
09/07 08:26:16 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 08:26:21 PM: Update 33017: task mnli, batch 17 (32970): accuracy: 0.8725, mnli_loss: 0.3315
09/07 08:26:31 PM: Update 33057: task mnli, batch 57 (33010): accuracy: 0.8677, mnli_loss: 0.3563
09/07 08:26:42 PM: Update 33095: task mnli, batch 95 (33048): accuracy: 0.8675, mnli_loss: 0.3578
09/07 08:26:52 PM: Update 33131: task mnli, batch 131 (33084): accuracy: 0.8674, mnli_loss: 0.3609
09/07 08:27:02 PM: Update 33169: task mnli, batch 169 (33122): accuracy: 0.8651, mnli_loss: 0.3670
09/07 08:27:12 PM: Update 33207: task mnli, batch 207 (33160): accuracy: 0.8671, mnli_loss: 0.3641
09/07 08:27:22 PM: Update 33235: task mnli, batch 235 (33188): accuracy: 0.8656, mnli_loss: 0.3670
09/07 08:27:32 PM: Update 33272: task mnli, batch 272 (33225): accuracy: 0.8620, mnli_loss: 0.3732
09/07 08:27:36 PM: Update 33287: task winograd-coreference, batch 1 (48): f1: 0.4348, acc: 0.4583, winograd-coreference_loss: 0.7251
09/07 08:27:43 PM: Update 33310: task mnli, batch 309 (33262): accuracy: 0.8634, mnli_loss: 0.3687
09/07 08:27:53 PM: Update 33349: task mnli, batch 348 (33301): accuracy: 0.8628, mnli_loss: 0.3687
09/07 08:28:03 PM: Update 33387: task mnli, batch 386 (33339): accuracy: 0.8648, mnli_loss: 0.3677
09/07 08:28:07 PM: Update 33400: task winograd-coreference, batch 2 (49): f1: 0.4000, acc: 0.4231, winograd-coreference_loss: 0.7237
09/07 08:28:13 PM: Update 33422: task mnli, batch 420 (33373): accuracy: 0.8650, mnli_loss: 0.3651
09/07 08:28:23 PM: Update 33461: task mnli, batch 459 (33412): accuracy: 0.8649, mnli_loss: 0.3625
09/07 08:28:33 PM: Update 33499: task mnli, batch 497 (33450): accuracy: 0.8640, mnli_loss: 0.3624
09/07 08:28:44 PM: Update 33539: task mnli, batch 537 (33490): accuracy: 0.8628, mnli_loss: 0.3662
09/07 08:28:54 PM: Update 33576: task mnli, batch 574 (33527): accuracy: 0.8643, mnli_loss: 0.3634
09/07 08:29:04 PM: Update 33613: task mnli, batch 611 (33564): accuracy: 0.8634, mnli_loss: 0.3638
09/07 08:29:14 PM: Update 33638: task mnli, batch 636 (33589): accuracy: 0.8633, mnli_loss: 0.3635
09/07 08:29:24 PM: Update 33675: task mnli, batch 673 (33626): accuracy: 0.8630, mnli_loss: 0.3642
09/07 08:29:34 PM: Update 33713: task mnli, batch 711 (33664): accuracy: 0.8623, mnli_loss: 0.3658
09/07 08:29:44 PM: Update 33752: task mnli, batch 750 (33703): accuracy: 0.8620, mnli_loss: 0.3647
09/07 08:29:55 PM: Update 33790: task mnli, batch 788 (33741): accuracy: 0.8613, mnli_loss: 0.3660
09/07 08:30:05 PM: Update 33827: task mnli, batch 825 (33778): accuracy: 0.8608, mnli_loss: 0.3662
09/07 08:30:15 PM: Update 33865: task mnli, batch 863 (33816): accuracy: 0.8602, mnli_loss: 0.3670
09/07 08:30:25 PM: Update 33903: task mnli, batch 901 (33854): accuracy: 0.8596, mnli_loss: 0.3683
09/07 08:30:28 PM: Update 33914: task winograd-coreference, batch 3 (50): f1: 0.3158, acc: 0.4800, winograd-coreference_loss: 0.7104
09/07 08:30:35 PM: Update 33941: task mnli, batch 938 (33891): accuracy: 0.8598, mnli_loss: 0.3678
09/07 08:30:45 PM: Update 33977: task mnli, batch 974 (33927): accuracy: 0.8594, mnli_loss: 0.3687
09/07 08:30:52 PM: ***** Step 34000 / Validation 34 *****
09/07 08:30:52 PM: mnli: trained on 997 batches, 0.061 epochs
09/07 08:30:52 PM: winograd-coreference: trained on 3 batches, 0.125 epochs
09/07 08:30:52 PM: Validating...
09/07 08:30:55 PM: Evaluate: task mnli, batch 53 (209): accuracy: 0.8302, mnli_loss: 0.4588
09/07 08:31:05 PM: Evaluate: task mnli, batch 206 (209): accuracy: 0.8204, mnli_loss: 0.4927
09/07 08:31:06 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.2500, acc: 0.5000, winograd-coreference_loss: 0.6699
09/07 08:31:06 PM: Updating LR scheduler:
09/07 08:31:06 PM: 	Best result seen so far for macro_avg: 0.720
09/07 08:31:06 PM: 	# validation passes without improvement: 3
09/07 08:31:06 PM: mnli_loss: training: 0.369287 validation: 0.496202
09/07 08:31:06 PM: winograd-coreference_loss: training: 0.710393 validation: 0.683113
09/07 08:31:06 PM: macro_avg: validation: 0.673923
09/07 08:31:06 PM: micro_avg: validation: 0.813088
09/07 08:31:06 PM: mnli_accuracy: training: 0.858857 validation: 0.819000
09/07 08:31:06 PM: winograd-coreference_f1: training: 0.315789 validation: 0.309859
09/07 08:31:06 PM: winograd-coreference_acc: training: 0.480000 validation: 0.528846
09/07 08:31:06 PM: Global learning rate: 1.25e-06
09/07 08:31:06 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 08:31:16 PM: Update 34031: task mnli, batch 31 (33981): accuracy: 0.8481, mnli_loss: 0.4090
09/07 08:31:26 PM: Update 34060: task mnli, batch 60 (34010): accuracy: 0.8471, mnli_loss: 0.3865
09/07 08:31:32 PM: Update 34084: task winograd-coreference, batch 1 (51): f1: 0.3529, acc: 0.5417, winograd-coreference_loss: 0.6909
09/07 08:31:36 PM: Update 34098: task mnli, batch 97 (34047): accuracy: 0.8539, mnli_loss: 0.3821
09/07 08:31:46 PM: Update 34136: task mnli, batch 135 (34085): accuracy: 0.8549, mnli_loss: 0.3810
09/07 08:31:56 PM: Update 34174: task mnli, batch 173 (34123): accuracy: 0.8571, mnli_loss: 0.3764
09/07 08:32:06 PM: Update 34211: task mnli, batch 210 (34160): accuracy: 0.8553, mnli_loss: 0.3809
09/07 08:32:16 PM: Update 34248: task mnli, batch 247 (34197): accuracy: 0.8537, mnli_loss: 0.3806
09/07 08:32:26 PM: Update 34286: task mnli, batch 285 (34235): accuracy: 0.8557, mnli_loss: 0.3769
09/07 08:32:33 PM: Update 34310: task winograd-coreference, batch 2 (52): f1: 0.2581, acc: 0.5208, winograd-coreference_loss: 0.6869
09/07 08:32:37 PM: Update 34323: task mnli, batch 321 (34271): accuracy: 0.8555, mnli_loss: 0.3783
09/07 08:32:47 PM: Update 34362: task mnli, batch 360 (34310): accuracy: 0.8565, mnli_loss: 0.3771
09/07 08:32:57 PM: Update 34399: task mnli, batch 397 (34347): accuracy: 0.8570, mnli_loss: 0.3758
09/07 08:33:07 PM: Update 34436: task mnli, batch 434 (34384): accuracy: 0.8565, mnli_loss: 0.3748
09/07 08:33:20 PM: Update 34473: task mnli, batch 471 (34421): accuracy: 0.8573, mnli_loss: 0.3720
09/07 08:33:30 PM: Update 34510: task mnli, batch 508 (34458): accuracy: 0.8583, mnli_loss: 0.3704
09/07 08:33:40 PM: Update 34545: task mnli, batch 543 (34493): accuracy: 0.8584, mnli_loss: 0.3707
09/07 08:33:50 PM: Update 34581: task mnli, batch 579 (34529): accuracy: 0.8580, mnli_loss: 0.3718
09/07 08:34:00 PM: Update 34618: task mnli, batch 616 (34566): accuracy: 0.8560, mnli_loss: 0.3747
09/07 08:34:11 PM: Update 34655: task mnli, batch 653 (34603): accuracy: 0.8555, mnli_loss: 0.3758
09/07 08:34:21 PM: Update 34693: task mnli, batch 691 (34641): accuracy: 0.8565, mnli_loss: 0.3746
09/07 08:34:31 PM: Update 34730: task mnli, batch 728 (34678): accuracy: 0.8563, mnli_loss: 0.3744
09/07 08:34:41 PM: Update 34769: task mnli, batch 767 (34717): accuracy: 0.8556, mnli_loss: 0.3755
09/07 08:34:51 PM: Update 34807: task mnli, batch 805 (34755): accuracy: 0.8560, mnli_loss: 0.3752
09/07 08:35:01 PM: Update 34845: task mnli, batch 843 (34793): accuracy: 0.8565, mnli_loss: 0.3740
09/07 08:35:11 PM: Update 34884: task mnli, batch 882 (34832): accuracy: 0.8573, mnli_loss: 0.3723
09/07 08:35:21 PM: Update 34915: task mnli, batch 913 (34863): accuracy: 0.8575, mnli_loss: 0.3729
09/07 08:35:32 PM: Update 34951: task mnli, batch 949 (34899): accuracy: 0.8580, mnli_loss: 0.3717
09/07 08:35:42 PM: Update 34990: task mnli, batch 988 (34938): accuracy: 0.8591, mnli_loss: 0.3701
09/07 08:35:45 PM: ***** Step 35000 / Validation 35 *****
09/07 08:35:45 PM: mnli: trained on 998 batches, 0.061 epochs
09/07 08:35:45 PM: winograd-coreference: trained on 2 batches, 0.083 epochs
09/07 08:35:45 PM: Validating...
09/07 08:35:52 PM: Evaluate: task mnli, batch 111 (209): accuracy: 0.8202, mnli_loss: 0.4901
09/07 08:35:58 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.3529, acc: 0.5417, winograd-coreference_loss: 0.6726
09/07 08:35:58 PM: Best result seen so far for micro.
09/07 08:35:58 PM: Updating LR scheduler:
09/07 08:35:58 PM: 	Best result seen so far for macro_avg: 0.720
09/07 08:35:58 PM: 	# validation passes without improvement: 4
09/07 08:35:58 PM: mnli_loss: training: 0.370578 validation: 0.495803
09/07 08:35:58 PM: winograd-coreference_loss: training: 0.686867 validation: 0.684362
09/07 08:35:58 PM: macro_avg: validation: 0.684138
09/07 08:35:58 PM: micro_avg: validation: 0.814655
09/07 08:35:58 PM: mnli_accuracy: training: 0.858910 validation: 0.820200
09/07 08:35:58 PM: winograd-coreference_f1: training: 0.258065 validation: 0.356164
09/07 08:35:58 PM: winograd-coreference_acc: training: 0.520833 validation: 0.548077
09/07 08:35:58 PM: Global learning rate: 1.25e-06
09/07 08:35:58 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 08:36:02 PM: Update 35009: task mnli, batch 9 (34957): accuracy: 0.8333, mnli_loss: 0.4214
09/07 08:36:12 PM: Update 35046: task mnli, batch 46 (34994): accuracy: 0.8460, mnli_loss: 0.3705
09/07 08:36:22 PM: Update 35085: task mnli, batch 85 (35033): accuracy: 0.8583, mnli_loss: 0.3613
09/07 08:36:32 PM: Update 35122: task mnli, batch 122 (35070): accuracy: 0.8600, mnli_loss: 0.3577
09/07 08:36:43 PM: Update 35161: task mnli, batch 161 (35109): accuracy: 0.8621, mnli_loss: 0.3506
09/07 08:36:53 PM: Update 35198: task mnli, batch 198 (35146): accuracy: 0.8607, mnli_loss: 0.3540
09/07 08:37:03 PM: Update 35235: task mnli, batch 235 (35183): accuracy: 0.8603, mnli_loss: 0.3569
09/07 08:37:11 PM: Update 35266: task winograd-coreference, batch 1 (53): f1: 0.4545, acc: 0.5000, winograd-coreference_loss: 0.7017
09/07 08:37:13 PM: Update 35273: task mnli, batch 272 (35220): accuracy: 0.8612, mnli_loss: 0.3570
09/07 08:37:25 PM: Update 35308: task mnli, batch 307 (35255): accuracy: 0.8613, mnli_loss: 0.3570
09/07 08:37:35 PM: Update 35346: task mnli, batch 345 (35293): accuracy: 0.8623, mnli_loss: 0.3570
09/07 08:37:45 PM: Update 35384: task mnli, batch 383 (35331): accuracy: 0.8632, mnli_loss: 0.3569
09/07 08:37:56 PM: Update 35421: task mnli, batch 420 (35368): accuracy: 0.8621, mnli_loss: 0.3600
09/07 08:38:06 PM: Update 35458: task mnli, batch 457 (35405): accuracy: 0.8622, mnli_loss: 0.3597
09/07 08:38:16 PM: Update 35496: task mnli, batch 495 (35443): accuracy: 0.8632, mnli_loss: 0.3571
09/07 08:38:26 PM: Update 35533: task mnli, batch 532 (35480): accuracy: 0.8633, mnli_loss: 0.3576
09/07 08:38:36 PM: Update 35570: task mnli, batch 569 (35517): accuracy: 0.8625, mnli_loss: 0.3576
09/07 08:38:46 PM: Update 35608: task mnli, batch 607 (35555): accuracy: 0.8627, mnli_loss: 0.3559
09/07 08:38:56 PM: Update 35646: task mnli, batch 645 (35593): accuracy: 0.8632, mnli_loss: 0.3560
09/07 08:39:07 PM: Update 35683: task mnli, batch 682 (35630): accuracy: 0.8630, mnli_loss: 0.3554
09/07 08:39:17 PM: Update 35719: task mnli, batch 718 (35666): accuracy: 0.8632, mnli_loss: 0.3555
09/07 08:39:27 PM: Update 35743: task mnli, batch 742 (35690): accuracy: 0.8636, mnli_loss: 0.3541
09/07 08:39:37 PM: Update 35781: task mnli, batch 780 (35728): accuracy: 0.8639, mnli_loss: 0.3539
09/07 08:39:46 PM: Update 35816: task winograd-coreference, batch 2 (54): f1: 0.5116, acc: 0.5625, winograd-coreference_loss: 0.6864
09/07 08:39:47 PM: Update 35819: task mnli, batch 817 (35765): accuracy: 0.8637, mnli_loss: 0.3551
09/07 08:39:57 PM: Update 35855: task mnli, batch 853 (35801): accuracy: 0.8628, mnli_loss: 0.3562
09/07 08:40:07 PM: Update 35895: task mnli, batch 893 (35841): accuracy: 0.8644, mnli_loss: 0.3539
09/07 08:40:17 PM: Update 35931: task mnli, batch 929 (35877): accuracy: 0.8649, mnli_loss: 0.3524
09/07 08:40:27 PM: Update 35969: task mnli, batch 967 (35915): accuracy: 0.8651, mnli_loss: 0.3515
09/07 08:40:36 PM: ***** Step 36000 / Validation 36 *****
09/07 08:40:36 PM: mnli: trained on 998 batches, 0.061 epochs
09/07 08:40:36 PM: winograd-coreference: trained on 2 batches, 0.083 epochs
09/07 08:40:36 PM: Validating...
09/07 08:40:38 PM: Evaluate: task mnli, batch 31 (209): accuracy: 0.8280, mnli_loss: 0.4832
09/07 08:40:48 PM: Evaluate: task mnli, batch 185 (209): accuracy: 0.8243, mnli_loss: 0.4973
09/07 08:40:49 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.4444, acc: 0.5833, winograd-coreference_loss: 0.6738
09/07 08:40:49 PM: Best result seen so far for mnli.
09/07 08:40:49 PM: Best result seen so far for micro.
09/07 08:40:49 PM: Updating LR scheduler:
09/07 08:40:49 PM: 	Best result seen so far for macro_avg: 0.720
09/07 08:40:49 PM: 	# validation passes without improvement: 0
09/07 08:40:49 PM: mnli_loss: training: 0.349644 validation: 0.504636
09/07 08:40:49 PM: winograd-coreference_loss: training: 0.686429 validation: 0.685111
09/07 08:40:49 PM: macro_avg: validation: 0.694354
09/07 08:40:49 PM: micro_avg: validation: 0.816223
09/07 08:40:49 PM: mnli_accuracy: training: 0.865892 validation: 0.821400
09/07 08:40:49 PM: winograd-coreference_f1: training: 0.511628 validation: 0.400000
09/07 08:40:49 PM: winograd-coreference_acc: training: 0.562500 validation: 0.567308
09/07 08:40:49 PM: Global learning rate: 6.25e-07
09/07 08:40:49 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 08:40:58 PM: Update 36027: task mnli, batch 27 (35973): accuracy: 0.8812, mnli_loss: 0.3204
09/07 08:41:08 PM: Update 36065: task mnli, batch 65 (36011): accuracy: 0.8712, mnli_loss: 0.3372
09/07 08:41:18 PM: Update 36103: task mnli, batch 103 (36049): accuracy: 0.8661, mnli_loss: 0.3418
09/07 08:41:28 PM: Update 36139: task mnli, batch 139 (36085): accuracy: 0.8645, mnli_loss: 0.3522
09/07 08:41:38 PM: Update 36166: task mnli, batch 166 (36112): accuracy: 0.8649, mnli_loss: 0.3591
09/07 08:41:48 PM: Update 36203: task mnli, batch 203 (36149): accuracy: 0.8620, mnli_loss: 0.3607
09/07 08:41:58 PM: Update 36240: task mnli, batch 240 (36186): accuracy: 0.8614, mnli_loss: 0.3627
09/07 08:42:09 PM: Update 36277: task mnli, batch 277 (36223): accuracy: 0.8636, mnli_loss: 0.3614
09/07 08:42:19 PM: Update 36316: task mnli, batch 316 (36262): accuracy: 0.8642, mnli_loss: 0.3615
09/07 08:42:29 PM: Update 36355: task mnli, batch 355 (36301): accuracy: 0.8643, mnli_loss: 0.3644
09/07 08:42:39 PM: Update 36394: task mnli, batch 394 (36340): accuracy: 0.8637, mnli_loss: 0.3643
09/07 08:42:49 PM: Update 36430: task mnli, batch 430 (36376): accuracy: 0.8650, mnli_loss: 0.3610
09/07 08:42:59 PM: Update 36468: task mnli, batch 468 (36414): accuracy: 0.8662, mnli_loss: 0.3585
09/07 08:43:09 PM: Update 36506: task mnli, batch 506 (36452): accuracy: 0.8663, mnli_loss: 0.3582
09/07 08:43:15 PM: Update 36526: task winograd-coreference, batch 1 (55): f1: 0.4211, acc: 0.5417, winograd-coreference_loss: 0.6937
09/07 08:43:20 PM: Update 36543: task mnli, batch 542 (36488): accuracy: 0.8654, mnli_loss: 0.3593
09/07 08:43:30 PM: Update 36569: task mnli, batch 568 (36514): accuracy: 0.8651, mnli_loss: 0.3576
09/07 08:43:40 PM: Update 36607: task mnli, batch 606 (36552): accuracy: 0.8670, mnli_loss: 0.3534
09/07 08:43:50 PM: Update 36645: task mnli, batch 644 (36590): accuracy: 0.8665, mnli_loss: 0.3525
09/07 08:44:00 PM: Update 36682: task mnli, batch 681 (36627): accuracy: 0.8659, mnli_loss: 0.3540
09/07 08:44:10 PM: Update 36719: task mnli, batch 718 (36664): accuracy: 0.8661, mnli_loss: 0.3526
09/07 08:44:20 PM: Update 36756: task mnli, batch 755 (36701): accuracy: 0.8669, mnli_loss: 0.3504
09/07 08:44:30 PM: Update 36792: task mnli, batch 791 (36737): accuracy: 0.8676, mnli_loss: 0.3493
09/07 08:44:40 PM: Update 36830: task mnli, batch 829 (36775): accuracy: 0.8680, mnli_loss: 0.3485
09/07 08:44:51 PM: Update 36868: task mnli, batch 867 (36813): accuracy: 0.8686, mnli_loss: 0.3468
09/07 08:45:01 PM: Update 36906: task mnli, batch 905 (36851): accuracy: 0.8691, mnli_loss: 0.3457
09/07 08:45:11 PM: Update 36945: task mnli, batch 944 (36890): accuracy: 0.8699, mnli_loss: 0.3447
09/07 08:45:22 PM: Update 36978: task mnli, batch 977 (36923): accuracy: 0.8701, mnli_loss: 0.3442
09/07 08:45:28 PM: ***** Step 37000 / Validation 37 *****
09/07 08:45:28 PM: mnli: trained on 999 batches, 0.061 epochs
09/07 08:45:28 PM: winograd-coreference: trained on 1 batches, 0.042 epochs
09/07 08:45:28 PM: Validating...
09/07 08:45:32 PM: Evaluate: task mnli, batch 66 (209): accuracy: 0.8295, mnli_loss: 0.4870
09/07 08:45:41 PM: Evaluate: task winograd-coreference, batch 1 (5): f1: 0.4444, acc: 0.5833, winograd-coreference_loss: 0.6750
09/07 08:45:42 PM: Best result seen so far for micro.
09/07 08:45:42 PM: Updating LR scheduler:
09/07 08:45:42 PM: 	Best result seen so far for macro_avg: 0.720
09/07 08:45:42 PM: 	# validation passes without improvement: 1
09/07 08:45:42 PM: Ran out of early stopping patience. Stopping training.
09/07 08:45:42 PM: mnli_loss: training: 0.343591 validation: 0.508760
09/07 08:45:42 PM: winograd-coreference_loss: training: 0.693741 validation: 0.685610
09/07 08:45:42 PM: macro_avg: validation: 0.708577
09/07 08:45:42 PM: micro_avg: validation: 0.816418
09/07 08:45:42 PM: mnli_accuracy: training: 0.869906 validation: 0.821000
09/07 08:45:42 PM: winograd-coreference_f1: training: 0.421053 validation: 0.461538
09/07 08:45:42 PM: winograd-coreference_acc: training: 0.541667 validation: 0.596154
09/07 08:45:42 PM: Global learning rate: 6.25e-07
09/07 08:45:42 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 08:45:42 PM: Stopped training after 37 validation checks
09/07 08:45:42 PM: Trained mnli for 36945 batches or 2.258 epochs
09/07 08:45:42 PM: Trained winograd-coreference for 55 batches or 2.292 epochs
09/07 08:45:42 PM: ***** VALIDATION RESULTS *****
09/07 08:45:42 PM: mnli_accuracy (for best val pass 36): mnli_loss: 0.50464, winograd-coreference_loss: 0.68511, macro_avg: 0.69435, micro_avg: 0.81622, mnli_accuracy: 0.82140, winograd-coreference_f1: 0.40000, winograd-coreference_acc: 0.56731
09/07 08:45:42 PM: winograd-coreference_acc (for best val pass 11): mnli_loss: 0.52565, winograd-coreference_loss: 0.67576, macro_avg: 0.71361, micro_avg: 0.78938, mnli_accuracy: 0.79260, winograd-coreference_f1: 0.05000, winograd-coreference_acc: 0.63462
09/07 08:45:42 PM: micro_avg (for best val pass 37): mnli_loss: 0.50876, winograd-coreference_loss: 0.68561, macro_avg: 0.70858, micro_avg: 0.81642, mnli_accuracy: 0.82100, winograd-coreference_f1: 0.46154, winograd-coreference_acc: 0.59615
09/07 08:45:42 PM: macro_avg (for best val pass 16): mnli_loss: 0.49611, winograd-coreference_loss: 0.68494, macro_avg: 0.71981, micro_avg: 0.80153, mnli_accuracy: 0.80500, winograd-coreference_f1: 0.00000, winograd-coreference_acc: 0.63462
09/07 08:45:42 PM: Evaluating...
09/07 08:45:43 PM: Loaded model state from diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval/model_state_pretrain_val_16.best.th
09/07 08:45:43 PM: Evaluating on: glue-diagnostic, split: val
09/07 08:45:48 PM: Task 'glue-diagnostic': sorting predictions by 'idx'
09/07 08:45:48 PM: Finished evaluating on: glue-diagnostic
09/07 08:45:48 PM: Wrote predictions for task: glue-diagnostic
09/07 08:45:48 PM: Task 'glue-diagnostic': Wrote predictions to diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 08:45:48 PM: Wrote all preds for split 'val' to diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 08:45:48 PM: Evaluating on: glue-diagnostic, split: test
09/07 08:45:52 PM: Task 'glue-diagnostic': sorting predictions by 'idx'
09/07 08:45:52 PM: Finished evaluating on: glue-diagnostic
09/07 08:45:52 PM: Wrote predictions for task: glue-diagnostic
09/07 08:45:52 PM: Task 'glue-diagnostic': Wrote predictions to diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 08:45:52 PM: Wrote all preds for split 'test' to diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 08:45:52 PM: Writing results for split 'val' to diagnostic_run_2/my-experiment/results.tsv
09/07 08:45:52 PM: micro_avg: 0.000, macro_avg: 0.000, glue-diagnostic_lex_sem: 0.348, glue-diagnostic_lex_sem__Lexical entailment: 0.270, glue-diagnostic_lex_sem__Redundancy: 0.677, glue-diagnostic_lex_sem__Quantifiers: 0.572, glue-diagnostic_lex_sem__Symmetry/Collectivity: 0.000, glue-diagnostic_lex_sem__Factivity: 0.122, glue-diagnostic_lex_sem__Morphological negation: 0.436, glue-diagnostic_lex_sem__Factivity;Quantifiers: 0.000, glue-diagnostic_lex_sem__Lexical entailment;Factivity: 0.000, glue-diagnostic_lex_sem__Named entities: 0.183, glue-diagnostic_lex_sem__Lexical entailment;Quantifiers: 1.000, glue-diagnostic_pr_ar_str: 0.395, glue-diagnostic_pr_ar_str__Intersectivity: 0.298, glue-diagnostic_pr_ar_str__Nominalization;Genitives/Partitives: 0.000, glue-diagnostic_pr_ar_str__Ellipsis/Implicits;Anaphora/Coreference: -0.577, glue-diagnostic_pr_ar_str__Core args: 0.318, glue-diagnostic_pr_ar_str__Relative clauses: 0.277, glue-diagnostic_pr_ar_str__Genitives/Partitives: 0.685, glue-diagnostic_pr_ar_str__Coordination scope;Prepositional phrases: 0.333, glue-diagnostic_pr_ar_str__Restrictivity;Relative clauses: -1.000, glue-diagnostic_pr_ar_str__Active/Passive;Prepositional phrases: 1.000, glue-diagnostic_pr_ar_str__Intersectivity;Ellipsis/Implicits: 0.000, glue-diagnostic_pr_ar_str__Datives: 0.673, glue-diagnostic_pr_ar_str__Prepositional phrases: 0.647, glue-diagnostic_pr_ar_str__Core args;Anaphora/Coreference: 0.447, glue-diagnostic_pr_ar_str__Relative clauses;Anaphora/Coreference: 1.000, glue-diagnostic_pr_ar_str__Relative clauses;Restrictivity: 0.577, glue-diagnostic_pr_ar_str__Ellipsis/Implicits: 0.317, glue-diagnostic_pr_ar_str__Coordination scope: 0.402, glue-diagnostic_pr_ar_str__Anaphora/Coreference: 0.270, glue-diagnostic_pr_ar_str__Restrictivity;Anaphora/Coreference: 0.000, glue-diagnostic_pr_ar_str__Anaphora/Coreference;Prepositional phrases: 1.000, glue-diagnostic_pr_ar_str__Nominalization: 0.367, glue-diagnostic_pr_ar_str__Active/Passive: 0.445, glue-diagnostic_pr_ar_str__Restrictivity: -0.250, glue-diagnostic_logic: 0.191, glue-diagnostic_logic__Conditionals: 0.247, glue-diagnostic_logic__Universal;Conjunction: 0.000, glue-diagnostic_logic__Downward monotone;Conditionals: -1.000, glue-diagnostic_logic__Intervals/Numbers: -0.170, glue-diagnostic_logic__Existential: 0.400, glue-diagnostic_logic__Upward monotone: 0.349, glue-diagnostic_logic__Disjunction;Conjunction: 0.000, glue-diagnostic_logic__Conjunction;Upward monotone: 1.000, glue-diagnostic_logic__Double negation;Negation: 0.000, glue-diagnostic_logic__Existential;Upward monotone: 1.000, glue-diagnostic_logic__Temporal: 0.025, glue-diagnostic_logic__Non-monotone: 0.127, glue-diagnostic_logic__Conjunction;Negation: 0.250, glue-diagnostic_logic__Downward monotone: -0.392, glue-diagnostic_logic__Temporal;Intervals/Numbers: 0.000, glue-diagnostic_logic__Universal;Negation: 0.000, glue-diagnostic_logic__Disjunction;Negation: -0.316, glue-diagnostic_logic__Universal: 0.682, glue-diagnostic_logic__Conjunction: 0.483, glue-diagnostic_logic__Disjunction: -0.374, glue-diagnostic_logic__Disjunction;Conditionals;Negation: 0.167, glue-diagnostic_logic__Existential;Negation: 0.000, glue-diagnostic_logic__Negation;Conditionals: 0.000, glue-diagnostic_logic__Temporal;Conjunction: 0.000, glue-diagnostic_logic__Double negation: 0.257, glue-diagnostic_logic__Negation: -0.008, glue-diagnostic_logic__Downward monotone;Existential;Negation: -1.000, glue-diagnostic_logic__Intervals/Numbers;Non-monotone: 1.000, glue-diagnostic_logic__Disjunction;Non-monotone: 0.000, glue-diagnostic_knowledge: 0.198, glue-diagnostic_knowledge__World knowledge: 0.158, glue-diagnostic_knowledge__Common sense: 0.234, glue-diagnostic_all_mcc: 0.312, glue-diagnostic_accuracy: 0.557
09/07 08:45:53 PM: Loaded model state from diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval/model_state_pretrain_val_16.best.th
09/07 08:45:53 PM: Evaluating on: mnli, split: val
09/07 08:46:23 PM: 	Task mnli: batch 452
09/07 08:46:48 PM: Task 'mnli': sorting predictions by 'idx'
09/07 08:46:48 PM: Finished evaluating on: mnli
09/07 08:46:48 PM: Wrote predictions for task: mnli
09/07 08:46:48 PM: Task 'mnli': Wrote predictions to diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 08:46:48 PM: Wrote all preds for split 'val' to diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 08:46:48 PM: Evaluating on: mnli, split: test
09/07 08:47:18 PM: 	Task mnli: batch 460
09/07 08:47:43 PM: Task 'mnli': sorting predictions by 'idx'
09/07 08:47:43 PM: Finished evaluating on: mnli
09/07 08:47:43 PM: There are 19643 examples in MNLI, 19643 were expected
09/07 08:47:43 PM: Wrote predictions for task: mnli
09/07 08:47:43 PM: Task 'mnli': Wrote predictions to diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 08:47:43 PM: Wrote all preds for split 'test' to diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 08:47:43 PM: Writing results for split 'val' to diagnostic_run_2/my-experiment/results.tsv
09/07 08:47:43 PM: micro_avg: 0.811, macro_avg: 0.811, mnli_accuracy: 0.811
09/07 08:47:43 PM: Loaded model state from diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval/model_state_pretrain_val_16.best.th
09/07 08:47:43 PM: Evaluating on: winograd-coreference, split: val
09/07 08:47:43 PM: Task 'winograd-coreference': sorting predictions by 'idx'
09/07 08:47:43 PM: Finished evaluating on: winograd-coreference
09/07 08:47:43 PM: Task 'winograd-coreference': Wrote predictions to diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 08:47:43 PM: Wrote all preds for split 'val' to diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 08:47:43 PM: Evaluating on: winograd-coreference, split: test
09/07 08:47:44 PM: Task 'winograd-coreference': sorting predictions by 'idx'
09/07 08:47:44 PM: Finished evaluating on: winograd-coreference
09/07 08:47:44 PM: Task 'winograd-coreference': Wrote predictions to diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 08:47:44 PM: Wrote all preds for split 'test' to diagnostic_run_2/my-experiment/mnli_wsc_diagnostic_eval
09/07 08:47:44 PM: Writing results for split 'val' to diagnostic_run_2/my-experiment/results.tsv
09/07 08:47:44 PM: micro_avg: 0.635, macro_avg: 0.635, winograd-coreference_f1: 0.000, winograd-coreference_acc: 0.635
09/07 08:47:44 PM: Done!
