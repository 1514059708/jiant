// Template for translation as a seq2seq problem.

// This imports the defaults, which can be overridden below.
include "defaults.conf"  // relative path to this file

cuda = 0
random_seed = 42

load_model = 0
reload_tasks = 0
reload_indexing = 0
reload_vocab = 0
do_pretrain = 1
do_target_task_training = 0
do_full_eval = 1

pretrain_tasks = "wmt14_en_de"
target_tasks = ""
max_seq_len = 64
max_word_v_size = 40000
max_targ_word_v_size = 40000
pair_attn = 0

input_module = "roberta-large"
sent_enc = "rnn"
d_word = 1000
d_hid = 1000
n_layers_enc = 1
d_proj = 1000
d_hid_attn = 1000
skip_embs = 0
batch_size = 8

optimizer = adam
lr = 0.001
//tokenizer = "MosesTokenizer"
val_interval = 1000
lr_patience = 100
patience = 200
max_vals = 10000
dropout = 0.2

allow_untrained_encoder_parameters = 1

s2s {
    d_hid_dec = 1000
    d_trg_emb = 1000
    n_layers_dec = 1
    attention = "Bahdanau"
    output_proj_input_dim = 1000
    beam_size = 10
}
