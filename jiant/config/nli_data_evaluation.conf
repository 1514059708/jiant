// Config settings used for new data evaluation experiments, as defined in
// data-collection-util/191123-paper-runs.sh

// This imports the defaults, which can be overridden below.
include "defaults.conf"

// Data and preprocessing settings
max_seq_len = 128

// Model settings
input_module = "bert-large-cased"
pytorch_transformers_embedding_mode = "top"
pair_attn = 0
s2s = {
    attention = none
}
sent_enc = "none"
sep_embs_for_skip = 1
classifier = log_reg
transfer_paradigm = finetune

// Training settings
dropout = 0.1 // following BERT paper
optimizer = bert_adam
batch_size = 4
max_epochs = 4
lr = .000003
min_lr = .0000001
lr_patience = 100000
patience = 500
max_vals = 100000
val_interval = 1000  // Overrides in use for individual target tasks

// Control-flow stuff
do_pretrain = 1
do_target_task_training = 1
do_full_eval = 1
write_preds = "val,test"
write_strict_glue_format = 1

// Caution!
allow_reuse_of_pretraining_parameters = 1

// Task-specific overrides
commitbank = {
	val_interval = 30
	max_epochs = 10
}
copa = {
	val_interval = 50
	max_epochs = 10
}
winograd-coreference = {
	val_interval = 70
	max_epochs = 10
}
record = {
	val_interval = 10000
	max_epochs = 2
}
rte = {
	val_interval = 625
}

// Share classifiers aggressively. This should simplify transfer evaluations.
nli-a += {use_classifier = mnli}
nli-b += {use_classifier = mnli}
nli-c += {use_classifier = mnli}
nli-d += {use_classifier = mnli}
nli-f += {use_classifier = mnli}
mnli-government += {use_classifier = mnli}
snli += {use_classifier = mnli}
anli += {use_classifier = mnli}
hans += {use_classifier = mnli}
mnli-two += {use_classifier = mnli}
broadcoverage-diagnostic = {use_classifier = mnli}

nli-a-ho += {use_classifier = mnli-ho}
nli-b-ho += {use_classifier = mnli-ho}
nli-c-ho += {use_classifier = mnli-ho}
nli-d-ho += {use_classifier = mnli-ho}
nli-f-ho += {use_classifier = mnli-two-ho}
mnli-government-ho += {use_classifier = mnli-ho}
snli-ho += {use_classifier = mnli-ho}
anli-ho += {use_classifier = mnli-ho}
