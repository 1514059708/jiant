09/07 05:09:55 PM: Git branch: master
09/07 05:09:56 PM: Git SHA: 883e7176a66d891d9d0238a6a08338d8f200af17
09/07 05:09:56 PM: Parsed args: 
{
  "batch_size": 24,
  "classifier": "log_reg",
  "cuda": 1,
  "do_target_task_training": 0,
  "dropout": 0.1,
  "dropout_embs": 0.1,
  "exp_dir": "diagnostic_run_2/mnli_wnli/",
  "exp_name": "mnli_wnli",
  "input_module": "bert-base-cased",
  "load_model": 0,
  "local_log_path": "diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval/log.log",
  "lr": "1e-5",
  "lr_patience": 4,
  "max_epochs": 3,
  "max_vals": 10000,
  "min_lr": 0.0,
  "optimizer": "bert_adam",
  "patience": 20,
  "pretrain_tasks": "mnli,wnli",
  "pytorch_transformers_output_mode": "top",
  "random_seed": 42,
  "remote_log_name": "mnli_wnli__mnli_wnli_diagnostic_eval",
  "run_dir": "diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval",
  "run_name": "mnli_wnli_diagnostic_eval",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "mnli,wnli,glue-diagnostic",
  "transfer_paradigm": "finetune",
  "write_preds": "val,test"
}
09/07 05:09:56 PM: Saved config to diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval/params.conf
09/07 05:09:56 PM: Using random seed 42
09/07 05:09:56 PM: Using GPU 1
09/07 05:09:56 PM: Loading tasks...
09/07 05:09:56 PM: Writing pre-preprocessed tasks to diagnostic_run_2/mnli_wnli/
09/07 05:09:56 PM: 	Creating task glue-diagnostic from scratch.
09/07 05:09:56 PM: 	Loading Tokenizer bert-base-cased
09/07 05:09:57 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/yp913/.cache/torch/pytorch_transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
09/07 05:09:57 PM: 	Finished loading diagnostic data.
09/07 05:09:57 PM: 	Finished creating score functions for diagnostic data.
09/07 05:09:57 PM: 	Task 'glue-diagnostic': |train|=1104 |val|=1104 |test|=1104
09/07 05:09:57 PM: 	Creating task mnli from scratch.
09/07 05:38:56 PM: 	Finished loading MNLI data.
09/07 05:39:00 PM: 	Task 'mnli': |train|=392702 |val|=19647 |test|=19643
09/07 05:39:00 PM: 	Creating task wnli from scratch.
09/07 05:39:03 PM: 	Finished loading Winograd.
09/07 05:39:03 PM: 	Task 'wnli': |train|=635 |val|=71 |test|=146
09/07 05:39:03 PM: 	Finished loading tasks: glue-diagnostic mnli wnli.
09/07 05:39:03 PM: 	Building vocab from scratch.
09/07 05:39:03 PM: 	Counting units for task glue-diagnostic.
09/07 05:39:03 PM: 	Counting units for task mnli.
09/07 05:39:15 PM: 	Counting units for task wnli.
09/07 05:39:15 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/yp913/.cache/torch/pytorch_transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
09/07 05:39:15 PM: Added pytorch_transformers vocab (bert-base-cased): 28996 tokens
09/07 05:39:15 PM: 	Saved vocab to diagnostic_run_2/mnli_wnli/vocab
09/07 05:39:15 PM: Loading token dictionary from diagnostic_run_2/mnli_wnli/vocab.
09/07 05:39:15 PM: 	Loaded vocab from diagnostic_run_2/mnli_wnli/vocab
09/07 05:39:15 PM: 	Vocab namespace bert_cased: size 28998
09/07 05:39:15 PM: 	Vocab namespace tokens: size 25704
09/07 05:39:15 PM: 	Vocab namespace chars: size 139
09/07 05:39:15 PM: 	Finished building vocab.
09/07 05:39:15 PM: 	Task glue-diagnostic (train): Indexing from scratch.
09/07 05:39:16 PM: 	Task glue-diagnostic (train): Saved 1104 instances to diagnostic_run_2/mnli_wnli/preproc/glue-diagnostic__train_data
09/07 05:39:16 PM: 	Task glue-diagnostic (val): Indexing from scratch.
09/07 05:39:17 PM: 	Task glue-diagnostic (val): Saved 1104 instances to diagnostic_run_2/mnli_wnli/preproc/glue-diagnostic__val_data
09/07 05:39:17 PM: 	Task glue-diagnostic (test): Indexing from scratch.
09/07 05:39:17 PM: 	Task glue-diagnostic (test): Saved 1104 instances to diagnostic_run_2/mnli_wnli/preproc/glue-diagnostic__test_data
09/07 05:39:17 PM: 	Task mnli (train): Indexing from scratch.
09/07 05:40:28 PM: 	Task mnli (train): Saved 392702 instances to diagnostic_run_2/mnli_wnli/preproc/mnli__train_data
09/07 05:40:28 PM: 	Task mnli (val): Indexing from scratch.
09/07 05:40:32 PM: 	Task mnli (val): Saved 19647 instances to diagnostic_run_2/mnli_wnli/preproc/mnli__val_data
09/07 05:40:32 PM: 	Task mnli (test): Indexing from scratch.
09/07 05:40:35 PM: 	Task mnli (test): Saved 19643 instances to diagnostic_run_2/mnli_wnli/preproc/mnli__test_data
09/07 05:40:35 PM: 	Task wnli (train): Indexing from scratch.
09/07 05:40:35 PM: 	Task wnli (train): Saved 635 instances to diagnostic_run_2/mnli_wnli/preproc/wnli__train_data
09/07 05:40:35 PM: 	Task wnli (val): Indexing from scratch.
09/07 05:40:35 PM: 	Task wnli (val): Saved 71 instances to diagnostic_run_2/mnli_wnli/preproc/wnli__val_data
09/07 05:40:35 PM: 	Task wnli (test): Indexing from scratch.
09/07 05:40:36 PM: 	Task wnli (test): Saved 146 instances to diagnostic_run_2/mnli_wnli/preproc/wnli__test_data
09/07 05:40:36 PM: 	Finished indexing tasks
09/07 05:40:36 PM: 	Creating trimmed target-only version of glue-diagnostic train.
09/07 05:40:36 PM: 	Creating trimmed pretraining-only version of mnli train.
09/07 05:40:36 PM: 	Creating trimmed target-only version of mnli train.
09/07 05:40:36 PM: 	Creating trimmed pretraining-only version of wnli train.
09/07 05:40:36 PM: 	Creating trimmed target-only version of wnli train.
09/07 05:40:36 PM: 	  Training on mnli, wnli
09/07 05:40:36 PM: 	  Evaluating on mnli, wnli, glue-diagnostic
09/07 05:40:36 PM: 	Finished loading tasks in 1839.367s
09/07 05:40:36 PM: 	 Tasks: ['glue-diagnostic', 'mnli', 'wnli']
09/07 05:40:36 PM: Building model...
09/07 05:40:36 PM: Using BERT model (bert-base-cased).
09/07 05:40:36 PM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json not found in cache, downloading to /tmp/tmp34ltqlgy
09/07 05:40:36 PM: copying /tmp/tmp34ltqlgy to cache at diagnostic_run_2/mnli_wnli/pytorch_transformers_cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.d7a3af18ce3a2ab7c0f48f04dc8daff45ed9a3ed333b9e9a79d012a0dedf87a6
09/07 05:40:36 PM: creating metadata file for diagnostic_run_2/mnli_wnli/pytorch_transformers_cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.d7a3af18ce3a2ab7c0f48f04dc8daff45ed9a3ed333b9e9a79d012a0dedf87a6
09/07 05:40:36 PM: removing temp file /tmp/tmp34ltqlgy
09/07 05:40:36 PM: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at diagnostic_run_2/mnli_wnli/pytorch_transformers_cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.d7a3af18ce3a2ab7c0f48f04dc8daff45ed9a3ed333b9e9a79d012a0dedf87a6
09/07 05:40:36 PM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

09/07 05:40:36 PM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin not found in cache, downloading to /tmp/tmpttyt1lbc
09/07 05:40:47 PM: copying /tmp/tmpttyt1lbc to cache at diagnostic_run_2/mnli_wnli/pytorch_transformers_cache/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
09/07 05:40:48 PM: creating metadata file for diagnostic_run_2/mnli_wnli/pytorch_transformers_cache/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
09/07 05:40:48 PM: removing temp file /tmp/tmpttyt1lbc
09/07 05:40:48 PM: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at diagnostic_run_2/mnli_wnli/pytorch_transformers_cache/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
09/07 05:40:51 PM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt not found in cache, downloading to /tmp/tmpevvrsslo
09/07 05:40:51 PM: copying /tmp/tmpevvrsslo to cache at diagnostic_run_2/mnli_wnli/pytorch_transformers_cache/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
09/07 05:40:51 PM: creating metadata file for diagnostic_run_2/mnli_wnli/pytorch_transformers_cache/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
09/07 05:40:51 PM: removing temp file /tmp/tmpevvrsslo
09/07 05:40:51 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at diagnostic_run_2/mnli_wnli/pytorch_transformers_cache/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
09/07 05:40:51 PM: Initializing parameters
09/07 05:40:51 PM: Done initializing parameters; the following parameters are using their default initialization from their code
09/07 05:40:51 PM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/07 05:40:51 PM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/07 05:40:51 PM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/07 05:40:51 PM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/07 05:40:51 PM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/07 05:40:51 PM:    _text_field_embedder.model.pooler.dense.bias
09/07 05:40:51 PM:    _text_field_embedder.model.pooler.dense.weight
09/07 05:40:51 PM: 	Task 'glue-diagnostic' params: {
  "cls_type": "log_reg",
  "d_hid": 512,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 1,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "mnli"
}
09/07 05:40:51 PM: 	Task 'mnli' params: {
  "cls_type": "log_reg",
  "d_hid": 512,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 1,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "mnli"
}
09/07 05:40:51 PM: 	Task 'wnli' params: {
  "cls_type": "log_reg",
  "d_hid": 512,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 1,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "wnli"
}
09/07 05:40:51 PM: Name of the task is different than the classifier it should use
09/07 05:40:51 PM: batch_first = True
09/07 05:40:51 PM: stateful = False
09/07 05:40:51 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/07 05:40:51 PM: CURRENTLY DEFINED PARAMETERS: 
09/07 05:40:51 PM: input_size = 1536
09/07 05:40:51 PM: hidden_size = 512
09/07 05:40:51 PM: num_layers = 1
09/07 05:40:51 PM: bidirectional = True
09/07 05:40:51 PM: batch_first = True
09/07 05:40:51 PM: Initializing parameters
09/07 05:40:51 PM: Done initializing parameters; the following parameters are using their default initialization from their code
09/07 05:40:51 PM:    _modeling_layer._module.bias_hh_l0
09/07 05:40:51 PM:    _modeling_layer._module.bias_hh_l0_reverse
09/07 05:40:51 PM:    _modeling_layer._module.bias_ih_l0
09/07 05:40:51 PM:    _modeling_layer._module.bias_ih_l0_reverse
09/07 05:40:51 PM:    _modeling_layer._module.weight_hh_l0
09/07 05:40:51 PM:    _modeling_layer._module.weight_hh_l0_reverse
09/07 05:40:51 PM:    _modeling_layer._module.weight_ih_l0
09/07 05:40:51 PM:    _modeling_layer._module.weight_ih_l0_reverse
09/07 05:40:51 PM: batch_first = True
09/07 05:40:51 PM: stateful = False
09/07 05:40:51 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/07 05:40:51 PM: CURRENTLY DEFINED PARAMETERS: 
09/07 05:40:51 PM: input_size = 1536
09/07 05:40:51 PM: hidden_size = 512
09/07 05:40:51 PM: num_layers = 1
09/07 05:40:51 PM: bidirectional = True
09/07 05:40:51 PM: batch_first = True
09/07 05:40:51 PM: Initializing parameters
09/07 05:40:51 PM: Done initializing parameters; the following parameters are using their default initialization from their code
09/07 05:40:51 PM:    _modeling_layer._module.bias_hh_l0
09/07 05:40:51 PM:    _modeling_layer._module.bias_hh_l0_reverse
09/07 05:40:51 PM:    _modeling_layer._module.bias_ih_l0
09/07 05:40:51 PM:    _modeling_layer._module.bias_ih_l0_reverse
09/07 05:40:51 PM:    _modeling_layer._module.weight_hh_l0
09/07 05:40:51 PM:    _modeling_layer._module.weight_hh_l0_reverse
09/07 05:40:51 PM:    _modeling_layer._module.weight_ih_l0
09/07 05:40:51 PM:    _modeling_layer._module.weight_ih_l0_reverse
09/07 05:40:56 PM: Model specification:
09/07 05:40:56 PM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.1)
  )
  (mnli_mdl): SingleClassifier(
    (pooler): Pooler()
    (classifier): Classifier(
      (classifier): Linear(in_features=768, out_features=3, bias=True)
    )
  )
  (wnli_mdl): SingleClassifier(
    (pooler): Pooler()
    (classifier): Classifier(
      (classifier): Linear(in_features=768, out_features=2, bias=True)
    )
  )
)
09/07 05:40:56 PM: Model parameters:
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Trainable parameter, count 22268928 with torch.Size([28996, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Trainable parameter, count 393216 with torch.Size([512, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/07 05:40:56 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/07 05:40:56 PM: 	mnli_mdl.classifier.classifier.weight: Trainable parameter, count 2304 with torch.Size([3, 768])
09/07 05:40:56 PM: 	mnli_mdl.classifier.classifier.bias: Trainable parameter, count 3 with torch.Size([3])
09/07 05:40:56 PM: 	wnli_mdl.classifier.classifier.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
09/07 05:40:56 PM: 	wnli_mdl.classifier.classifier.bias: Trainable parameter, count 2 with torch.Size([2])
09/07 05:40:56 PM: Total number of parameters: 108314117 (1.08314e+08)
09/07 05:40:56 PM: Number of trainable parameters: 108314117 (1.08314e+08)
09/07 05:40:56 PM: Finished building model in 20.304s
09/07 05:40:56 PM: Will run the following steps for this experiment:
Training model on tasks: mnli,wnli 
Evaluating model on tasks: mnli,wnli,glue-diagnostic 

09/07 05:40:56 PM: Training...
09/07 05:40:56 PM: patience = 20
09/07 05:40:56 PM: val_interval = 1000
09/07 05:40:56 PM: max_vals = 10000
09/07 05:40:56 PM: cuda_device = 1
09/07 05:40:56 PM: grad_norm = 5.0
09/07 05:40:56 PM: grad_clipping = None
09/07 05:40:56 PM: lr_decay = 0.99
09/07 05:40:56 PM: min_lr = 1e-07
09/07 05:40:56 PM: keep_all_checkpoints = 0
09/07 05:40:56 PM: val_data_limit = 5000
09/07 05:40:56 PM: max_epochs = 3
09/07 05:40:56 PM: dec_val_scale = 250
09/07 05:40:56 PM: training_data_fraction = 1
09/07 05:40:56 PM: type = bert_adam
09/07 05:40:56 PM: parameter_groups = None
09/07 05:40:56 PM: Number of trainable parameters: 108314117
09/07 05:40:56 PM: infer_type_and_cast = True
09/07 05:40:56 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/07 05:40:56 PM: CURRENTLY DEFINED PARAMETERS: 
09/07 05:40:56 PM: lr = 1e-5
09/07 05:40:56 PM: t_total = 50000
09/07 05:40:56 PM: warmup = 0.1
09/07 05:40:56 PM: type = reduce_on_plateau
09/07 05:40:56 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/07 05:40:56 PM: CURRENTLY DEFINED PARAMETERS: 
09/07 05:40:56 PM: mode = max
09/07 05:40:56 PM: factor = 0.5
09/07 05:40:56 PM: patience = 4
09/07 05:40:56 PM: threshold = 0.0001
09/07 05:40:56 PM: threshold_mode = abs
09/07 05:40:56 PM: verbose = True
09/07 05:40:56 PM: type = bert_adam
09/07 05:40:56 PM: parameter_groups = None
09/07 05:40:56 PM: Number of trainable parameters: 108314117
09/07 05:40:56 PM: infer_type_and_cast = True
09/07 05:40:56 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/07 05:40:56 PM: CURRENTLY DEFINED PARAMETERS: 
09/07 05:40:56 PM: lr = 1e-5
09/07 05:40:56 PM: t_total = 1000
09/07 05:40:56 PM: warmup = 0.1
09/07 05:40:56 PM: type = reduce_on_plateau
09/07 05:40:56 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/07 05:40:56 PM: CURRENTLY DEFINED PARAMETERS: 
09/07 05:40:56 PM: mode = max
09/07 05:40:56 PM: factor = 0.5
09/07 05:40:56 PM: patience = 4
09/07 05:40:56 PM: threshold = 0.0001
09/07 05:40:56 PM: threshold_mode = abs
09/07 05:40:56 PM: verbose = True
09/07 05:40:56 PM: type = bert_adam
09/07 05:40:56 PM: parameter_groups = None
09/07 05:40:56 PM: Number of trainable parameters: 108314117
09/07 05:40:56 PM: infer_type_and_cast = True
09/07 05:40:56 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/07 05:40:56 PM: CURRENTLY DEFINED PARAMETERS: 
09/07 05:40:56 PM: lr = 1e-5
09/07 05:40:56 PM: t_total = 50000
09/07 05:40:56 PM: warmup = 0.1
09/07 05:40:56 PM: type = reduce_on_plateau
09/07 05:40:56 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/07 05:40:56 PM: CURRENTLY DEFINED PARAMETERS: 
09/07 05:40:56 PM: mode = max
09/07 05:40:56 PM: factor = 0.5
09/07 05:40:56 PM: patience = 4
09/07 05:40:56 PM: threshold = 0.0001
09/07 05:40:56 PM: threshold_mode = abs
09/07 05:40:56 PM: verbose = True
09/07 05:40:56 PM: Starting training without restoring from a checkpoint.
09/07 05:40:56 PM: Training examples per task, before any subsampling: {'mnli': 392702, 'wnli': 635}
09/07 05:40:56 PM: Using weighting method: proportional, with normalized sample weights [0.9984 0.0016] 
09/07 05:40:56 PM: Beginning training with stopping criteria based on metric: macro_avg
09/07 05:41:06 PM: Update 30: task mnli, batch 30 (30): accuracy: 0.3539, mnli_loss: 1.1100
09/07 05:41:16 PM: Update 73: task mnli, batch 73 (73): accuracy: 0.3297, mnli_loss: 1.1148
09/07 05:41:26 PM: Update 117: task mnli, batch 117 (117): accuracy: 0.3300, mnli_loss: 1.1117
09/07 05:41:36 PM: Update 159: task mnli, batch 159 (159): accuracy: 0.3277, mnli_loss: 1.1120
09/07 05:41:46 PM: Update 199: task wnli, batch 1 (1): accuracy: 0.6364, wnli_loss: 0.6907
09/07 05:41:47 PM: Update 200: task mnli, batch 199 (199): accuracy: 0.3268, mnli_loss: 1.1113
09/07 05:41:57 PM: Update 238: task mnli, batch 237 (237): accuracy: 0.3292, mnli_loss: 1.1103
09/07 05:42:01 PM: Update 255: task wnli, batch 2 (2): accuracy: 0.4286, wnli_loss: 0.7392
09/07 05:42:07 PM: Update 277: task mnli, batch 275 (275): accuracy: 0.3372, mnli_loss: 1.1080
09/07 05:42:17 PM: Update 315: task mnli, batch 313 (313): accuracy: 0.3432, mnli_loss: 1.1060
09/07 05:42:27 PM: Update 354: task mnli, batch 352 (352): accuracy: 0.3460, mnli_loss: 1.1049
09/07 05:42:37 PM: Update 394: task mnli, batch 392 (392): accuracy: 0.3490, mnli_loss: 1.1035
09/07 05:42:47 PM: Update 421: task mnli, batch 419 (419): accuracy: 0.3523, mnli_loss: 1.1022
09/07 05:42:57 PM: Update 462: task mnli, batch 460 (460): accuracy: 0.3555, mnli_loss: 1.1007
09/07 05:43:07 PM: Update 500: task mnli, batch 498 (498): accuracy: 0.3608, mnli_loss: 1.0982
09/07 05:43:13 PM: Update 521: task wnli, batch 3 (3): accuracy: 0.4576, wnli_loss: 0.7209
09/07 05:43:18 PM: Update 540: task mnli, batch 537 (537): accuracy: 0.3648, mnli_loss: 1.0964
09/07 05:43:28 PM: Update 579: task mnli, batch 576 (576): accuracy: 0.3707, mnli_loss: 1.0936
09/07 05:43:38 PM: Update 617: task mnli, batch 614 (614): accuracy: 0.3751, mnli_loss: 1.0911
09/07 05:43:48 PM: Update 657: task mnli, batch 654 (654): accuracy: 0.3805, mnli_loss: 1.0880
09/07 05:43:58 PM: Update 697: task mnli, batch 694 (694): accuracy: 0.3871, mnli_loss: 1.0838
09/07 05:44:06 PM: Update 726: task wnli, batch 4 (4): accuracy: 0.4699, wnli_loss: 0.7102
09/07 05:44:08 PM: Update 734: task mnli, batch 730 (730): accuracy: 0.3937, mnli_loss: 1.0801
09/07 05:44:18 PM: Update 773: task mnli, batch 769 (769): accuracy: 0.4019, mnli_loss: 1.0740
09/07 05:44:26 PM: Update 804: task wnli, batch 5 (5): accuracy: 0.4486, wnli_loss: 0.7142
09/07 05:44:28 PM: Update 811: task mnli, batch 806 (806): accuracy: 0.4087, mnli_loss: 1.0690
09/07 05:44:39 PM: Update 840: task mnli, batch 835 (835): accuracy: 0.4140, mnli_loss: 1.0646
09/07 05:44:49 PM: Update 878: task mnli, batch 873 (873): accuracy: 0.4209, mnli_loss: 1.0584
09/07 05:44:59 PM: Update 915: task mnli, batch 910 (910): accuracy: 0.4275, mnli_loss: 1.0518
09/07 05:45:09 PM: Update 955: task mnli, batch 950 (950): accuracy: 0.4341, mnli_loss: 1.0450
09/07 05:45:20 PM: Update 995: task mnli, batch 990 (990): accuracy: 0.4401, mnli_loss: 1.0386
09/07 05:45:21 PM: ***** Step 1000 / Validation 1 *****
09/07 05:45:21 PM: mnli: trained on 995 batches, 0.061 epochs
09/07 05:45:21 PM: wnli: trained on 5 batches, 0.185 epochs
09/07 05:45:21 PM: Validating...
09/07 05:45:30 PM: Evaluate: task mnli, batch 135 (209): accuracy: 0.5988, mnli_loss: 0.8764
09/07 05:45:35 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.7500, wnli_loss: 0.6385
09/07 05:45:35 PM: Best result seen so far for mnli.
09/07 05:45:35 PM: Best result seen so far for wnli.
09/07 05:45:35 PM: Best result seen so far for micro.
09/07 05:45:35 PM: Best result seen so far for macro.
09/07 05:45:35 PM: Updating LR scheduler:
09/07 05:45:35 PM: 	Best result seen so far for macro_avg: 0.588
09/07 05:45:35 PM: 	# validation passes without improvement: 0
09/07 05:45:35 PM: mnli_loss: training: 1.037848 validation: 0.882249
09/07 05:45:35 PM: wnli_loss: training: 0.714206 validation: 0.692966
09/07 05:45:35 PM: macro_avg: validation: 0.587532
09/07 05:45:35 PM: micro_avg: validation: 0.597318
09/07 05:45:35 PM: mnli_accuracy: training: 0.440853 validation: 0.597600
09/07 05:45:35 PM: wnli_accuracy: training: 0.448598 validation: 0.577465
09/07 05:45:35 PM: Global learning rate: 1e-05
09/07 05:45:35 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 05:45:40 PM: Update 1016: task mnli, batch 15 (1010): accuracy: 0.5889, mnli_loss: 0.8769
09/07 05:45:50 PM: Update 1055: task mnli, batch 54 (1049): accuracy: 0.5903, mnli_loss: 0.8896
09/07 05:45:52 PM: Update 1065: task wnli, batch 2 (7): accuracy: 0.4583, wnli_loss: 0.7200
09/07 05:46:00 PM: Update 1097: task mnli, batch 95 (1090): accuracy: 0.5895, mnli_loss: 0.8837
09/07 05:46:10 PM: Update 1135: task mnli, batch 133 (1128): accuracy: 0.5921, mnli_loss: 0.8826
09/07 05:46:20 PM: Update 1175: task mnli, batch 173 (1168): accuracy: 0.5966, mnli_loss: 0.8796
09/07 05:46:31 PM: Update 1216: task mnli, batch 214 (1209): accuracy: 0.5981, mnli_loss: 0.8756
09/07 05:46:41 PM: Update 1255: task mnli, batch 253 (1248): accuracy: 0.6039, mnli_loss: 0.8678
09/07 05:46:51 PM: Update 1286: task mnli, batch 284 (1279): accuracy: 0.6034, mnli_loss: 0.8654
09/07 05:46:53 PM: Update 1293: task wnli, batch 3 (8): accuracy: 0.5417, wnli_loss: 0.6871
09/07 05:47:01 PM: Update 1327: task mnli, batch 324 (1319): accuracy: 0.6048, mnli_loss: 0.8623
09/07 05:47:11 PM: Update 1367: task mnli, batch 364 (1359): accuracy: 0.6087, mnli_loss: 0.8550
09/07 05:47:21 PM: Update 1403: task mnli, batch 400 (1395): accuracy: 0.6111, mnli_loss: 0.8516
09/07 05:47:31 PM: Update 1441: task mnli, batch 438 (1433): accuracy: 0.6114, mnli_loss: 0.8511
09/07 05:47:42 PM: Update 1481: task mnli, batch 478 (1473): accuracy: 0.6145, mnli_loss: 0.8478
09/07 05:47:52 PM: Update 1520: task mnli, batch 517 (1512): accuracy: 0.6181, mnli_loss: 0.8419
09/07 05:47:53 PM: Update 1523: task wnli, batch 4 (9): accuracy: 0.5312, wnli_loss: 0.6965
09/07 05:48:02 PM: Update 1558: task mnli, batch 554 (1549): accuracy: 0.6215, mnli_loss: 0.8379
09/07 05:48:12 PM: Update 1598: task mnli, batch 594 (1589): accuracy: 0.6246, mnli_loss: 0.8328
09/07 05:48:22 PM: Update 1636: task mnli, batch 632 (1627): accuracy: 0.6274, mnli_loss: 0.8295
09/07 05:48:32 PM: Update 1675: task mnli, batch 671 (1666): accuracy: 0.6274, mnli_loss: 0.8292
09/07 05:48:42 PM: Update 1704: task mnli, batch 700 (1695): accuracy: 0.6279, mnli_loss: 0.8289
09/07 05:48:46 PM: Update 1718: task wnli, batch 5 (10): accuracy: 0.5250, wnli_loss: 0.6922
09/07 05:48:53 PM: Update 1742: task mnli, batch 737 (1732): accuracy: 0.6300, mnli_loss: 0.8255
09/07 05:49:03 PM: Update 1781: task mnli, batch 776 (1771): accuracy: 0.6315, mnli_loss: 0.8243
09/07 05:49:13 PM: Update 1821: task mnli, batch 816 (1811): accuracy: 0.6324, mnli_loss: 0.8226
09/07 05:49:23 PM: Update 1859: task mnli, batch 854 (1849): accuracy: 0.6344, mnli_loss: 0.8194
09/07 05:49:33 PM: Update 1898: task mnli, batch 893 (1888): accuracy: 0.6363, mnli_loss: 0.8163
09/07 05:49:43 PM: Update 1937: task mnli, batch 932 (1927): accuracy: 0.6373, mnli_loss: 0.8145
09/07 05:49:53 PM: Update 1974: task mnli, batch 969 (1964): accuracy: 0.6387, mnli_loss: 0.8136
09/07 05:50:00 PM: ***** Step 2000 / Validation 2 *****
09/07 05:50:00 PM: mnli: trained on 995 batches, 0.061 epochs
09/07 05:50:00 PM: wnli: trained on 5 batches, 0.185 epochs
09/07 05:50:00 PM: Validating...
09/07 05:50:03 PM: Evaluate: task mnli, batch 50 (209): accuracy: 0.7000, mnli_loss: 0.6953
09/07 05:50:13 PM: Evaluate: task mnli, batch 206 (209): accuracy: 0.6911, mnli_loss: 0.7177
09/07 05:50:14 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.5833, wnli_loss: 0.6766
09/07 05:50:14 PM: Best result seen so far for mnli.
09/07 05:50:14 PM: Best result seen so far for micro.
09/07 05:50:14 PM: Best result seen so far for macro.
09/07 05:50:14 PM: Updating LR scheduler:
09/07 05:50:14 PM: 	Best result seen so far for macro_avg: 0.613
09/07 05:50:14 PM: 	# validation passes without improvement: 0
09/07 05:50:14 PM: mnli_loss: training: 0.811542 validation: 0.718322
09/07 05:50:14 PM: wnli_loss: training: 0.692157 validation: 0.690437
09/07 05:50:14 PM: macro_avg: validation: 0.612906
09/07 05:50:14 PM: micro_avg: validation: 0.688424
09/07 05:50:14 PM: mnli_accuracy: training: 0.639541 validation: 0.690600
09/07 05:50:14 PM: wnli_accuracy: training: 0.525000 validation: 0.535211
09/07 05:50:14 PM: Global learning rate: 1e-05
09/07 05:50:14 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 05:50:23 PM: Update 2033: task mnli, batch 33 (2023): accuracy: 0.6806, mnli_loss: 0.7159
09/07 05:50:34 PM: Update 2072: task mnli, batch 72 (2062): accuracy: 0.6887, mnli_loss: 0.7276
09/07 05:50:44 PM: Update 2101: task mnli, batch 101 (2091): accuracy: 0.6780, mnli_loss: 0.7506
09/07 05:50:54 PM: Update 2139: task mnli, batch 139 (2129): accuracy: 0.6737, mnli_loss: 0.7605
09/07 05:51:04 PM: Update 2177: task mnli, batch 177 (2167): accuracy: 0.6750, mnli_loss: 0.7568
09/07 05:51:14 PM: Update 2216: task mnli, batch 216 (2206): accuracy: 0.6756, mnli_loss: 0.7524
09/07 05:51:24 PM: Update 2255: task mnli, batch 255 (2245): accuracy: 0.6754, mnli_loss: 0.7509
09/07 05:51:34 PM: Update 2292: task mnli, batch 292 (2282): accuracy: 0.6767, mnli_loss: 0.7474
09/07 05:51:44 PM: Update 2331: task mnli, batch 331 (2321): accuracy: 0.6789, mnli_loss: 0.7444
09/07 05:51:55 PM: Update 2369: task mnli, batch 369 (2359): accuracy: 0.6815, mnli_loss: 0.7413
09/07 05:52:05 PM: Update 2408: task mnli, batch 408 (2398): accuracy: 0.6852, mnli_loss: 0.7350
09/07 05:52:15 PM: Update 2446: task mnli, batch 446 (2436): accuracy: 0.6875, mnli_loss: 0.7307
09/07 05:52:25 PM: Update 2485: task mnli, batch 485 (2475): accuracy: 0.6888, mnli_loss: 0.7300
09/07 05:52:35 PM: Update 2513: task mnli, batch 513 (2503): accuracy: 0.6874, mnli_loss: 0.7321
09/07 05:52:45 PM: Update 2552: task mnli, batch 552 (2542): accuracy: 0.6881, mnli_loss: 0.7311
09/07 05:52:56 PM: Update 2590: task mnli, batch 590 (2580): accuracy: 0.6881, mnli_loss: 0.7301
09/07 05:53:06 PM: Update 2629: task mnli, batch 629 (2619): accuracy: 0.6905, mnli_loss: 0.7275
09/07 05:53:16 PM: Update 2667: task mnli, batch 667 (2657): accuracy: 0.6903, mnli_loss: 0.7279
09/07 05:53:26 PM: Update 2704: task mnli, batch 704 (2694): accuracy: 0.6898, mnli_loss: 0.7286
09/07 05:53:36 PM: Update 2741: task mnli, batch 741 (2731): accuracy: 0.6911, mnli_loss: 0.7272
09/07 05:53:46 PM: Update 2779: task mnli, batch 779 (2769): accuracy: 0.6910, mnli_loss: 0.7267
09/07 05:53:56 PM: Update 2822: task mnli, batch 822 (2812): accuracy: 0.6923, mnli_loss: 0.7257
09/07 05:54:06 PM: Update 2864: task mnli, batch 864 (2854): accuracy: 0.6935, mnli_loss: 0.7243
09/07 05:54:16 PM: Update 2906: task mnli, batch 906 (2896): accuracy: 0.6944, mnli_loss: 0.7223
09/07 05:54:27 PM: Update 2939: task mnli, batch 939 (2929): accuracy: 0.6946, mnli_loss: 0.7217
09/07 05:54:37 PM: Update 2982: task mnli, batch 982 (2972): accuracy: 0.6955, mnli_loss: 0.7200
09/07 05:54:41 PM: ***** Step 3000 / Validation 3 *****
09/07 05:54:41 PM: mnli: trained on 1000 batches, 0.061 epochs
09/07 05:54:41 PM: wnli: trained on 0 batches, 0.000 epochs
09/07 05:54:41 PM: Validating...
09/07 05:54:47 PM: Evaluate: task mnli, batch 90 (209): accuracy: 0.7250, mnli_loss: 0.6604
09/07 05:54:55 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.4583, wnli_loss: 0.6792
09/07 05:54:55 PM: Best result seen so far for mnli.
09/07 05:54:55 PM: Best result seen so far for micro.
09/07 05:54:55 PM: Best result seen so far for macro.
09/07 05:54:55 PM: Updating LR scheduler:
09/07 05:54:55 PM: 	Best result seen so far for macro_avg: 0.621
09/07 05:54:55 PM: 	# validation passes without improvement: 0
09/07 05:54:55 PM: mnli_loss: training: 0.719513 validation: 0.662070
09/07 05:54:55 PM: wnli_loss: training: 0.000000 validation: 0.681892
09/07 05:54:55 PM: macro_avg: validation: 0.620563
09/07 05:54:55 PM: micro_avg: validation: 0.717216
09/07 05:54:55 PM: mnli_accuracy: training: 0.695737 validation: 0.720000
09/07 05:54:55 PM: wnli_accuracy: validation: 0.521127
09/07 05:54:55 PM: Global learning rate: 1e-05
09/07 05:54:55 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 05:54:57 PM: Update 3006: task mnli, batch 6 (2996): accuracy: 0.7708, mnli_loss: 0.6335
09/07 05:55:07 PM: Update 3048: task mnli, batch 48 (3038): accuracy: 0.7049, mnli_loss: 0.6960
09/07 05:55:17 PM: Update 3090: task mnli, batch 90 (3080): accuracy: 0.7088, mnli_loss: 0.6903
09/07 05:55:27 PM: Update 3132: task mnli, batch 132 (3122): accuracy: 0.7093, mnli_loss: 0.6862
09/07 05:55:38 PM: Update 3173: task mnli, batch 173 (3163): accuracy: 0.7069, mnli_loss: 0.6890
09/07 05:55:48 PM: Update 3212: task mnli, batch 212 (3202): accuracy: 0.7040, mnli_loss: 0.6971
09/07 05:55:58 PM: Update 3254: task mnli, batch 254 (3244): accuracy: 0.7080, mnli_loss: 0.6945
09/07 05:56:08 PM: Update 3296: task mnli, batch 296 (3286): accuracy: 0.7097, mnli_loss: 0.6907
09/07 05:56:18 PM: Update 3338: task mnli, batch 338 (3328): accuracy: 0.7103, mnli_loss: 0.6883
09/07 05:56:23 PM: Update 3348: task wnli, batch 1 (11): accuracy: 0.6250, wnli_loss: 0.6579
09/07 05:56:28 PM: Update 3371: task mnli, batch 370 (3360): accuracy: 0.7115, mnli_loss: 0.6850
09/07 05:56:38 PM: Update 3411: task mnli, batch 410 (3400): accuracy: 0.7139, mnli_loss: 0.6828
09/07 05:56:41 PM: Update 3420: task wnli, batch 2 (12): accuracy: 0.5000, wnli_loss: 0.6927
09/07 05:56:48 PM: Update 3453: task mnli, batch 451 (3441): accuracy: 0.7130, mnli_loss: 0.6831
09/07 05:56:59 PM: Update 3494: task mnli, batch 492 (3482): accuracy: 0.7126, mnli_loss: 0.6841
09/07 05:57:09 PM: Update 3536: task mnli, batch 534 (3524): accuracy: 0.7117, mnli_loss: 0.6845
09/07 05:57:19 PM: Update 3578: task mnli, batch 576 (3566): accuracy: 0.7120, mnli_loss: 0.6839
09/07 05:57:29 PM: Update 3620: task mnli, batch 618 (3608): accuracy: 0.7132, mnli_loss: 0.6822
09/07 05:57:39 PM: Update 3661: task mnli, batch 659 (3649): accuracy: 0.7143, mnli_loss: 0.6814
09/07 05:57:49 PM: Update 3702: task mnli, batch 700 (3690): accuracy: 0.7153, mnli_loss: 0.6802
09/07 05:57:59 PM: Update 3745: task mnli, batch 743 (3733): accuracy: 0.7161, mnli_loss: 0.6787
09/07 05:58:10 PM: Update 3779: task mnli, batch 777 (3767): accuracy: 0.7170, mnli_loss: 0.6764
09/07 05:58:20 PM: Update 3821: task mnli, batch 819 (3809): accuracy: 0.7189, mnli_loss: 0.6735
09/07 05:58:30 PM: Update 3864: task mnli, batch 862 (3852): accuracy: 0.7202, mnli_loss: 0.6718
09/07 05:58:40 PM: Update 3906: task mnli, batch 904 (3894): accuracy: 0.7202, mnli_loss: 0.6703
09/07 05:58:50 PM: Update 3949: task mnli, batch 947 (3937): accuracy: 0.7212, mnli_loss: 0.6698
09/07 05:59:00 PM: Update 3989: task mnli, batch 987 (3977): accuracy: 0.7204, mnli_loss: 0.6703
09/07 05:59:03 PM: ***** Step 4000 / Validation 4 *****
09/07 05:59:03 PM: mnli: trained on 998 batches, 0.061 epochs
09/07 05:59:03 PM: wnli: trained on 2 batches, 0.074 epochs
09/07 05:59:03 PM: Validating...
09/07 05:59:10 PM: Evaluate: task mnli, batch 110 (209): accuracy: 0.7352, mnli_loss: 0.6266
09/07 05:59:17 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.6667, wnli_loss: 0.6633
09/07 05:59:17 PM: Best result seen so far for mnli.
09/07 05:59:17 PM: Best result seen so far for micro.
09/07 05:59:17 PM: Best result seen so far for macro.
09/07 05:59:17 PM: Updating LR scheduler:
09/07 05:59:17 PM: 	Best result seen so far for macro_avg: 0.654
09/07 05:59:17 PM: 	# validation passes without improvement: 0
09/07 05:59:17 PM: mnli_loss: training: 0.670020 validation: 0.614847
09/07 05:59:17 PM: wnli_loss: training: 0.692668 validation: 0.690385
09/07 05:59:17 PM: macro_avg: validation: 0.653690
09/07 05:59:17 PM: micro_avg: validation: 0.741471
09/07 05:59:17 PM: mnli_accuracy: training: 0.720505 validation: 0.744000
09/07 05:59:17 PM: wnli_accuracy: training: 0.500000 validation: 0.563380
09/07 05:59:17 PM: Global learning rate: 1e-05
09/07 05:59:17 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 05:59:20 PM: Update 4010: task mnli, batch 10 (3998): accuracy: 0.6917, mnli_loss: 0.6971
09/07 05:59:28 PM: Update 4043: task wnli, batch 1 (13): accuracy: 0.3750, wnli_loss: 0.7478
09/07 05:59:30 PM: Update 4051: task mnli, batch 50 (4038): accuracy: 0.7217, mnli_loss: 0.6603
09/07 05:59:41 PM: Update 4091: task mnli, batch 90 (4078): accuracy: 0.7278, mnli_loss: 0.6438
09/07 05:59:51 PM: Update 4133: task mnli, batch 132 (4120): accuracy: 0.7301, mnli_loss: 0.6481
09/07 06:00:01 PM: Update 4177: task mnli, batch 176 (4164): accuracy: 0.7273, mnli_loss: 0.6479
09/07 06:00:11 PM: Update 4204: task mnli, batch 203 (4191): accuracy: 0.7309, mnli_loss: 0.6451
09/07 06:00:21 PM: Update 4245: task mnli, batch 244 (4232): accuracy: 0.7293, mnli_loss: 0.6485
09/07 06:00:31 PM: Update 4287: task mnli, batch 286 (4274): accuracy: 0.7277, mnli_loss: 0.6515
09/07 06:00:41 PM: Update 4329: task mnli, batch 328 (4316): accuracy: 0.7275, mnli_loss: 0.6525
09/07 06:00:51 PM: Update 4370: task mnli, batch 369 (4357): accuracy: 0.7274, mnli_loss: 0.6510
09/07 06:01:02 PM: Update 4413: task mnli, batch 412 (4400): accuracy: 0.7281, mnli_loss: 0.6500
09/07 06:01:12 PM: Update 4454: task mnli, batch 453 (4441): accuracy: 0.7311, mnli_loss: 0.6483
09/07 06:01:22 PM: Update 4496: task mnli, batch 495 (4483): accuracy: 0.7322, mnli_loss: 0.6475
09/07 06:01:32 PM: Update 4536: task mnli, batch 535 (4523): accuracy: 0.7329, mnli_loss: 0.6464
09/07 06:01:33 PM: Update 4543: task wnli, batch 2 (14): accuracy: 0.3125, wnli_loss: 0.7705
09/07 06:01:42 PM: Update 4579: task mnli, batch 577 (4565): accuracy: 0.7324, mnli_loss: 0.6467
09/07 06:01:44 PM: Update 4585: task wnli, batch 3 (15): accuracy: 0.3472, wnli_loss: 0.7584
09/07 06:01:52 PM: Update 4612: task mnli, batch 609 (4597): accuracy: 0.7339, mnli_loss: 0.6457
09/07 06:02:02 PM: Update 4654: task mnli, batch 651 (4639): accuracy: 0.7349, mnli_loss: 0.6438
09/07 06:02:13 PM: Update 4695: task mnli, batch 692 (4680): accuracy: 0.7329, mnli_loss: 0.6455
09/07 06:02:23 PM: Update 4736: task mnli, batch 733 (4721): accuracy: 0.7334, mnli_loss: 0.6451
09/07 06:02:33 PM: Update 4777: task mnli, batch 774 (4762): accuracy: 0.7342, mnli_loss: 0.6436
09/07 06:02:43 PM: Update 4819: task mnli, batch 816 (4804): accuracy: 0.7338, mnli_loss: 0.6437
09/07 06:02:53 PM: Update 4860: task mnli, batch 857 (4845): accuracy: 0.7354, mnli_loss: 0.6414
09/07 06:03:03 PM: Update 4901: task mnli, batch 898 (4886): accuracy: 0.7352, mnli_loss: 0.6411
09/07 06:03:13 PM: Update 4943: task mnli, batch 940 (4928): accuracy: 0.7362, mnli_loss: 0.6401
09/07 06:03:23 PM: Update 4984: task mnli, batch 981 (4969): accuracy: 0.7359, mnli_loss: 0.6397
09/07 06:03:27 PM: ***** Step 5000 / Validation 5 *****
09/07 06:03:27 PM: mnli: trained on 997 batches, 0.061 epochs
09/07 06:03:27 PM: wnli: trained on 3 batches, 0.111 epochs
09/07 06:03:27 PM: Validating...
09/07 06:03:33 PM: Evaluate: task mnli, batch 96 (209): accuracy: 0.7609, mnli_loss: 0.5839
09/07 06:03:41 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.5000, wnli_loss: 0.7102
09/07 06:03:41 PM: Best result seen so far for mnli.
09/07 06:03:41 PM: Best result seen so far for micro.
09/07 06:03:41 PM: Updating LR scheduler:
09/07 06:03:41 PM: 	Best result seen so far for macro_avg: 0.654
09/07 06:03:41 PM: 	# validation passes without improvement: 1
09/07 06:03:41 PM: mnli_loss: training: 0.640182 validation: 0.588412
09/07 06:03:41 PM: wnli_loss: training: 0.758356 validation: 0.701375
09/07 06:03:41 PM: macro_avg: validation: 0.632921
09/07 06:03:41 PM: micro_avg: validation: 0.755275
09/07 06:03:41 PM: mnli_accuracy: training: 0.735530 validation: 0.758800
09/07 06:03:41 PM: wnli_accuracy: training: 0.347222 validation: 0.507042
09/07 06:03:41 PM: Global learning rate: 1e-05
09/07 06:03:41 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 06:03:43 PM: Update 5005: task mnli, batch 5 (4990): accuracy: 0.7833, mnli_loss: 0.5572
09/07 06:03:53 PM: Update 5037: task mnli, batch 37 (5022): accuracy: 0.7398, mnli_loss: 0.6146
09/07 06:04:03 PM: Update 5076: task mnli, batch 76 (5061): accuracy: 0.7467, mnli_loss: 0.6201
09/07 06:04:14 PM: Update 5116: task mnli, batch 116 (5101): accuracy: 0.7406, mnli_loss: 0.6356
09/07 06:04:24 PM: Update 5157: task mnli, batch 157 (5142): accuracy: 0.7426, mnli_loss: 0.6355
09/07 06:04:34 PM: Update 5199: task mnli, batch 199 (5184): accuracy: 0.7406, mnli_loss: 0.6352
09/07 06:04:44 PM: Update 5240: task mnli, batch 240 (5225): accuracy: 0.7401, mnli_loss: 0.6341
09/07 06:04:54 PM: Update 5282: task mnli, batch 282 (5267): accuracy: 0.7399, mnli_loss: 0.6338
09/07 06:05:04 PM: Update 5324: task mnli, batch 324 (5309): accuracy: 0.7432, mnli_loss: 0.6281
09/07 06:05:14 PM: Update 5365: task mnli, batch 365 (5350): accuracy: 0.7429, mnli_loss: 0.6254
09/07 06:05:24 PM: Update 5406: task mnli, batch 406 (5391): accuracy: 0.7409, mnli_loss: 0.6256
09/07 06:05:34 PM: Update 5440: task mnli, batch 440 (5425): accuracy: 0.7390, mnli_loss: 0.6282
09/07 06:05:45 PM: Update 5480: task mnli, batch 480 (5465): accuracy: 0.7389, mnli_loss: 0.6266
09/07 06:05:55 PM: Update 5522: task mnli, batch 522 (5507): accuracy: 0.7398, mnli_loss: 0.6250
09/07 06:06:05 PM: Update 5564: task mnli, batch 564 (5549): accuracy: 0.7393, mnli_loss: 0.6256
09/07 06:06:15 PM: Update 5606: task mnli, batch 606 (5591): accuracy: 0.7415, mnli_loss: 0.6226
09/07 06:06:25 PM: Update 5649: task mnli, batch 649 (5634): accuracy: 0.7406, mnli_loss: 0.6228
09/07 06:06:32 PM: Update 5677: task wnli, batch 1 (16): accuracy: 0.5000, wnli_loss: 0.7133
09/07 06:06:35 PM: Update 5689: task mnli, batch 688 (5673): accuracy: 0.7404, mnli_loss: 0.6221
09/07 06:06:45 PM: Update 5730: task mnli, batch 729 (5714): accuracy: 0.7401, mnli_loss: 0.6214
09/07 06:06:52 PM: Update 5758: task wnli, batch 2 (17): accuracy: 0.4583, wnli_loss: 0.7194
09/07 06:06:55 PM: Update 5772: task mnli, batch 770 (5755): accuracy: 0.7409, mnli_loss: 0.6193
09/07 06:07:06 PM: Update 5815: task mnli, batch 813 (5798): accuracy: 0.7426, mnli_loss: 0.6157
09/07 06:07:18 PM: Update 5856: task mnli, batch 854 (5839): accuracy: 0.7435, mnli_loss: 0.6155
09/07 06:07:28 PM: Update 5897: task mnli, batch 895 (5880): accuracy: 0.7440, mnli_loss: 0.6152
09/07 06:07:38 PM: Update 5938: task mnli, batch 936 (5921): accuracy: 0.7438, mnli_loss: 0.6158
09/07 06:07:44 PM: Update 5964: task wnli, batch 3 (18): accuracy: 0.5000, wnli_loss: 0.7020
09/07 06:07:48 PM: Update 5980: task mnli, batch 977 (5962): accuracy: 0.7446, mnli_loss: 0.6145
09/07 06:07:53 PM: ***** Step 6000 / Validation 6 *****
09/07 06:07:53 PM: mnli: trained on 997 batches, 0.061 epochs
09/07 06:07:53 PM: wnli: trained on 3 batches, 0.111 epochs
09/07 06:07:53 PM: Validating...
09/07 06:07:58 PM: Evaluate: task mnli, batch 73 (209): accuracy: 0.7683, mnli_loss: 0.5672
09/07 06:08:07 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.6250, wnli_loss: 0.6884
09/07 06:08:08 PM: Best result seen so far for mnli.
09/07 06:08:08 PM: Best result seen so far for wnli.
09/07 06:08:08 PM: Best result seen so far for micro.
09/07 06:08:08 PM: Best result seen so far for macro.
09/07 06:08:08 PM: Updating LR scheduler:
09/07 06:08:08 PM: 	Best result seen so far for macro_avg: 0.679
09/07 06:08:08 PM: 	# validation passes without improvement: 0
09/07 06:08:08 PM: mnli_loss: training: 0.614240 validation: 0.579673
09/07 06:08:08 PM: wnli_loss: training: 0.702006 validation: 0.692237
09/07 06:08:08 PM: macro_avg: validation: 0.678675
09/07 06:08:08 PM: micro_avg: validation: 0.763360
09/07 06:08:08 PM: mnli_accuracy: training: 0.744729 validation: 0.765800
09/07 06:08:08 PM: wnli_accuracy: training: 0.500000 validation: 0.591549
09/07 06:08:08 PM: Global learning rate: 1e-05
09/07 06:08:08 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 06:08:09 PM: Update 6001: task mnli, batch 1 (5983): accuracy: 0.8333, mnli_loss: 0.6457
09/07 06:08:19 PM: Update 6042: task mnli, batch 42 (6024): accuracy: 0.7530, mnli_loss: 0.5945
09/07 06:08:29 PM: Update 6082: task mnli, batch 82 (6064): accuracy: 0.7566, mnli_loss: 0.5965
09/07 06:08:40 PM: Update 6123: task mnli, batch 123 (6105): accuracy: 0.7585, mnli_loss: 0.5939
09/07 06:08:50 PM: Update 6165: task mnli, batch 165 (6147): accuracy: 0.7558, mnli_loss: 0.5968
09/07 06:09:00 PM: Update 6207: task mnli, batch 207 (6189): accuracy: 0.7568, mnli_loss: 0.5952
09/07 06:09:10 PM: Update 6249: task mnli, batch 249 (6231): accuracy: 0.7523, mnli_loss: 0.6019
09/07 06:09:20 PM: Update 6280: task mnli, batch 280 (6262): accuracy: 0.7554, mnli_loss: 0.5944
09/07 06:09:30 PM: Update 6320: task mnli, batch 320 (6302): accuracy: 0.7551, mnli_loss: 0.5945
09/07 06:09:40 PM: Update 6360: task mnli, batch 360 (6342): accuracy: 0.7530, mnli_loss: 0.5980
09/07 06:09:50 PM: Update 6404: task mnli, batch 404 (6386): accuracy: 0.7547, mnli_loss: 0.5947
09/07 06:09:55 PM: Update 6420: task wnli, batch 1 (19): accuracy: 0.4583, wnli_loss: 0.7390
09/07 06:10:00 PM: Update 6444: task mnli, batch 443 (6425): accuracy: 0.7556, mnli_loss: 0.5944
09/07 06:10:11 PM: Update 6484: task mnli, batch 483 (6465): accuracy: 0.7561, mnli_loss: 0.5934
09/07 06:10:21 PM: Update 6525: task mnli, batch 524 (6506): accuracy: 0.7551, mnli_loss: 0.5952
09/07 06:10:31 PM: Update 6569: task mnli, batch 568 (6550): accuracy: 0.7575, mnli_loss: 0.5931
09/07 06:10:41 PM: Update 6611: task mnli, batch 610 (6592): accuracy: 0.7566, mnli_loss: 0.5937
09/07 06:10:51 PM: Update 6652: task mnli, batch 651 (6633): accuracy: 0.7548, mnli_loss: 0.5977
09/07 06:11:03 PM: Update 6692: task mnli, batch 691 (6673): accuracy: 0.7554, mnli_loss: 0.5977
09/07 06:11:10 PM: Update 6722: task wnli, batch 2 (20): accuracy: 0.5000, wnli_loss: 0.7102
09/07 06:11:13 PM: Update 6735: task mnli, batch 733 (6715): accuracy: 0.7559, mnli_loss: 0.5963
09/07 06:11:24 PM: Update 6779: task mnli, batch 777 (6759): accuracy: 0.7568, mnli_loss: 0.5940
09/07 06:11:34 PM: Update 6818: task mnli, batch 816 (6798): accuracy: 0.7560, mnli_loss: 0.5965
09/07 06:11:44 PM: Update 6859: task mnli, batch 857 (6839): accuracy: 0.7562, mnli_loss: 0.5961
09/07 06:11:54 PM: Update 6900: task mnli, batch 898 (6880): accuracy: 0.7565, mnli_loss: 0.5969
09/07 06:12:04 PM: Update 6940: task mnli, batch 938 (6920): accuracy: 0.7567, mnli_loss: 0.5960
09/07 06:12:14 PM: Update 6980: task mnli, batch 978 (6960): accuracy: 0.7569, mnli_loss: 0.5949
09/07 06:12:19 PM: ***** Step 7000 / Validation 7 *****
09/07 06:12:19 PM: mnli: trained on 998 batches, 0.061 epochs
09/07 06:12:19 PM: wnli: trained on 2 batches, 0.074 epochs
09/07 06:12:19 PM: Validating...
09/07 06:12:24 PM: Evaluate: task mnli, batch 77 (209): accuracy: 0.7679, mnli_loss: 0.5574
09/07 06:12:33 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.3750, wnli_loss: 0.7946
09/07 06:12:33 PM: Updating LR scheduler:
09/07 06:12:33 PM: 	Best result seen so far for macro_avg: 0.679
09/07 06:12:33 PM: 	# validation passes without improvement: 1
09/07 06:12:33 PM: mnli_loss: training: 0.596073 validation: 0.571141
09/07 06:12:33 PM: wnli_loss: training: 0.710165 validation: 0.747973
09/07 06:12:33 PM: macro_avg: validation: 0.607152
09/07 06:12:33 PM: micro_avg: validation: 0.759219
09/07 06:12:33 PM: mnli_accuracy: training: 0.756058 validation: 0.763600
09/07 06:12:33 PM: wnli_accuracy: training: 0.500000 validation: 0.450704
09/07 06:12:33 PM: Global learning rate: 1e-05
09/07 06:12:33 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 06:12:34 PM: Update 7002: task mnli, batch 2 (6982): accuracy: 0.7292, mnli_loss: 0.5444
09/07 06:12:45 PM: Update 7044: task mnli, batch 44 (7024): accuracy: 0.7557, mnli_loss: 0.5763
09/07 06:12:55 PM: Update 7087: task mnli, batch 87 (7067): accuracy: 0.7514, mnli_loss: 0.5808
09/07 06:13:05 PM: Update 7121: task mnli, batch 121 (7101): accuracy: 0.7514, mnli_loss: 0.5802
09/07 06:13:15 PM: Update 7162: task mnli, batch 162 (7142): accuracy: 0.7536, mnli_loss: 0.5782
09/07 06:13:25 PM: Update 7204: task mnli, batch 204 (7184): accuracy: 0.7508, mnli_loss: 0.5804
09/07 06:13:35 PM: Update 7245: task mnli, batch 245 (7225): accuracy: 0.7527, mnli_loss: 0.5811
09/07 06:13:45 PM: Update 7285: task mnli, batch 285 (7265): accuracy: 0.7529, mnli_loss: 0.5847
09/07 06:13:55 PM: Update 7326: task mnli, batch 326 (7306): accuracy: 0.7544, mnli_loss: 0.5838
09/07 06:14:06 PM: Update 7368: task mnli, batch 368 (7348): accuracy: 0.7540, mnli_loss: 0.5861
09/07 06:14:16 PM: Update 7411: task mnli, batch 411 (7391): accuracy: 0.7566, mnli_loss: 0.5833
09/07 06:14:26 PM: Update 7453: task mnli, batch 453 (7433): accuracy: 0.7560, mnli_loss: 0.5859
09/07 06:14:27 PM: Update 7456: task wnli, batch 1 (21): accuracy: 0.6250, wnli_loss: 0.6911
09/07 06:14:36 PM: Update 7494: task mnli, batch 493 (7473): accuracy: 0.7569, mnli_loss: 0.5851
09/07 06:14:46 PM: Update 7528: task mnli, batch 527 (7507): accuracy: 0.7577, mnli_loss: 0.5844
09/07 06:14:57 PM: Update 7571: task mnli, batch 570 (7550): accuracy: 0.7591, mnli_loss: 0.5842
09/07 06:15:07 PM: Update 7612: task mnli, batch 611 (7591): accuracy: 0.7599, mnli_loss: 0.5846
09/07 06:15:17 PM: Update 7653: task mnli, batch 652 (7632): accuracy: 0.7611, mnli_loss: 0.5816
09/07 06:15:27 PM: Update 7693: task mnli, batch 692 (7672): accuracy: 0.7629, mnli_loss: 0.5798
09/07 06:15:29 PM: Update 7702: task wnli, batch 2 (22): accuracy: 0.4792, wnli_loss: 0.7078
09/07 06:15:37 PM: Update 7733: task mnli, batch 731 (7711): accuracy: 0.7644, mnli_loss: 0.5773
09/07 06:15:47 PM: Update 7777: task mnli, batch 775 (7755): accuracy: 0.7657, mnli_loss: 0.5739
09/07 06:15:57 PM: Update 7819: task mnli, batch 817 (7797): accuracy: 0.7664, mnli_loss: 0.5724
09/07 06:16:07 PM: Update 7862: task mnli, batch 860 (7840): accuracy: 0.7664, mnli_loss: 0.5734
09/07 06:16:17 PM: Update 7903: task mnli, batch 901 (7881): accuracy: 0.7668, mnli_loss: 0.5739
09/07 06:16:27 PM: Update 7943: task mnli, batch 941 (7921): accuracy: 0.7663, mnli_loss: 0.5755
09/07 06:16:38 PM: Update 7976: task mnli, batch 974 (7954): accuracy: 0.7661, mnli_loss: 0.5754
09/07 06:16:43 PM: ***** Step 8000 / Validation 8 *****
09/07 06:16:43 PM: mnli: trained on 998 batches, 0.061 epochs
09/07 06:16:43 PM: wnli: trained on 2 batches, 0.074 epochs
09/07 06:16:43 PM: Validating...
09/07 06:16:48 PM: Evaluate: task mnli, batch 52 (209): accuracy: 0.7821, mnli_loss: 0.5242
09/07 06:16:58 PM: Evaluate: task mnli, batch 200 (209): accuracy: 0.7773, mnli_loss: 0.5418
09/07 06:16:58 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.6667, wnli_loss: 0.6802
09/07 06:16:58 PM: Best result seen so far for mnli.
09/07 06:16:58 PM: Best result seen so far for micro.
09/07 06:16:58 PM: Updating LR scheduler:
09/07 06:16:58 PM: 	Best result seen so far for macro_avg: 0.679
09/07 06:16:58 PM: 	# validation passes without improvement: 2
09/07 06:16:58 PM: mnli_loss: training: 0.577216 validation: 0.546773
09/07 06:16:58 PM: wnli_loss: training: 0.707756 validation: 0.695723
09/07 06:16:58 PM: macro_avg: validation: 0.669290
09/07 06:16:58 PM: micro_avg: validation: 0.772234
09/07 06:16:58 PM: mnli_accuracy: training: 0.765296 validation: 0.775200
09/07 06:16:58 PM: wnli_accuracy: training: 0.479167 validation: 0.563380
09/07 06:16:58 PM: Global learning rate: 1e-05
09/07 06:16:58 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 06:17:08 PM: Update 8036: task mnli, batch 36 (8014): accuracy: 0.7662, mnli_loss: 0.5620
09/07 06:17:18 PM: Update 8077: task mnli, batch 77 (8055): accuracy: 0.7689, mnli_loss: 0.5613
09/07 06:17:28 PM: Update 8118: task mnli, batch 118 (8096): accuracy: 0.7737, mnli_loss: 0.5643
09/07 06:17:38 PM: Update 8159: task mnli, batch 159 (8137): accuracy: 0.7699, mnli_loss: 0.5729
09/07 06:17:48 PM: Update 8201: task mnli, batch 201 (8179): accuracy: 0.7687, mnli_loss: 0.5758
09/07 06:17:58 PM: Update 8242: task mnli, batch 242 (8220): accuracy: 0.7622, mnli_loss: 0.5841
09/07 06:18:08 PM: Update 8284: task mnli, batch 284 (8262): accuracy: 0.7604, mnli_loss: 0.5862
09/07 06:18:19 PM: Update 8325: task mnli, batch 325 (8303): accuracy: 0.7614, mnli_loss: 0.5842
09/07 06:18:30 PM: Update 8363: task mnli, batch 363 (8341): accuracy: 0.7624, mnli_loss: 0.5817
09/07 06:18:40 PM: Update 8405: task mnli, batch 405 (8383): accuracy: 0.7618, mnli_loss: 0.5806
09/07 06:18:50 PM: Update 8447: task mnli, batch 447 (8425): accuracy: 0.7645, mnli_loss: 0.5758
09/07 06:19:00 PM: Update 8489: task mnli, batch 489 (8467): accuracy: 0.7632, mnli_loss: 0.5794
09/07 06:19:10 PM: Update 8531: task mnli, batch 531 (8509): accuracy: 0.7626, mnli_loss: 0.5789
09/07 06:19:21 PM: Update 8572: task mnli, batch 572 (8550): accuracy: 0.7641, mnli_loss: 0.5765
09/07 06:19:31 PM: Update 8612: task mnli, batch 612 (8590): accuracy: 0.7641, mnli_loss: 0.5759
09/07 06:19:41 PM: Update 8652: task mnli, batch 652 (8630): accuracy: 0.7644, mnli_loss: 0.5756
09/07 06:19:51 PM: Update 8693: task mnli, batch 693 (8671): accuracy: 0.7639, mnli_loss: 0.5758
09/07 06:20:01 PM: Update 8734: task mnli, batch 734 (8712): accuracy: 0.7635, mnli_loss: 0.5772
09/07 06:20:11 PM: Update 8776: task mnli, batch 776 (8754): accuracy: 0.7647, mnli_loss: 0.5749
09/07 06:20:21 PM: Update 8808: task mnli, batch 808 (8786): accuracy: 0.7649, mnli_loss: 0.5754
09/07 06:20:31 PM: Update 8850: task mnli, batch 850 (8828): accuracy: 0.7646, mnli_loss: 0.5754
09/07 06:20:41 PM: Update 8894: task mnli, batch 894 (8872): accuracy: 0.7639, mnli_loss: 0.5762
09/07 06:20:51 PM: Update 8935: task mnli, batch 935 (8913): accuracy: 0.7636, mnli_loss: 0.5770
09/07 06:21:01 PM: Update 8977: task mnli, batch 977 (8955): accuracy: 0.7642, mnli_loss: 0.5757
09/07 06:21:07 PM: ***** Step 9000 / Validation 9 *****
09/07 06:21:07 PM: mnli: trained on 1000 batches, 0.061 epochs
09/07 06:21:07 PM: wnli: trained on 0 batches, 0.000 epochs
09/07 06:21:07 PM: Validating...
09/07 06:21:12 PM: Evaluate: task mnli, batch 68 (209): accuracy: 0.7812, mnli_loss: 0.5302
09/07 06:21:21 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.6667, wnli_loss: 0.6763
09/07 06:21:21 PM: Best result seen so far for mnli.
09/07 06:21:21 PM: Best result seen so far for micro.
09/07 06:21:21 PM: Updating LR scheduler:
09/07 06:21:21 PM: 	Best result seen so far for macro_avg: 0.679
09/07 06:21:21 PM: 	# validation passes without improvement: 3
09/07 06:21:21 PM: mnli_loss: training: 0.575442 validation: 0.545067
09/07 06:21:21 PM: wnli_loss: training: 0.000000 validation: 0.692618
09/07 06:21:21 PM: macro_avg: validation: 0.678532
09/07 06:21:21 PM: micro_avg: validation: 0.776770
09/07 06:21:21 PM: mnli_accuracy: training: 0.764134 validation: 0.779600
09/07 06:21:21 PM: wnli_accuracy: validation: 0.577465
09/07 06:21:21 PM: Global learning rate: 1e-05
09/07 06:21:21 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 06:21:22 PM: Update 9001: task mnli, batch 1 (8979): accuracy: 0.6667, mnli_loss: 0.7002
09/07 06:21:32 PM: Update 9042: task mnli, batch 42 (9020): accuracy: 0.7827, mnli_loss: 0.5439
09/07 06:21:42 PM: Update 9082: task mnli, batch 82 (9060): accuracy: 0.7708, mnli_loss: 0.5840
09/07 06:21:53 PM: Update 9125: task mnli, batch 125 (9103): accuracy: 0.7763, mnli_loss: 0.5722
09/07 06:22:03 PM: Update 9164: task mnli, batch 164 (9142): accuracy: 0.7718, mnli_loss: 0.5724
09/07 06:22:13 PM: Update 9197: task mnli, batch 197 (9175): accuracy: 0.7695, mnli_loss: 0.5725
09/07 06:22:23 PM: Update 9239: task mnli, batch 239 (9217): accuracy: 0.7704, mnli_loss: 0.5700
09/07 06:22:33 PM: Update 9282: task mnli, batch 282 (9260): accuracy: 0.7743, mnli_loss: 0.5605
09/07 06:22:43 PM: Update 9323: task mnli, batch 323 (9301): accuracy: 0.7754, mnli_loss: 0.5580
09/07 06:22:53 PM: Update 9364: task mnli, batch 364 (9342): accuracy: 0.7741, mnli_loss: 0.5616
09/07 06:23:03 PM: Update 9406: task mnli, batch 406 (9384): accuracy: 0.7750, mnli_loss: 0.5611
09/07 06:23:13 PM: Update 9448: task mnli, batch 448 (9426): accuracy: 0.7746, mnli_loss: 0.5614
09/07 06:23:24 PM: Update 9490: task mnli, batch 490 (9468): accuracy: 0.7751, mnli_loss: 0.5616
09/07 06:23:34 PM: Update 9530: task mnli, batch 530 (9508): accuracy: 0.7760, mnli_loss: 0.5614
09/07 06:23:44 PM: Update 9570: task mnli, batch 570 (9548): accuracy: 0.7768, mnli_loss: 0.5612
09/07 06:23:54 PM: Update 9611: task mnli, batch 611 (9589): accuracy: 0.7752, mnli_loss: 0.5634
09/07 06:24:04 PM: Update 9641: task mnli, batch 641 (9619): accuracy: 0.7745, mnli_loss: 0.5638
09/07 06:24:09 PM: Update 9658: task wnli, batch 1 (23): accuracy: 0.4583, wnli_loss: 0.7195
09/07 06:24:14 PM: Update 9682: task mnli, batch 681 (9659): accuracy: 0.7755, mnli_loss: 0.5626
09/07 06:24:24 PM: Update 9724: task mnli, batch 723 (9701): accuracy: 0.7735, mnli_loss: 0.5640
09/07 06:24:34 PM: Update 9766: task mnli, batch 765 (9743): accuracy: 0.7728, mnli_loss: 0.5641
09/07 06:24:45 PM: Update 9807: task mnli, batch 806 (9784): accuracy: 0.7740, mnli_loss: 0.5632
09/07 06:24:55 PM: Update 9850: task mnli, batch 849 (9827): accuracy: 0.7730, mnli_loss: 0.5635
09/07 06:25:05 PM: Update 9892: task mnli, batch 891 (9869): accuracy: 0.7733, mnli_loss: 0.5619
09/07 06:25:15 PM: Update 9934: task mnli, batch 933 (9911): accuracy: 0.7746, mnli_loss: 0.5598
09/07 06:25:25 PM: Update 9976: task mnli, batch 975 (9953): accuracy: 0.7743, mnli_loss: 0.5603
09/07 06:25:31 PM: ***** Step 10000 / Validation 10 *****
09/07 06:25:31 PM: mnli: trained on 999 batches, 0.061 epochs
09/07 06:25:31 PM: wnli: trained on 1 batches, 0.037 epochs
09/07 06:25:31 PM: Validating...
09/07 06:25:35 PM: Evaluate: task mnli, batch 60 (209): accuracy: 0.7965, mnli_loss: 0.4967
09/07 06:25:45 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.5000, wnli_loss: 0.7098
09/07 06:25:46 PM: Best result seen so far for mnli.
09/07 06:25:46 PM: Best result seen so far for micro.
09/07 06:25:46 PM: Updating LR scheduler:
09/07 06:25:46 PM: 	Best result seen so far for macro_avg: 0.679
09/07 06:25:46 PM: 	# validation passes without improvement: 4
09/07 06:25:46 PM: mnli_loss: training: 0.561179 validation: 0.523051
09/07 06:25:46 PM: wnli_loss: training: 0.719482 validation: 0.700873
09/07 06:25:46 PM: macro_avg: validation: 0.641979
09/07 06:25:46 PM: micro_avg: validation: 0.786827
09/07 06:25:46 PM: mnli_accuracy: training: 0.774082 validation: 0.791000
09/07 06:25:46 PM: wnli_accuracy: training: 0.458333 validation: 0.492958
09/07 06:25:46 PM: Global learning rate: 1e-05
09/07 06:25:46 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 06:25:47 PM: Update 10001: task mnli, batch 1 (9978): accuracy: 0.7500, mnli_loss: 0.5288
09/07 06:25:57 PM: Update 10032: task mnli, batch 32 (10009): accuracy: 0.7934, mnli_loss: 0.5331
09/07 06:26:07 PM: Update 10075: task mnli, batch 75 (10052): accuracy: 0.7902, mnli_loss: 0.5155
09/07 06:26:17 PM: Update 10115: task mnli, batch 115 (10092): accuracy: 0.7878, mnli_loss: 0.5291
09/07 06:26:19 PM: Update 10126: task wnli, batch 1 (24): accuracy: 0.3750, wnli_loss: 0.7395
09/07 06:26:27 PM: Update 10156: task mnli, batch 155 (10132): accuracy: 0.7904, mnli_loss: 0.5272
09/07 06:26:37 PM: Update 10197: task mnli, batch 196 (10173): accuracy: 0.7866, mnli_loss: 0.5365
09/07 06:26:47 PM: Update 10238: task mnli, batch 237 (10214): accuracy: 0.7873, mnli_loss: 0.5350
09/07 06:26:57 PM: Update 10278: task mnli, batch 277 (10254): accuracy: 0.7904, mnli_loss: 0.5323
09/07 06:27:07 PM: Update 10319: task mnli, batch 318 (10295): accuracy: 0.7879, mnli_loss: 0.5353
09/07 06:27:17 PM: Update 10361: task mnli, batch 360 (10337): accuracy: 0.7865, mnli_loss: 0.5365
09/07 06:27:28 PM: Update 10403: task mnli, batch 402 (10379): accuracy: 0.7877, mnli_loss: 0.5355
09/07 06:27:38 PM: Update 10446: task mnli, batch 445 (10422): accuracy: 0.7884, mnli_loss: 0.5346
09/07 06:27:48 PM: Update 10478: task mnli, batch 477 (10454): accuracy: 0.7868, mnli_loss: 0.5373
09/07 06:27:55 PM: Update 10508: task wnli, batch 2 (25): accuracy: 0.3125, wnli_loss: 0.7439
09/07 06:27:58 PM: Update 10520: task mnli, batch 518 (10495): accuracy: 0.7869, mnli_loss: 0.5365
09/07 06:28:08 PM: Update 10562: task mnli, batch 560 (10537): accuracy: 0.7866, mnli_loss: 0.5354
09/07 06:28:19 PM: Update 10602: task mnli, batch 600 (10577): accuracy: 0.7857, mnli_loss: 0.5374
09/07 06:28:29 PM: Update 10643: task mnli, batch 641 (10618): accuracy: 0.7859, mnli_loss: 0.5356
09/07 06:28:39 PM: Update 10685: task mnli, batch 683 (10660): accuracy: 0.7860, mnli_loss: 0.5337
09/07 06:28:49 PM: Update 10727: task mnli, batch 725 (10702): accuracy: 0.7857, mnli_loss: 0.5337
09/07 06:28:59 PM: Update 10769: task mnli, batch 767 (10744): accuracy: 0.7848, mnli_loss: 0.5356
09/07 06:29:09 PM: Update 10810: task mnli, batch 808 (10785): accuracy: 0.7836, mnli_loss: 0.5368
09/07 06:29:19 PM: Update 10852: task mnli, batch 850 (10827): accuracy: 0.7839, mnli_loss: 0.5363
09/07 06:29:29 PM: Update 10885: task mnli, batch 883 (10860): accuracy: 0.7835, mnli_loss: 0.5369
09/07 06:29:39 PM: Update 10926: task mnli, batch 924 (10901): accuracy: 0.7839, mnli_loss: 0.5358
09/07 06:29:49 PM: Update 10966: task mnli, batch 964 (10941): accuracy: 0.7844, mnli_loss: 0.5357
09/07 06:29:57 PM: ***** Step 11000 / Validation 11 *****
09/07 06:29:57 PM: mnli: trained on 998 batches, 0.061 epochs
09/07 06:29:57 PM: wnli: trained on 2 batches, 0.074 epochs
09/07 06:29:57 PM: Validating...
09/07 06:29:59 PM: Evaluate: task mnli, batch 28 (209): accuracy: 0.7872, mnli_loss: 0.5060
09/07 06:30:09 PM: Evaluate: task mnli, batch 177 (209): accuracy: 0.7987, mnli_loss: 0.5013
09/07 06:30:12 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.3333, wnli_loss: 0.7173
09/07 06:30:12 PM: Best result seen so far for mnli.
09/07 06:30:12 PM: Best result seen so far for micro.
09/07 06:30:12 PM: Updating LR scheduler:
09/07 06:30:12 PM: 	Best result seen so far for macro_avg: 0.679
09/07 06:30:12 PM: 	# validation passes without improvement: 0
09/07 06:30:12 PM: mnli_loss: training: 0.536213 validation: 0.510422
09/07 06:30:12 PM: wnli_loss: training: 0.743875 validation: 0.705450
09/07 06:30:12 PM: macro_avg: validation: 0.595583
09/07 06:30:12 PM: micro_avg: validation: 0.791165
09/07 06:30:12 PM: mnli_accuracy: training: 0.783810 validation: 0.796800
09/07 06:30:12 PM: wnli_accuracy: training: 0.312500 validation: 0.394366
09/07 06:30:12 PM: Global learning rate: 5e-06
09/07 06:30:12 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 06:30:19 PM: Update 11028: task mnli, batch 28 (11003): accuracy: 0.7753, mnli_loss: 0.5243
09/07 06:30:29 PM: Update 11071: task mnli, batch 71 (11046): accuracy: 0.7817, mnli_loss: 0.5202
09/07 06:30:40 PM: Update 11112: task mnli, batch 112 (11087): accuracy: 0.7790, mnli_loss: 0.5404
09/07 06:30:50 PM: Update 11154: task mnli, batch 154 (11129): accuracy: 0.7771, mnli_loss: 0.5427
09/07 06:31:00 PM: Update 11195: task mnli, batch 195 (11170): accuracy: 0.7793, mnli_loss: 0.5342
09/07 06:31:04 PM: Update 11210: task wnli, batch 1 (26): accuracy: 0.4583, wnli_loss: 0.6932
09/07 06:31:10 PM: Update 11237: task mnli, batch 236 (11211): accuracy: 0.7777, mnli_loss: 0.5393
09/07 06:31:20 PM: Update 11277: task mnli, batch 276 (11251): accuracy: 0.7794, mnli_loss: 0.5342
09/07 06:31:30 PM: Update 11305: task mnli, batch 304 (11279): accuracy: 0.7777, mnli_loss: 0.5350
09/07 06:31:41 PM: Update 11346: task mnli, batch 345 (11320): accuracy: 0.7767, mnli_loss: 0.5397
09/07 06:31:51 PM: Update 11388: task mnli, batch 387 (11362): accuracy: 0.7786, mnli_loss: 0.5378
09/07 06:32:01 PM: Update 11430: task mnli, batch 429 (11404): accuracy: 0.7794, mnli_loss: 0.5376
09/07 06:32:11 PM: Update 11473: task mnli, batch 472 (11447): accuracy: 0.7799, mnli_loss: 0.5365
09/07 06:32:17 PM: Update 11498: task wnli, batch 2 (27): accuracy: 0.5000, wnli_loss: 0.7094
09/07 06:32:21 PM: Update 11516: task mnli, batch 514 (11489): accuracy: 0.7811, mnli_loss: 0.5353
09/07 06:32:31 PM: Update 11558: task mnli, batch 556 (11531): accuracy: 0.7809, mnli_loss: 0.5344
09/07 06:32:42 PM: Update 11600: task mnli, batch 598 (11573): accuracy: 0.7801, mnli_loss: 0.5358
09/07 06:32:52 PM: Update 11644: task mnli, batch 642 (11617): accuracy: 0.7805, mnli_loss: 0.5333
09/07 06:33:02 PM: Update 11684: task mnli, batch 682 (11657): accuracy: 0.7801, mnli_loss: 0.5343
09/07 06:33:12 PM: Update 11717: task mnli, batch 715 (11690): accuracy: 0.7792, mnli_loss: 0.5359
09/07 06:33:22 PM: Update 11758: task wnli, batch 3 (28): accuracy: 0.4407, wnli_loss: 0.7158
09/07 06:33:23 PM: Update 11759: task mnli, batch 756 (11731): accuracy: 0.7799, mnli_loss: 0.5354
09/07 06:33:33 PM: Update 11800: task mnli, batch 797 (11772): accuracy: 0.7798, mnli_loss: 0.5364
09/07 06:33:43 PM: Update 11842: task mnli, batch 839 (11814): accuracy: 0.7800, mnli_loss: 0.5372
09/07 06:33:53 PM: Update 11883: task mnli, batch 880 (11855): accuracy: 0.7798, mnli_loss: 0.5376
09/07 06:34:03 PM: Update 11926: task mnli, batch 923 (11898): accuracy: 0.7800, mnli_loss: 0.5374
09/07 06:34:13 PM: Update 11965: task mnli, batch 962 (11937): accuracy: 0.7799, mnli_loss: 0.5379
09/07 06:34:22 PM: Update 11999: task wnli, batch 4 (29): accuracy: 0.5060, wnli_loss: 0.7057
09/07 06:34:22 PM: ***** Step 12000 / Validation 12 *****
09/07 06:34:22 PM: mnli: trained on 996 batches, 0.061 epochs
09/07 06:34:22 PM: wnli: trained on 4 batches, 0.148 epochs
09/07 06:34:22 PM: Validating...
09/07 06:34:23 PM: Evaluate: task mnli, batch 22 (209): accuracy: 0.8125, mnli_loss: 0.4589
09/07 06:34:33 PM: Evaluate: task mnli, batch 171 (209): accuracy: 0.8034, mnli_loss: 0.4943
09/07 06:34:36 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.5833, wnli_loss: 0.6863
09/07 06:34:36 PM: Best result seen so far for mnli.
09/07 06:34:36 PM: Best result seen so far for micro.
09/07 06:34:36 PM: Updating LR scheduler:
09/07 06:34:36 PM: 	Best result seen so far for macro_avg: 0.679
09/07 06:34:36 PM: 	# validation passes without improvement: 1
09/07 06:34:36 PM: mnli_loss: training: 0.535924 validation: 0.503268
09/07 06:34:36 PM: wnli_loss: training: 0.705698 validation: 0.692035
09/07 06:34:36 PM: macro_avg: validation: 0.661363
09/07 06:34:36 PM: micro_avg: validation: 0.797673
09/07 06:34:36 PM: mnli_accuracy: training: 0.781062 validation: 0.801600
09/07 06:34:36 PM: wnli_accuracy: training: 0.506024 validation: 0.521127
09/07 06:34:36 PM: Global learning rate: 5e-06
09/07 06:34:36 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 06:34:44 PM: Update 12028: task mnli, batch 28 (11999): accuracy: 0.8125, mnli_loss: 0.5012
09/07 06:34:54 PM: Update 12068: task mnli, batch 68 (12039): accuracy: 0.7880, mnli_loss: 0.5224
09/07 06:35:04 PM: Update 12111: task mnli, batch 111 (12082): accuracy: 0.7917, mnli_loss: 0.5181
09/07 06:35:11 PM: Update 12131: task wnli, batch 1 (30): accuracy: 0.5000, wnli_loss: 0.6870
09/07 06:35:14 PM: Update 12144: task mnli, batch 143 (12114): accuracy: 0.7926, mnli_loss: 0.5137
09/07 06:35:24 PM: Update 12188: task mnli, batch 187 (12158): accuracy: 0.7937, mnli_loss: 0.5120
09/07 06:35:34 PM: Update 12229: task mnli, batch 228 (12199): accuracy: 0.7910, mnli_loss: 0.5154
09/07 06:35:44 PM: Update 12270: task mnli, batch 269 (12240): accuracy: 0.7866, mnli_loss: 0.5238
09/07 06:35:55 PM: Update 12311: task mnli, batch 310 (12281): accuracy: 0.7869, mnli_loss: 0.5238
09/07 06:36:05 PM: Update 12352: task mnli, batch 351 (12322): accuracy: 0.7860, mnli_loss: 0.5255
09/07 06:36:15 PM: Update 12393: task mnli, batch 392 (12363): accuracy: 0.7851, mnli_loss: 0.5293
09/07 06:36:25 PM: Update 12433: task mnli, batch 432 (12403): accuracy: 0.7863, mnli_loss: 0.5273
09/07 06:36:35 PM: Update 12476: task mnli, batch 475 (12446): accuracy: 0.7898, mnli_loss: 0.5222
09/07 06:36:45 PM: Update 12517: task mnli, batch 516 (12487): accuracy: 0.7880, mnli_loss: 0.5250
09/07 06:36:48 PM: Update 12530: task wnli, batch 2 (31): accuracy: 0.5625, wnli_loss: 0.6779
09/07 06:36:55 PM: Update 12546: task mnli, batch 544 (12515): accuracy: 0.7873, mnli_loss: 0.5256
09/07 06:37:05 PM: Update 12586: task mnli, batch 584 (12555): accuracy: 0.7869, mnli_loss: 0.5251
09/07 06:37:15 PM: Update 12628: task mnli, batch 626 (12597): accuracy: 0.7876, mnli_loss: 0.5238
09/07 06:37:26 PM: Update 12671: task mnli, batch 669 (12640): accuracy: 0.7889, mnli_loss: 0.5219
09/07 06:37:36 PM: Update 12712: task mnli, batch 710 (12681): accuracy: 0.7889, mnli_loss: 0.5213
09/07 06:37:43 PM: Update 12740: task wnli, batch 3 (32): accuracy: 0.5278, wnli_loss: 0.6996
09/07 06:37:46 PM: Update 12753: task mnli, batch 750 (12721): accuracy: 0.7883, mnli_loss: 0.5218
09/07 06:37:56 PM: Update 12794: task mnli, batch 791 (12762): accuracy: 0.7884, mnli_loss: 0.5220
09/07 06:38:06 PM: Update 12835: task mnli, batch 832 (12803): accuracy: 0.7894, mnli_loss: 0.5209
09/07 06:38:16 PM: Update 12877: task mnli, batch 874 (12845): accuracy: 0.7901, mnli_loss: 0.5196
09/07 06:38:26 PM: Update 12919: task mnli, batch 916 (12887): accuracy: 0.7908, mnli_loss: 0.5180
09/07 06:38:36 PM: Update 12957: task wnli, batch 4 (33): accuracy: 0.5208, wnli_loss: 0.7058
09/07 06:38:37 PM: Update 12959: task mnli, batch 955 (12926): accuracy: 0.7899, mnli_loss: 0.5184
09/07 06:38:47 PM: Update 12992: task mnli, batch 988 (12959): accuracy: 0.7900, mnli_loss: 0.5184
09/07 06:38:49 PM: ***** Step 13000 / Validation 13 *****
09/07 06:38:49 PM: mnli: trained on 996 batches, 0.061 epochs
09/07 06:38:49 PM: wnli: trained on 4 batches, 0.148 epochs
09/07 06:38:49 PM: Validating...
09/07 06:38:57 PM: Evaluate: task mnli, batch 120 (209): accuracy: 0.7962, mnli_loss: 0.4992
09/07 06:39:03 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.4583, wnli_loss: 0.7014
09/07 06:39:03 PM: Updating LR scheduler:
09/07 06:39:03 PM: 	Best result seen so far for macro_avg: 0.679
09/07 06:39:03 PM: 	# validation passes without improvement: 2
09/07 06:39:03 PM: mnli_loss: training: 0.519347 validation: 0.503693
09/07 06:39:03 PM: wnli_loss: training: 0.705796 validation: 0.696534
09/07 06:39:03 PM: macro_avg: validation: 0.666706
09/07 06:39:03 PM: micro_avg: validation: 0.794518
09/07 06:39:03 PM: mnli_accuracy: training: 0.789657 validation: 0.798200
09/07 06:39:03 PM: wnli_accuracy: training: 0.520833 validation: 0.535211
09/07 06:39:03 PM: Global learning rate: 5e-06
09/07 06:39:03 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 06:39:07 PM: Update 13013: task mnli, batch 13 (12980): accuracy: 0.7981, mnli_loss: 0.5172
09/07 06:39:17 PM: Update 13054: task mnli, batch 54 (13021): accuracy: 0.7832, mnli_loss: 0.5349
09/07 06:39:27 PM: Update 13096: task mnli, batch 96 (13063): accuracy: 0.7869, mnli_loss: 0.5345
09/07 06:39:37 PM: Update 13136: task mnli, batch 136 (13103): accuracy: 0.7831, mnli_loss: 0.5442
09/07 06:39:47 PM: Update 13177: task mnli, batch 177 (13144): accuracy: 0.7801, mnli_loss: 0.5467
09/07 06:39:58 PM: Update 13219: task mnli, batch 219 (13186): accuracy: 0.7791, mnli_loss: 0.5454
09/07 06:40:08 PM: Update 13262: task mnli, batch 262 (13229): accuracy: 0.7796, mnli_loss: 0.5450
09/07 06:40:18 PM: Update 13304: task mnli, batch 304 (13271): accuracy: 0.7829, mnli_loss: 0.5386
09/07 06:40:28 PM: Update 13345: task mnli, batch 345 (13312): accuracy: 0.7835, mnli_loss: 0.5388
09/07 06:40:39 PM: Update 13378: task mnli, batch 378 (13345): accuracy: 0.7851, mnli_loss: 0.5380
09/07 06:40:49 PM: Update 13418: task mnli, batch 418 (13385): accuracy: 0.7845, mnli_loss: 0.5362
09/07 06:40:59 PM: Update 13460: task mnli, batch 460 (13427): accuracy: 0.7854, mnli_loss: 0.5355
09/07 06:41:09 PM: Update 13502: task mnli, batch 502 (13469): accuracy: 0.7854, mnli_loss: 0.5343
09/07 06:41:19 PM: Update 13543: task mnli, batch 543 (13510): accuracy: 0.7869, mnli_loss: 0.5310
09/07 06:41:29 PM: Update 13585: task mnli, batch 585 (13552): accuracy: 0.7877, mnli_loss: 0.5294
09/07 06:41:39 PM: Update 13625: task mnli, batch 625 (13592): accuracy: 0.7869, mnli_loss: 0.5310
09/07 06:41:50 PM: Update 13666: task mnli, batch 666 (13633): accuracy: 0.7873, mnli_loss: 0.5290
09/07 06:42:00 PM: Update 13710: task mnli, batch 710 (13677): accuracy: 0.7888, mnli_loss: 0.5259
09/07 06:42:10 PM: Update 13753: task mnli, batch 753 (13720): accuracy: 0.7885, mnli_loss: 0.5266
09/07 06:42:20 PM: Update 13791: task mnli, batch 791 (13758): accuracy: 0.7881, mnli_loss: 0.5266
09/07 06:42:30 PM: Update 13823: task mnli, batch 823 (13790): accuracy: 0.7877, mnli_loss: 0.5283
09/07 06:42:40 PM: Update 13864: task mnli, batch 864 (13831): accuracy: 0.7877, mnli_loss: 0.5287
09/07 06:42:50 PM: Update 13905: task mnli, batch 905 (13872): accuracy: 0.7871, mnli_loss: 0.5301
09/07 06:43:01 PM: Update 13947: task mnli, batch 947 (13914): accuracy: 0.7875, mnli_loss: 0.5304
09/07 06:43:11 PM: Update 13988: task mnli, batch 988 (13955): accuracy: 0.7871, mnli_loss: 0.5300
09/07 06:43:14 PM: ***** Step 14000 / Validation 14 *****
09/07 06:43:14 PM: mnli: trained on 1000 batches, 0.061 epochs
09/07 06:43:14 PM: wnli: trained on 0 batches, 0.000 epochs
09/07 06:43:14 PM: Validating...
09/07 06:43:21 PM: Evaluate: task mnli, batch 104 (209): accuracy: 0.7973, mnli_loss: 0.5019
09/07 06:43:28 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.5000, wnli_loss: 0.7018
09/07 06:43:28 PM: Best result seen so far for macro.
09/07 06:43:28 PM: Updating LR scheduler:
09/07 06:43:28 PM: 	Best result seen so far for macro_avg: 0.687
09/07 06:43:28 PM: 	# validation passes without improvement: 0
09/07 06:43:28 PM: mnli_loss: training: 0.530398 validation: 0.509992
09/07 06:43:28 PM: wnli_loss: training: 0.000000 validation: 0.695015
09/07 06:43:28 PM: macro_avg: validation: 0.687232
09/07 06:43:28 PM: micro_avg: validation: 0.793926
09/07 06:43:28 PM: mnli_accuracy: training: 0.787025 validation: 0.797000
09/07 06:43:28 PM: wnli_accuracy: validation: 0.577465
09/07 06:43:28 PM: Global learning rate: 5e-06
09/07 06:43:28 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 06:43:31 PM: Update 14007: task mnli, batch 7 (13974): accuracy: 0.7798, mnli_loss: 0.5049
09/07 06:43:41 PM: Update 14049: task mnli, batch 49 (14016): accuracy: 0.7908, mnli_loss: 0.5121
09/07 06:43:51 PM: Update 14091: task mnli, batch 91 (14058): accuracy: 0.7912, mnli_loss: 0.5228
09/07 06:44:01 PM: Update 14132: task mnli, batch 132 (14099): accuracy: 0.7850, mnli_loss: 0.5392
09/07 06:44:11 PM: Update 14174: task mnli, batch 174 (14141): accuracy: 0.7900, mnli_loss: 0.5376
09/07 06:44:23 PM: Update 14212: task mnli, batch 212 (14179): accuracy: 0.7882, mnli_loss: 0.5420
09/07 06:44:33 PM: Update 14252: task mnli, batch 252 (14219): accuracy: 0.7866, mnli_loss: 0.5443
09/07 06:44:43 PM: Update 14293: task mnli, batch 293 (14260): accuracy: 0.7866, mnli_loss: 0.5409
09/07 06:44:53 PM: Update 14336: task mnli, batch 336 (14303): accuracy: 0.7859, mnli_loss: 0.5374
09/07 06:45:03 PM: Update 14376: task mnli, batch 376 (14343): accuracy: 0.7866, mnli_loss: 0.5333
09/07 06:45:13 PM: Update 14416: task mnli, batch 416 (14383): accuracy: 0.7881, mnli_loss: 0.5297
09/07 06:45:24 PM: Update 14458: task mnli, batch 458 (14425): accuracy: 0.7878, mnli_loss: 0.5291
09/07 06:45:34 PM: Update 14502: task mnli, batch 502 (14469): accuracy: 0.7886, mnli_loss: 0.5270
09/07 06:45:44 PM: Update 14543: task mnli, batch 543 (14510): accuracy: 0.7875, mnli_loss: 0.5297
09/07 06:45:54 PM: Update 14583: task mnli, batch 583 (14550): accuracy: 0.7883, mnli_loss: 0.5275
09/07 06:46:04 PM: Update 14624: task mnli, batch 624 (14591): accuracy: 0.7889, mnli_loss: 0.5249
09/07 06:46:15 PM: Update 14657: task mnli, batch 657 (14624): accuracy: 0.7880, mnli_loss: 0.5248
09/07 06:46:25 PM: Update 14697: task mnli, batch 697 (14664): accuracy: 0.7882, mnli_loss: 0.5229
09/07 06:46:35 PM: Update 14739: task mnli, batch 739 (14706): accuracy: 0.7893, mnli_loss: 0.5207
09/07 06:46:45 PM: Update 14781: task mnli, batch 781 (14748): accuracy: 0.7891, mnli_loss: 0.5198
09/07 06:46:55 PM: Update 14821: task mnli, batch 821 (14788): accuracy: 0.7893, mnli_loss: 0.5194
09/07 06:47:05 PM: Update 14862: task mnli, batch 862 (14829): accuracy: 0.7881, mnli_loss: 0.5220
09/07 06:47:16 PM: Update 14904: task mnli, batch 904 (14871): accuracy: 0.7887, mnli_loss: 0.5211
09/07 06:47:26 PM: Update 14944: task mnli, batch 944 (14911): accuracy: 0.7887, mnli_loss: 0.5203
09/07 06:47:36 PM: Update 14987: task mnli, batch 987 (14954): accuracy: 0.7903, mnli_loss: 0.5175
09/07 06:47:39 PM: ***** Step 15000 / Validation 15 *****
09/07 06:47:39 PM: mnli: trained on 1000 batches, 0.061 epochs
09/07 06:47:39 PM: wnli: trained on 0 batches, 0.000 epochs
09/07 06:47:39 PM: Validating...
09/07 06:47:46 PM: Evaluate: task mnli, batch 85 (209): accuracy: 0.8093, mnli_loss: 0.4738
09/07 06:47:55 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.5000, wnli_loss: 0.6931
09/07 06:47:55 PM: Best result seen so far for mnli.
09/07 06:47:55 PM: Best result seen so far for wnli.
09/07 06:47:55 PM: Best result seen so far for micro.
09/07 06:47:55 PM: Best result seen so far for macro.
09/07 06:47:55 PM: Updating LR scheduler:
09/07 06:47:55 PM: 	Best result seen so far for macro_avg: 0.705
09/07 06:47:55 PM: 	# validation passes without improvement: 0
09/07 06:47:55 PM: mnli_loss: training: 0.518085 validation: 0.496158
09/07 06:47:55 PM: wnli_loss: training: 0.000000 validation: 0.691849
09/07 06:47:55 PM: macro_avg: validation: 0.705417
09/07 06:47:55 PM: micro_avg: validation: 0.802406
09/07 06:47:55 PM: mnli_accuracy: training: 0.790068 validation: 0.805200
09/07 06:47:55 PM: wnli_accuracy: validation: 0.605634
09/07 06:47:55 PM: Global learning rate: 5e-06
09/07 06:47:55 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 06:47:56 PM: Update 15001: task mnli, batch 1 (14968): accuracy: 0.9167, mnli_loss: 0.2633
09/07 06:48:06 PM: Update 15043: task mnli, batch 43 (15010): accuracy: 0.8014, mnli_loss: 0.5318
09/07 06:48:17 PM: Update 15077: task mnli, batch 77 (15044): accuracy: 0.7935, mnli_loss: 0.5297
09/07 06:48:27 PM: Update 15119: task mnli, batch 119 (15086): accuracy: 0.8013, mnli_loss: 0.5138
09/07 06:48:37 PM: Update 15161: task mnli, batch 161 (15128): accuracy: 0.7956, mnli_loss: 0.5241
09/07 06:48:47 PM: Update 15203: task mnli, batch 203 (15170): accuracy: 0.7952, mnli_loss: 0.5238
09/07 06:48:57 PM: Update 15244: task mnli, batch 244 (15211): accuracy: 0.7963, mnli_loss: 0.5225
09/07 06:49:08 PM: Update 15285: task mnli, batch 285 (15252): accuracy: 0.7976, mnli_loss: 0.5218
09/07 06:49:18 PM: Update 15327: task mnli, batch 327 (15294): accuracy: 0.7983, mnli_loss: 0.5213
09/07 06:49:28 PM: Update 15367: task mnli, batch 367 (15334): accuracy: 0.7975, mnli_loss: 0.5225
09/07 06:49:38 PM: Update 15408: task mnli, batch 408 (15375): accuracy: 0.7933, mnli_loss: 0.5262
09/07 06:49:48 PM: Update 15448: task mnli, batch 448 (15415): accuracy: 0.7923, mnli_loss: 0.5243
09/07 06:49:58 PM: Update 15480: task mnli, batch 480 (15447): accuracy: 0.7916, mnli_loss: 0.5253
09/07 06:50:08 PM: Update 15521: task mnli, batch 521 (15488): accuracy: 0.7908, mnli_loss: 0.5259
09/07 06:50:11 PM: Update 15532: task wnli, batch 1 (34): accuracy: 0.7083, wnli_loss: 0.6405
09/07 06:50:18 PM: Update 15561: task mnli, batch 560 (15527): accuracy: 0.7926, mnli_loss: 0.5230
09/07 06:50:29 PM: Update 15602: task mnli, batch 601 (15568): accuracy: 0.7914, mnli_loss: 0.5262
09/07 06:50:39 PM: Update 15644: task mnli, batch 643 (15610): accuracy: 0.7917, mnli_loss: 0.5236
09/07 06:50:49 PM: Update 15684: task mnli, batch 683 (15650): accuracy: 0.7912, mnli_loss: 0.5239
09/07 06:50:59 PM: Update 15727: task mnli, batch 726 (15693): accuracy: 0.7906, mnli_loss: 0.5236
09/07 06:51:09 PM: Update 15767: task mnli, batch 766 (15733): accuracy: 0.7910, mnli_loss: 0.5232
09/07 06:51:13 PM: Update 15782: task wnli, batch 2 (35): accuracy: 0.6042, wnli_loss: 0.6676
09/07 06:51:19 PM: Update 15809: task mnli, batch 807 (15774): accuracy: 0.7920, mnli_loss: 0.5212
09/07 06:51:27 PM: Update 15840: task wnli, batch 3 (36): accuracy: 0.5556, wnli_loss: 0.6792
09/07 06:51:30 PM: Update 15850: task mnli, batch 847 (15814): accuracy: 0.7927, mnli_loss: 0.5193
09/07 06:51:40 PM: Update 15884: task mnli, batch 881 (15848): accuracy: 0.7930, mnli_loss: 0.5196
09/07 06:51:42 PM: Update 15893: task wnli, batch 4 (37): accuracy: 0.5729, wnli_loss: 0.6801
09/07 06:51:50 PM: Update 15925: task mnli, batch 921 (15888): accuracy: 0.7937, mnli_loss: 0.5195
09/07 06:52:00 PM: Update 15967: task mnli, batch 963 (15930): accuracy: 0.7934, mnli_loss: 0.5193
09/07 06:52:08 PM: ***** Step 16000 / Validation 16 *****
09/07 06:52:08 PM: mnli: trained on 996 batches, 0.061 epochs
09/07 06:52:08 PM: wnli: trained on 4 batches, 0.148 epochs
09/07 06:52:08 PM: Validating...
09/07 06:52:10 PM: Evaluate: task mnli, batch 30 (209): accuracy: 0.8042, mnli_loss: 0.4859
09/07 06:52:20 PM: Evaluate: task mnli, batch 178 (209): accuracy: 0.8066, mnli_loss: 0.4947
09/07 06:52:22 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.4167, wnli_loss: 0.7175
09/07 06:52:23 PM: Updating LR scheduler:
09/07 06:52:23 PM: 	Best result seen so far for macro_avg: 0.705
09/07 06:52:23 PM: 	# validation passes without improvement: 1
09/07 06:52:23 PM: mnli_loss: training: 0.519328 validation: 0.502409
09/07 06:52:23 PM: wnli_loss: training: 0.680092 validation: 0.704256
09/07 06:52:23 PM: macro_avg: validation: 0.620510
09/07 06:52:23 PM: micro_avg: validation: 0.799251
09/07 06:52:23 PM: mnli_accuracy: training: 0.792755 validation: 0.804400
09/07 06:52:23 PM: wnli_accuracy: training: 0.572917 validation: 0.436620
09/07 06:52:23 PM: Global learning rate: 5e-06
09/07 06:52:23 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 06:52:30 PM: Update 16027: task mnli, batch 27 (15990): accuracy: 0.7809, mnli_loss: 0.5197
09/07 06:52:40 PM: Update 16067: task mnli, batch 67 (16030): accuracy: 0.7935, mnli_loss: 0.5094
09/07 06:52:50 PM: Update 16108: task mnli, batch 108 (16071): accuracy: 0.7990, mnli_loss: 0.5032
09/07 06:53:00 PM: Update 16149: task mnli, batch 149 (16112): accuracy: 0.7961, mnli_loss: 0.5087
09/07 06:53:11 PM: Update 16190: task mnli, batch 190 (16153): accuracy: 0.7936, mnli_loss: 0.5189
09/07 06:53:21 PM: Update 16232: task mnli, batch 232 (16195): accuracy: 0.7945, mnli_loss: 0.5202
09/07 06:53:26 PM: Update 16254: task wnli, batch 1 (38): accuracy: 0.5000, wnli_loss: 0.6833
09/07 06:53:31 PM: Update 16273: task mnli, batch 272 (16235): accuracy: 0.7989, mnli_loss: 0.5145
09/07 06:53:41 PM: Update 16310: task mnli, batch 309 (16272): accuracy: 0.7989, mnli_loss: 0.5164
09/07 06:53:51 PM: Update 16351: task mnli, batch 350 (16313): accuracy: 0.7973, mnli_loss: 0.5171
09/07 06:54:01 PM: Update 16393: task mnli, batch 392 (16355): accuracy: 0.7959, mnli_loss: 0.5171
09/07 06:54:12 PM: Update 16428: task mnli, batch 427 (16390): accuracy: 0.7945, mnli_loss: 0.5183
09/07 06:54:22 PM: Update 16470: task mnli, batch 469 (16432): accuracy: 0.7940, mnli_loss: 0.5189
09/07 06:54:32 PM: Update 16510: task mnli, batch 509 (16472): accuracy: 0.7955, mnli_loss: 0.5173
09/07 06:54:42 PM: Update 16552: task mnli, batch 551 (16514): accuracy: 0.7977, mnli_loss: 0.5133
09/07 06:54:52 PM: Update 16593: task mnli, batch 592 (16555): accuracy: 0.7973, mnli_loss: 0.5130
09/07 06:55:02 PM: Update 16634: task mnli, batch 633 (16596): accuracy: 0.7970, mnli_loss: 0.5129
09/07 06:55:12 PM: Update 16675: task mnli, batch 674 (16637): accuracy: 0.7981, mnli_loss: 0.5111
09/07 06:55:22 PM: Update 16717: task mnli, batch 716 (16679): accuracy: 0.7989, mnli_loss: 0.5086
09/07 06:55:32 PM: Update 16760: task mnli, batch 759 (16722): accuracy: 0.7989, mnli_loss: 0.5088
09/07 06:55:43 PM: Update 16801: task mnli, batch 800 (16763): accuracy: 0.7986, mnli_loss: 0.5096
09/07 06:55:53 PM: Update 16832: task mnli, batch 831 (16794): accuracy: 0.7996, mnli_loss: 0.5073
09/07 06:56:03 PM: Update 16873: task mnli, batch 872 (16835): accuracy: 0.7992, mnli_loss: 0.5081
09/07 06:56:14 PM: Update 16916: task mnli, batch 915 (16878): accuracy: 0.8001, mnli_loss: 0.5073
09/07 06:56:24 PM: Update 16957: task mnli, batch 956 (16919): accuracy: 0.8011, mnli_loss: 0.5049
09/07 06:56:34 PM: Update 16998: task mnli, batch 997 (16960): accuracy: 0.8008, mnli_loss: 0.5050
09/07 06:56:34 PM: ***** Step 17000 / Validation 17 *****
09/07 06:56:34 PM: mnli: trained on 999 batches, 0.061 epochs
09/07 06:56:34 PM: wnli: trained on 1 batches, 0.037 epochs
09/07 06:56:34 PM: Validating...
09/07 06:56:44 PM: Evaluate: task mnli, batch 142 (209): accuracy: 0.8052, mnli_loss: 0.4874
09/07 06:56:48 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.5000, wnli_loss: 0.7027
09/07 06:56:48 PM: Best result seen so far for mnli.
09/07 06:56:48 PM: Updating LR scheduler:
09/07 06:56:48 PM: 	Best result seen so far for macro_avg: 0.705
09/07 06:56:48 PM: 	# validation passes without improvement: 2
09/07 06:56:48 PM: mnli_loss: training: 0.505431 validation: 0.487870
09/07 06:56:48 PM: wnli_loss: training: 0.683335 validation: 0.695575
09/07 06:56:48 PM: macro_avg: validation: 0.649879
09/07 06:56:48 PM: micro_avg: validation: 0.802406
09/07 06:56:48 PM: mnli_accuracy: training: 0.800752 validation: 0.806800
09/07 06:56:48 PM: wnli_accuracy: training: 0.500000 validation: 0.492958
09/07 06:56:48 PM: Global learning rate: 5e-06
09/07 06:56:48 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 06:56:54 PM: Update 17018: task mnli, batch 18 (16980): accuracy: 0.7801, mnli_loss: 0.5334
09/07 06:57:04 PM: Update 17061: task mnli, batch 61 (17023): accuracy: 0.8033, mnli_loss: 0.4848
09/07 06:57:14 PM: Update 17103: task mnli, batch 103 (17065): accuracy: 0.7981, mnli_loss: 0.4931
09/07 06:57:24 PM: Update 17144: task mnli, batch 144 (17106): accuracy: 0.8001, mnli_loss: 0.4934
09/07 06:57:26 PM: Update 17151: task wnli, batch 1 (39): accuracy: 0.4167, wnli_loss: 0.6963
09/07 06:57:34 PM: Update 17187: task mnli, batch 186 (17148): accuracy: 0.8047, mnli_loss: 0.4846
09/07 06:57:45 PM: Update 17228: task mnli, batch 227 (17189): accuracy: 0.8051, mnli_loss: 0.4858
09/07 06:57:55 PM: Update 17262: task mnli, batch 261 (17223): accuracy: 0.8020, mnli_loss: 0.4940
09/07 06:58:05 PM: Update 17304: task mnli, batch 303 (17265): accuracy: 0.8037, mnli_loss: 0.4905
09/07 06:58:15 PM: Update 17345: task mnli, batch 344 (17306): accuracy: 0.8041, mnli_loss: 0.4902
09/07 06:58:25 PM: Update 17387: task mnli, batch 386 (17348): accuracy: 0.8034, mnli_loss: 0.4903
09/07 06:58:35 PM: Update 17428: task mnli, batch 427 (17389): accuracy: 0.8012, mnli_loss: 0.4927
09/07 06:58:45 PM: Update 17470: task mnli, batch 469 (17431): accuracy: 0.8015, mnli_loss: 0.4924
09/07 06:58:56 PM: Update 17511: task mnli, batch 510 (17472): accuracy: 0.8013, mnli_loss: 0.4912
09/07 06:59:06 PM: Update 17552: task mnli, batch 551 (17513): accuracy: 0.7989, mnli_loss: 0.4954
09/07 06:59:13 PM: Update 17579: task wnli, batch 2 (40): accuracy: 0.4167, wnli_loss: 0.7088
09/07 06:59:16 PM: Update 17593: task mnli, batch 591 (17553): accuracy: 0.7997, mnli_loss: 0.4941
09/07 06:59:26 PM: Update 17636: task mnli, batch 634 (17596): accuracy: 0.8001, mnli_loss: 0.4936
09/07 06:59:27 PM: Update 17640: task wnli, batch 3 (41): accuracy: 0.4444, wnli_loss: 0.7153
09/07 06:59:36 PM: Update 17670: task mnli, batch 667 (17629): accuracy: 0.8003, mnli_loss: 0.4941
09/07 06:59:46 PM: Update 17713: task mnli, batch 710 (17672): accuracy: 0.8006, mnli_loss: 0.4957
09/07 06:59:56 PM: Update 17754: task mnli, batch 751 (17713): accuracy: 0.8025, mnli_loss: 0.4925
09/07 07:00:06 PM: Update 17795: task mnli, batch 792 (17754): accuracy: 0.8014, mnli_loss: 0.4947
09/07 07:00:17 PM: Update 17836: task mnli, batch 833 (17795): accuracy: 0.8016, mnli_loss: 0.4944
09/07 07:00:27 PM: Update 17878: task mnli, batch 875 (17837): accuracy: 0.8018, mnli_loss: 0.4943
09/07 07:00:37 PM: Update 17920: task mnli, batch 917 (17879): accuracy: 0.8024, mnli_loss: 0.4941
09/07 07:00:47 PM: Update 17961: task mnli, batch 958 (17920): accuracy: 0.8028, mnli_loss: 0.4944
09/07 07:00:53 PM: Update 17986: task wnli, batch 4 (42): accuracy: 0.4479, wnli_loss: 0.7231
09/07 07:00:57 PM: ***** Step 18000 / Validation 18 *****
09/07 07:00:57 PM: mnli: trained on 995 batches, 0.061 epochs
09/07 07:00:57 PM: wnli: trained on 5 batches, 0.185 epochs
09/07 07:00:57 PM: Validating...
09/07 07:00:57 PM: Evaluate: task mnli, batch 7 (209): accuracy: 0.7917, mnli_loss: 0.4527
09/07 07:01:07 PM: Evaluate: task mnli, batch 156 (209): accuracy: 0.8130, mnli_loss: 0.4755
09/07 07:01:11 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.5417, wnli_loss: 0.6825
09/07 07:01:11 PM: Best result seen so far for mnli.
09/07 07:01:11 PM: Best result seen so far for micro.
09/07 07:01:11 PM: Updating LR scheduler:
09/07 07:01:11 PM: 	Best result seen so far for macro_avg: 0.705
09/07 07:01:11 PM: 	# validation passes without improvement: 3
09/07 07:01:11 PM: mnli_loss: training: 0.493604 validation: 0.479605
09/07 07:01:11 PM: wnli_loss: training: 0.720417 validation: 0.689753
09/07 07:01:11 PM: macro_avg: validation: 0.672506
09/07 07:01:11 PM: micro_avg: validation: 0.805955
09/07 07:01:11 PM: mnli_accuracy: training: 0.803386 validation: 0.809800
09/07 07:01:11 PM: wnli_accuracy: training: 0.458333 validation: 0.535211
09/07 07:01:11 PM: Global learning rate: 5e-06
09/07 07:01:11 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 07:01:18 PM: Update 18020: task mnli, batch 20 (17977): accuracy: 0.8021, mnli_loss: 0.4858
09/07 07:01:28 PM: Update 18061: task mnli, batch 61 (18018): accuracy: 0.8060, mnli_loss: 0.4831
09/07 07:01:38 PM: Update 18095: task mnli, batch 95 (18052): accuracy: 0.8041, mnli_loss: 0.4897
09/07 07:01:48 PM: Update 18137: task mnli, batch 137 (18094): accuracy: 0.8079, mnli_loss: 0.4816
09/07 07:01:58 PM: Update 18181: task mnli, batch 181 (18138): accuracy: 0.8086, mnli_loss: 0.4781
09/07 07:02:08 PM: Update 18223: task mnli, batch 223 (18180): accuracy: 0.8080, mnli_loss: 0.4820
09/07 07:02:18 PM: Update 18264: task mnli, batch 264 (18221): accuracy: 0.8091, mnli_loss: 0.4787
09/07 07:02:29 PM: Update 18306: task mnli, batch 306 (18263): accuracy: 0.8103, mnli_loss: 0.4769
09/07 07:02:39 PM: Update 18348: task mnli, batch 348 (18305): accuracy: 0.8078, mnli_loss: 0.4823
09/07 07:02:49 PM: Update 18388: task mnli, batch 388 (18345): accuracy: 0.8069, mnli_loss: 0.4840
09/07 07:02:59 PM: Update 18429: task mnli, batch 429 (18386): accuracy: 0.8067, mnli_loss: 0.4867
09/07 07:03:09 PM: Update 18469: task mnli, batch 469 (18426): accuracy: 0.8041, mnli_loss: 0.4905
09/07 07:03:20 PM: Update 18505: task mnli, batch 505 (18462): accuracy: 0.8019, mnli_loss: 0.4934
09/07 07:03:31 PM: Update 18546: task mnli, batch 546 (18503): accuracy: 0.8025, mnli_loss: 0.4924
09/07 07:03:41 PM: Update 18588: task mnli, batch 588 (18545): accuracy: 0.8031, mnli_loss: 0.4904
09/07 07:03:51 PM: Update 18630: task mnli, batch 630 (18587): accuracy: 0.8040, mnli_loss: 0.4900
09/07 07:04:01 PM: Update 18671: task mnli, batch 671 (18628): accuracy: 0.8050, mnli_loss: 0.4883
09/07 07:04:11 PM: Update 18712: task mnli, batch 712 (18669): accuracy: 0.8034, mnli_loss: 0.4906
09/07 07:04:21 PM: Update 18754: task mnli, batch 754 (18711): accuracy: 0.8033, mnli_loss: 0.4920
09/07 07:04:31 PM: Update 18795: task mnli, batch 795 (18752): accuracy: 0.8046, mnli_loss: 0.4895
09/07 07:04:41 PM: Update 18836: task mnli, batch 836 (18793): accuracy: 0.8048, mnli_loss: 0.4894
09/07 07:04:51 PM: Update 18879: task mnli, batch 879 (18836): accuracy: 0.8049, mnli_loss: 0.4882
09/07 07:05:01 PM: Update 18920: task mnli, batch 920 (18877): accuracy: 0.8051, mnli_loss: 0.4873
09/07 07:05:11 PM: Update 18949: task mnli, batch 949 (18906): accuracy: 0.8047, mnli_loss: 0.4893
09/07 07:05:22 PM: Update 18988: task mnli, batch 988 (18945): accuracy: 0.8043, mnli_loss: 0.4892
09/07 07:05:25 PM: ***** Step 19000 / Validation 19 *****
09/07 07:05:25 PM: mnli: trained on 1000 batches, 0.061 epochs
09/07 07:05:25 PM: wnli: trained on 0 batches, 0.000 epochs
09/07 07:05:25 PM: Validating...
09/07 07:05:32 PM: Evaluate: task mnli, batch 105 (209): accuracy: 0.8103, mnli_loss: 0.4726
09/07 07:05:39 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.5417, wnli_loss: 0.6846
09/07 07:05:39 PM: Updating LR scheduler:
09/07 07:05:39 PM: 	Best result seen so far for macro_avg: 0.705
09/07 07:05:39 PM: 	# validation passes without improvement: 4
09/07 07:05:39 PM: mnli_loss: training: 0.488387 validation: 0.485261
09/07 07:05:39 PM: wnli_loss: training: 0.000000 validation: 0.689731
09/07 07:05:39 PM: macro_avg: validation: 0.672206
09/07 07:05:39 PM: micro_avg: validation: 0.805364
09/07 07:05:39 PM: mnli_accuracy: training: 0.804721 validation: 0.809200
09/07 07:05:39 PM: wnli_accuracy: validation: 0.535211
09/07 07:05:39 PM: Global learning rate: 5e-06
09/07 07:05:39 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 07:05:42 PM: Update 19008: task mnli, batch 8 (18965): accuracy: 0.8385, mnli_loss: 0.4021
09/07 07:05:52 PM: Update 19048: task mnli, batch 48 (19005): accuracy: 0.8038, mnli_loss: 0.4572
09/07 07:06:02 PM: Update 19090: task mnli, batch 90 (19047): accuracy: 0.8065, mnli_loss: 0.4722
09/07 07:06:12 PM: Update 19130: task mnli, batch 130 (19087): accuracy: 0.8071, mnli_loss: 0.4687
09/07 07:06:22 PM: Update 19173: task mnli, batch 173 (19130): accuracy: 0.8080, mnli_loss: 0.4683
09/07 07:06:32 PM: Update 19214: task mnli, batch 214 (19171): accuracy: 0.8072, mnli_loss: 0.4678
09/07 07:06:43 PM: Update 19259: task mnli, batch 259 (19216): accuracy: 0.8108, mnli_loss: 0.4639
09/07 07:06:53 PM: Update 19300: task mnli, batch 300 (19257): accuracy: 0.8108, mnli_loss: 0.4653
09/07 07:07:05 PM: Update 19339: task mnli, batch 339 (19296): accuracy: 0.8108, mnli_loss: 0.4669
09/07 07:07:15 PM: Update 19379: task mnli, batch 379 (19336): accuracy: 0.8104, mnli_loss: 0.4660
09/07 07:07:21 PM: Update 19407: task wnli, batch 1 (44): accuracy: 0.5417, wnli_loss: 0.6917
09/07 07:07:25 PM: Update 19421: task mnli, batch 420 (19377): accuracy: 0.8103, mnli_loss: 0.4643
09/07 07:07:35 PM: Update 19463: task mnli, batch 462 (19419): accuracy: 0.8125, mnli_loss: 0.4600
09/07 07:07:45 PM: Update 19502: task mnli, batch 501 (19458): accuracy: 0.8102, mnli_loss: 0.4640
09/07 07:07:55 PM: Update 19545: task mnli, batch 544 (19501): accuracy: 0.8099, mnli_loss: 0.4641
09/07 07:08:05 PM: Update 19587: task mnli, batch 586 (19543): accuracy: 0.8097, mnli_loss: 0.4654
09/07 07:08:16 PM: Update 19629: task mnli, batch 628 (19585): accuracy: 0.8117, mnli_loss: 0.4636
09/07 07:08:26 PM: Update 19670: task mnli, batch 669 (19626): accuracy: 0.8120, mnli_loss: 0.4652
09/07 07:08:36 PM: Update 19713: task mnli, batch 712 (19669): accuracy: 0.8129, mnli_loss: 0.4627
09/07 07:08:46 PM: Update 19754: task mnli, batch 753 (19710): accuracy: 0.8129, mnli_loss: 0.4654
09/07 07:08:57 PM: Update 19785: task mnli, batch 784 (19741): accuracy: 0.8119, mnli_loss: 0.4677
09/07 07:09:07 PM: Update 19826: task mnli, batch 825 (19782): accuracy: 0.8118, mnli_loss: 0.4673
09/07 07:09:17 PM: Update 19867: task mnli, batch 866 (19823): accuracy: 0.8119, mnli_loss: 0.4677
09/07 07:09:21 PM: Update 19884: task wnli, batch 2 (45): accuracy: 0.5417, wnli_loss: 0.6940
09/07 07:09:27 PM: Update 19908: task mnli, batch 906 (19863): accuracy: 0.8123, mnli_loss: 0.4684
09/07 07:09:37 PM: Update 19951: task mnli, batch 949 (19906): accuracy: 0.8117, mnli_loss: 0.4704
09/07 07:09:47 PM: Update 19992: task mnli, batch 990 (19947): accuracy: 0.8123, mnli_loss: 0.4696
09/07 07:09:49 PM: ***** Step 20000 / Validation 20 *****
09/07 07:09:49 PM: mnli: trained on 998 batches, 0.061 epochs
09/07 07:09:49 PM: wnli: trained on 2 batches, 0.074 epochs
09/07 07:09:49 PM: Validating...
09/07 07:09:57 PM: Evaluate: task mnli, batch 123 (209): accuracy: 0.8032, mnli_loss: 0.4951
09/07 07:10:03 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.4167, wnli_loss: 0.7117
09/07 07:10:03 PM: Updating LR scheduler:
09/07 07:10:03 PM: 	Best result seen so far for macro_avg: 0.705
09/07 07:10:03 PM: 	# validation passes without improvement: 0
09/07 07:10:03 PM: mnli_loss: training: 0.469700 validation: 0.497010
09/07 07:10:03 PM: wnli_loss: training: 0.694034 validation: 0.706260
09/07 07:10:03 PM: macro_avg: validation: 0.621210
09/07 07:10:03 PM: micro_avg: validation: 0.800631
09/07 07:10:03 PM: mnli_accuracy: training: 0.812333 validation: 0.805800
09/07 07:10:03 PM: wnli_accuracy: training: 0.541667 validation: 0.436620
09/07 07:10:03 PM: Global learning rate: 2.5e-06
09/07 07:10:03 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 07:10:07 PM: Update 20012: task mnli, batch 12 (19967): accuracy: 0.8160, mnli_loss: 0.4355
09/07 07:10:15 PM: Update 20045: task wnli, batch 1 (46): accuracy: 0.5417, wnli_loss: 0.6926
09/07 07:10:17 PM: Update 20056: task mnli, batch 55 (20010): accuracy: 0.8167, mnli_loss: 0.4527
09/07 07:10:28 PM: Update 20098: task mnli, batch 97 (20052): accuracy: 0.8209, mnli_loss: 0.4559
09/07 07:10:38 PM: Update 20140: task mnli, batch 139 (20094): accuracy: 0.8189, mnli_loss: 0.4561
09/07 07:10:50 PM: Update 20176: task mnli, batch 175 (20130): accuracy: 0.8161, mnli_loss: 0.4607
09/07 07:11:00 PM: Update 20217: task mnli, batch 216 (20171): accuracy: 0.8147, mnli_loss: 0.4586
09/07 07:11:10 PM: Update 20257: task mnli, batch 256 (20211): accuracy: 0.8175, mnli_loss: 0.4558
09/07 07:11:12 PM: Update 20262: task wnli, batch 2 (47): accuracy: 0.4792, wnli_loss: 0.7007
09/07 07:11:20 PM: Update 20298: task mnli, batch 296 (20251): accuracy: 0.8165, mnli_loss: 0.4587
09/07 07:11:30 PM: Update 20337: task mnli, batch 335 (20290): accuracy: 0.8200, mnli_loss: 0.4547
09/07 07:11:41 PM: Update 20379: task mnli, batch 377 (20332): accuracy: 0.8183, mnli_loss: 0.4559
09/07 07:11:51 PM: Update 20421: task mnli, batch 419 (20374): accuracy: 0.8206, mnli_loss: 0.4547
09/07 07:12:01 PM: Update 20463: task mnli, batch 461 (20416): accuracy: 0.8218, mnli_loss: 0.4530
09/07 07:12:11 PM: Update 20505: task mnli, batch 503 (20458): accuracy: 0.8219, mnli_loss: 0.4518
09/07 07:12:21 PM: Update 20546: task mnli, batch 544 (20499): accuracy: 0.8219, mnli_loss: 0.4515
09/07 07:12:31 PM: Update 20589: task mnli, batch 587 (20542): accuracy: 0.8227, mnli_loss: 0.4492
09/07 07:12:41 PM: Update 20621: task mnli, batch 619 (20574): accuracy: 0.8232, mnli_loss: 0.4476
09/07 07:12:51 PM: Update 20664: task mnli, batch 662 (20617): accuracy: 0.8252, mnli_loss: 0.4448
09/07 07:13:01 PM: Update 20705: task mnli, batch 703 (20658): accuracy: 0.8259, mnli_loss: 0.4430
09/07 07:13:12 PM: Update 20744: task mnli, batch 742 (20697): accuracy: 0.8255, mnli_loss: 0.4451
09/07 07:13:17 PM: Update 20766: task wnli, batch 3 (48): accuracy: 0.5139, wnli_loss: 0.6926
09/07 07:13:22 PM: Update 20785: task mnli, batch 782 (20737): accuracy: 0.8257, mnli_loss: 0.4448
09/07 07:13:32 PM: Update 20826: task mnli, batch 822 (20777): accuracy: 0.8253, mnli_loss: 0.4461
09/07 07:13:42 PM: Update 20867: task mnli, batch 863 (20818): accuracy: 0.8258, mnli_loss: 0.4459
09/07 07:13:52 PM: Update 20910: task mnli, batch 906 (20861): accuracy: 0.8257, mnli_loss: 0.4469
09/07 07:14:02 PM: Update 20951: task mnli, batch 947 (20902): accuracy: 0.8259, mnli_loss: 0.4462
09/07 07:14:13 PM: Update 20993: task mnli, batch 989 (20944): accuracy: 0.8267, mnli_loss: 0.4450
09/07 07:14:14 PM: ***** Step 21000 / Validation 21 *****
09/07 07:14:14 PM: mnli: trained on 996 batches, 0.061 epochs
09/07 07:14:14 PM: wnli: trained on 4 batches, 0.148 epochs
09/07 07:14:14 PM: Validating...
09/07 07:14:23 PM: Evaluate: task mnli, batch 126 (209): accuracy: 0.8092, mnli_loss: 0.4977
09/07 07:14:28 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.4583, wnli_loss: 0.7105
09/07 07:14:28 PM: Best result seen so far for mnli.
09/07 07:14:28 PM: Updating LR scheduler:
09/07 07:14:28 PM: 	Best result seen so far for macro_avg: 0.705
09/07 07:14:28 PM: 	# validation passes without improvement: 1
09/07 07:14:28 PM: mnli_loss: training: 0.444822 validation: 0.504297
09/07 07:14:28 PM: wnli_loss: training: 0.699706 validation: 0.706360
09/07 07:14:28 PM: macro_avg: validation: 0.630552
09/07 07:14:28 PM: micro_avg: validation: 0.805364
09/07 07:14:28 PM: mnli_accuracy: training: 0.826775 validation: 0.810400
09/07 07:14:28 PM: wnli_accuracy: training: 0.510417 validation: 0.450704
09/07 07:14:28 PM: Global learning rate: 2.5e-06
09/07 07:14:28 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 07:14:35 PM: Update 21013: task mnli, batch 13 (20964): accuracy: 0.8388, mnli_loss: 0.4648
09/07 07:14:45 PM: Update 21055: task mnli, batch 55 (21006): accuracy: 0.8216, mnli_loss: 0.4637
09/07 07:14:55 PM: Update 21096: task mnli, batch 96 (21047): accuracy: 0.8267, mnli_loss: 0.4462
09/07 07:15:05 PM: Update 21140: task mnli, batch 140 (21091): accuracy: 0.8300, mnli_loss: 0.4358
09/07 07:15:15 PM: Update 21181: task mnli, batch 181 (21132): accuracy: 0.8245, mnli_loss: 0.4497
09/07 07:15:25 PM: Update 21221: task mnli, batch 221 (21172): accuracy: 0.8278, mnli_loss: 0.4444
09/07 07:15:35 PM: Update 21263: task mnli, batch 263 (21214): accuracy: 0.8249, mnli_loss: 0.4457
09/07 07:15:46 PM: Update 21305: task mnli, batch 305 (21256): accuracy: 0.8258, mnli_loss: 0.4422
09/07 07:15:56 PM: Update 21346: task mnli, batch 346 (21297): accuracy: 0.8286, mnli_loss: 0.4345
09/07 07:16:06 PM: Update 21386: task mnli, batch 386 (21337): accuracy: 0.8292, mnli_loss: 0.4353
09/07 07:16:16 PM: Update 21427: task mnli, batch 427 (21378): accuracy: 0.8288, mnli_loss: 0.4377
09/07 07:16:26 PM: Update 21460: task mnli, batch 460 (21411): accuracy: 0.8286, mnli_loss: 0.4383
09/07 07:16:32 PM: Update 21485: task wnli, batch 1 (50): accuracy: 0.5000, wnli_loss: 0.7346
09/07 07:16:36 PM: Update 21502: task mnli, batch 501 (21452): accuracy: 0.8307, mnli_loss: 0.4361
09/07 07:16:46 PM: Update 21545: task mnli, batch 544 (21495): accuracy: 0.8307, mnli_loss: 0.4357
09/07 07:16:56 PM: Update 21587: task mnli, batch 586 (21537): accuracy: 0.8324, mnli_loss: 0.4316
09/07 07:17:06 PM: Update 21627: task mnli, batch 626 (21577): accuracy: 0.8314, mnli_loss: 0.4335
09/07 07:17:16 PM: Update 21667: task mnli, batch 666 (21617): accuracy: 0.8313, mnli_loss: 0.4360
09/07 07:17:26 PM: Update 21707: task mnli, batch 706 (21657): accuracy: 0.8321, mnli_loss: 0.4346
09/07 07:17:37 PM: Update 21748: task mnli, batch 747 (21698): accuracy: 0.8329, mnli_loss: 0.4323
09/07 07:17:47 PM: Update 21790: task mnli, batch 789 (21740): accuracy: 0.8330, mnli_loss: 0.4313
09/07 07:17:57 PM: Update 21832: task mnli, batch 831 (21782): accuracy: 0.8334, mnli_loss: 0.4306
09/07 07:18:07 PM: Update 21861: task mnli, batch 860 (21811): accuracy: 0.8327, mnli_loss: 0.4311
09/07 07:18:17 PM: Update 21903: task mnli, batch 902 (21853): accuracy: 0.8332, mnli_loss: 0.4304
09/07 07:18:19 PM: Update 21910: task wnli, batch 2 (51): accuracy: 0.5208, wnli_loss: 0.7164
09/07 07:18:27 PM: Update 21944: task mnli, batch 942 (21893): accuracy: 0.8338, mnli_loss: 0.4302
09/07 07:18:34 PM: Update 21973: task wnli, batch 3 (52): accuracy: 0.5000, wnli_loss: 0.7097
09/07 07:18:37 PM: Update 21988: task mnli, batch 985 (21936): accuracy: 0.8344, mnli_loss: 0.4292
09/07 07:18:40 PM: ***** Step 22000 / Validation 22 *****
09/07 07:18:40 PM: mnli: trained on 997 batches, 0.061 epochs
09/07 07:18:40 PM: wnli: trained on 3 batches, 0.111 epochs
09/07 07:18:40 PM: Validating...
09/07 07:18:47 PM: Evaluate: task mnli, batch 107 (209): accuracy: 0.8107, mnli_loss: 0.4754
09/07 07:18:54 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.5417, wnli_loss: 0.6813
09/07 07:18:54 PM: Best result seen so far for mnli.
09/07 07:18:54 PM: Best result seen so far for micro.
09/07 07:18:54 PM: Updating LR scheduler:
09/07 07:18:54 PM: 	Best result seen so far for macro_avg: 0.705
09/07 07:18:54 PM: 	# validation passes without improvement: 2
09/07 07:18:54 PM: mnli_loss: training: 0.428459 validation: 0.487211
09/07 07:18:54 PM: wnli_loss: training: 0.709715 validation: 0.696691
09/07 07:18:54 PM: macro_avg: validation: 0.666463
09/07 07:18:54 PM: micro_avg: validation: 0.807730
09/07 07:18:54 PM: mnli_accuracy: training: 0.834714 validation: 0.811800
09/07 07:18:54 PM: wnli_accuracy: training: 0.500000 validation: 0.521127
09/07 07:18:54 PM: Global learning rate: 2.5e-06
09/07 07:18:54 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 07:18:57 PM: Update 22009: task mnli, batch 9 (21957): accuracy: 0.8194, mnli_loss: 0.4285
09/07 07:19:08 PM: Update 22051: task mnli, batch 50 (21998): accuracy: 0.8392, mnli_loss: 0.4017
09/07 07:19:18 PM: Update 22091: task mnli, batch 90 (22038): accuracy: 0.8412, mnli_loss: 0.4064
09/07 07:19:28 PM: Update 22133: task mnli, batch 132 (22080): accuracy: 0.8359, mnli_loss: 0.4104
09/07 07:19:38 PM: Update 22175: task mnli, batch 174 (22122): accuracy: 0.8405, mnli_loss: 0.4061
09/07 07:19:48 PM: Update 22215: task mnli, batch 214 (22162): accuracy: 0.8403, mnli_loss: 0.4086
09/07 07:19:58 PM: Update 22257: task mnli, batch 256 (22204): accuracy: 0.8407, mnli_loss: 0.4114
09/07 07:20:04 PM: Update 22272: task wnli, batch 2 (54): accuracy: 0.6250, wnli_loss: 0.6788
09/07 07:20:08 PM: Update 22287: task mnli, batch 285 (22233): accuracy: 0.8422, mnli_loss: 0.4100
09/07 07:20:19 PM: Update 22329: task mnli, batch 327 (22275): accuracy: 0.8426, mnli_loss: 0.4111
09/07 07:20:29 PM: Update 22370: task mnli, batch 368 (22316): accuracy: 0.8429, mnli_loss: 0.4082
09/07 07:20:39 PM: Update 22409: task mnli, batch 407 (22355): accuracy: 0.8423, mnli_loss: 0.4095
09/07 07:20:49 PM: Update 22450: task mnli, batch 448 (22396): accuracy: 0.8428, mnli_loss: 0.4088
09/07 07:20:59 PM: Update 22493: task mnli, batch 491 (22439): accuracy: 0.8409, mnli_loss: 0.4148
09/07 07:21:09 PM: Update 22536: task mnli, batch 534 (22482): accuracy: 0.8424, mnli_loss: 0.4116
09/07 07:21:19 PM: Update 22580: task mnli, batch 578 (22526): accuracy: 0.8440, mnli_loss: 0.4089
09/07 07:21:29 PM: Update 22620: task mnli, batch 618 (22566): accuracy: 0.8433, mnli_loss: 0.4106
09/07 07:21:36 PM: Update 22649: task wnli, batch 3 (55): accuracy: 0.6102, wnli_loss: 0.7086
09/07 07:21:39 PM: Update 22660: task mnli, batch 657 (22605): accuracy: 0.8431, mnli_loss: 0.4095
09/07 07:21:49 PM: Update 22694: task mnli, batch 691 (22639): accuracy: 0.8435, mnli_loss: 0.4087
09/07 07:21:59 PM: Update 22734: task mnli, batch 731 (22679): accuracy: 0.8431, mnli_loss: 0.4096
09/07 07:22:09 PM: Update 22775: task mnli, batch 772 (22720): accuracy: 0.8438, mnli_loss: 0.4084
09/07 07:22:19 PM: Update 22816: task mnli, batch 813 (22761): accuracy: 0.8441, mnli_loss: 0.4087
09/07 07:22:29 PM: Update 22859: task mnli, batch 856 (22804): accuracy: 0.8438, mnli_loss: 0.4107
09/07 07:22:40 PM: Update 22900: task mnli, batch 897 (22845): accuracy: 0.8439, mnli_loss: 0.4110
09/07 07:22:50 PM: Update 22943: task mnli, batch 940 (22888): accuracy: 0.8443, mnli_loss: 0.4095
09/07 07:23:00 PM: Update 22983: task mnli, batch 980 (22928): accuracy: 0.8445, mnli_loss: 0.4088
09/07 07:23:04 PM: ***** Step 23000 / Validation 23 *****
09/07 07:23:04 PM: mnli: trained on 997 batches, 0.061 epochs
09/07 07:23:04 PM: wnli: trained on 3 batches, 0.111 epochs
09/07 07:23:04 PM: Validating...
09/07 07:23:10 PM: Evaluate: task mnli, batch 87 (209): accuracy: 0.8209, mnli_loss: 0.4744
09/07 07:23:18 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.5417, wnli_loss: 0.6839
09/07 07:23:18 PM: Best result seen so far for mnli.
09/07 07:23:18 PM: Best result seen so far for micro.
09/07 07:23:18 PM: Updating LR scheduler:
09/07 07:23:18 PM: 	Best result seen so far for macro_avg: 0.705
09/07 07:23:18 PM: 	# validation passes without improvement: 3
09/07 07:23:18 PM: mnli_loss: training: 0.409324 validation: 0.499715
09/07 07:23:18 PM: wnli_loss: training: 0.708611 validation: 0.698104
09/07 07:23:18 PM: macro_avg: validation: 0.659621
09/07 07:23:18 PM: micro_avg: validation: 0.807927
09/07 07:23:18 PM: mnli_accuracy: training: 0.844471 validation: 0.812200
09/07 07:23:18 PM: wnli_accuracy: training: 0.610169 validation: 0.507042
09/07 07:23:18 PM: Global learning rate: 2.5e-06
09/07 07:23:18 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 07:23:20 PM: Update 23004: task mnli, batch 4 (22949): accuracy: 0.8229, mnli_loss: 0.4205
09/07 07:23:30 PM: Update 23044: task mnli, batch 44 (22989): accuracy: 0.8277, mnli_loss: 0.4343
09/07 07:23:40 PM: Update 23087: task mnli, batch 87 (23032): accuracy: 0.8348, mnli_loss: 0.4284
09/07 07:23:51 PM: Update 23120: task mnli, batch 120 (23065): accuracy: 0.8294, mnli_loss: 0.4396
09/07 07:24:01 PM: Update 23162: task mnli, batch 162 (23107): accuracy: 0.8253, mnli_loss: 0.4391
09/07 07:24:11 PM: Update 23202: task mnli, batch 202 (23147): accuracy: 0.8271, mnli_loss: 0.4326
09/07 07:24:21 PM: Update 23244: task mnli, batch 244 (23189): accuracy: 0.8287, mnli_loss: 0.4297
09/07 07:24:31 PM: Update 23285: task mnli, batch 285 (23230): accuracy: 0.8311, mnli_loss: 0.4214
09/07 07:24:41 PM: Update 23327: task mnli, batch 327 (23272): accuracy: 0.8320, mnli_loss: 0.4211
09/07 07:24:51 PM: Update 23368: task mnli, batch 368 (23313): accuracy: 0.8335, mnli_loss: 0.4201
09/07 07:25:02 PM: Update 23410: task mnli, batch 410 (23355): accuracy: 0.8337, mnli_loss: 0.4182
09/07 07:25:12 PM: Update 23451: task mnli, batch 451 (23396): accuracy: 0.8363, mnli_loss: 0.4144
09/07 07:25:22 PM: Update 23492: task mnli, batch 492 (23437): accuracy: 0.8387, mnli_loss: 0.4104
09/07 07:25:32 PM: Update 23525: task mnli, batch 525 (23470): accuracy: 0.8397, mnli_loss: 0.4085
09/07 07:25:42 PM: Update 23565: task mnli, batch 565 (23510): accuracy: 0.8397, mnli_loss: 0.4083
09/07 07:25:52 PM: Update 23606: task mnli, batch 606 (23551): accuracy: 0.8394, mnli_loss: 0.4095
09/07 07:25:59 PM: Update 23637: task wnli, batch 1 (56): accuracy: 0.6250, wnli_loss: 0.6744
09/07 07:26:02 PM: Update 23648: task mnli, batch 647 (23592): accuracy: 0.8392, mnli_loss: 0.4085
09/07 07:26:12 PM: Update 23691: task mnli, batch 690 (23635): accuracy: 0.8401, mnli_loss: 0.4069
09/07 07:26:22 PM: Update 23733: task mnli, batch 732 (23677): accuracy: 0.8410, mnli_loss: 0.4055
09/07 07:26:32 PM: Update 23776: task mnli, batch 775 (23720): accuracy: 0.8415, mnli_loss: 0.4065
09/07 07:26:43 PM: Update 23818: task mnli, batch 817 (23762): accuracy: 0.8416, mnli_loss: 0.4057
09/07 07:26:53 PM: Update 23858: task mnli, batch 857 (23802): accuracy: 0.8413, mnli_loss: 0.4054
09/07 07:27:03 PM: Update 23899: task mnli, batch 898 (23843): accuracy: 0.8407, mnli_loss: 0.4065
09/07 07:27:16 PM: Update 23939: task mnli, batch 938 (23883): accuracy: 0.8405, mnli_loss: 0.4058
09/07 07:27:26 PM: Update 23979: task mnli, batch 978 (23923): accuracy: 0.8406, mnli_loss: 0.4062
09/07 07:27:31 PM: ***** Step 24000 / Validation 24 *****
09/07 07:27:31 PM: mnli: trained on 999 batches, 0.061 epochs
09/07 07:27:31 PM: wnli: trained on 1 batches, 0.037 epochs
09/07 07:27:31 PM: Validating...
09/07 07:27:36 PM: Evaluate: task mnli, batch 78 (209): accuracy: 0.8162, mnli_loss: 0.4896
09/07 07:27:45 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.5417, wnli_loss: 0.6858
09/07 07:27:45 PM: Best result seen so far for mnli.
09/07 07:27:45 PM: Updating LR scheduler:
09/07 07:27:45 PM: 	Best result seen so far for macro_avg: 0.705
09/07 07:27:45 PM: 	# validation passes without improvement: 4
09/07 07:27:45 PM: mnli_loss: training: 0.407124 validation: 0.508557
09/07 07:27:45 PM: wnli_loss: training: 0.674436 validation: 0.699258
09/07 07:27:45 PM: macro_avg: validation: 0.652679
09/07 07:27:45 PM: micro_avg: validation: 0.807927
09/07 07:27:45 PM: mnli_accuracy: training: 0.840264 validation: 0.812400
09/07 07:27:45 PM: wnli_accuracy: training: 0.625000 validation: 0.492958
09/07 07:27:45 PM: Global learning rate: 2.5e-06
09/07 07:27:45 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 07:27:46 PM: Update 24001: task mnli, batch 1 (23945): accuracy: 0.8750, mnli_loss: 0.3770
09/07 07:27:56 PM: Update 24041: task mnli, batch 41 (23985): accuracy: 0.8394, mnli_loss: 0.3902
09/07 07:28:06 PM: Update 24083: task mnli, batch 83 (24027): accuracy: 0.8429, mnli_loss: 0.3991
09/07 07:28:11 PM: Update 24100: task wnli, batch 1 (57): accuracy: 0.5833, wnli_loss: 0.6703
09/07 07:28:16 PM: Update 24123: task mnli, batch 122 (24066): accuracy: 0.8432, mnli_loss: 0.4081
09/07 07:28:27 PM: Update 24165: task mnli, batch 164 (24108): accuracy: 0.8450, mnli_loss: 0.4089
09/07 07:28:37 PM: Update 24206: task mnli, batch 205 (24149): accuracy: 0.8451, mnli_loss: 0.4062
09/07 07:28:47 PM: Update 24248: task mnli, batch 247 (24191): accuracy: 0.8455, mnli_loss: 0.4014
09/07 07:28:57 PM: Update 24288: task mnli, batch 287 (24231): accuracy: 0.8479, mnli_loss: 0.3950
09/07 07:29:07 PM: Update 24331: task mnli, batch 330 (24274): accuracy: 0.8472, mnli_loss: 0.3939
09/07 07:29:17 PM: Update 24362: task mnli, batch 361 (24305): accuracy: 0.8481, mnli_loss: 0.3907
09/07 07:29:28 PM: Update 24402: task mnli, batch 401 (24345): accuracy: 0.8464, mnli_loss: 0.3937
09/07 07:29:38 PM: Update 24443: task mnli, batch 442 (24386): accuracy: 0.8444, mnli_loss: 0.3984
09/07 07:29:48 PM: Update 24484: task mnli, batch 483 (24427): accuracy: 0.8425, mnli_loss: 0.4038
09/07 07:29:58 PM: Update 24527: task mnli, batch 526 (24470): accuracy: 0.8427, mnli_loss: 0.4030
09/07 07:30:08 PM: Update 24568: task mnli, batch 567 (24511): accuracy: 0.8426, mnli_loss: 0.4025
09/07 07:30:18 PM: Update 24610: task mnli, batch 609 (24553): accuracy: 0.8434, mnli_loss: 0.4028
09/07 07:30:28 PM: Update 24651: task mnli, batch 650 (24594): accuracy: 0.8434, mnli_loss: 0.4032
09/07 07:30:38 PM: Update 24693: task mnli, batch 692 (24636): accuracy: 0.8434, mnli_loss: 0.4026
09/07 07:30:49 PM: Update 24736: task mnli, batch 735 (24679): accuracy: 0.8439, mnli_loss: 0.4019
09/07 07:31:01 PM: Update 24774: task mnli, batch 773 (24717): accuracy: 0.8434, mnli_loss: 0.4031
09/07 07:31:11 PM: Update 24815: task mnli, batch 814 (24758): accuracy: 0.8434, mnli_loss: 0.4023
09/07 07:31:22 PM: Update 24855: task mnli, batch 854 (24798): accuracy: 0.8431, mnli_loss: 0.4022
09/07 07:31:32 PM: Update 24896: task mnli, batch 895 (24839): accuracy: 0.8429, mnli_loss: 0.4031
09/07 07:31:42 PM: Update 24937: task mnli, batch 936 (24880): accuracy: 0.8423, mnli_loss: 0.4032
09/07 07:31:52 PM: Update 24978: task mnli, batch 977 (24921): accuracy: 0.8426, mnli_loss: 0.4025
09/07 07:31:58 PM: ***** Step 25000 / Validation 25 *****
09/07 07:31:58 PM: mnli: trained on 999 batches, 0.061 epochs
09/07 07:31:58 PM: wnli: trained on 1 batches, 0.037 epochs
09/07 07:31:58 PM: Validating...
09/07 07:32:02 PM: Evaluate: task mnli, batch 64 (209): accuracy: 0.8197, mnli_loss: 0.4856
09/07 07:32:12 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.5833, wnli_loss: 0.6763
09/07 07:32:12 PM: Updating LR scheduler:
09/07 07:32:12 PM: 	Best result seen so far for macro_avg: 0.705
09/07 07:32:12 PM: 	# validation passes without improvement: 0
09/07 07:32:12 PM: mnli_loss: training: 0.402980 validation: 0.510313
09/07 07:32:12 PM: wnli_loss: training: 0.670306 validation: 0.700347
09/07 07:32:12 PM: macro_avg: validation: 0.673406
09/07 07:32:12 PM: micro_avg: validation: 0.807730
09/07 07:32:12 PM: mnli_accuracy: training: 0.842362 validation: 0.811600
09/07 07:32:12 PM: wnli_accuracy: training: 0.583333 validation: 0.535211
09/07 07:32:12 PM: Global learning rate: 1.25e-06
09/07 07:32:12 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 07:32:14 PM: Update 25001: task mnli, batch 1 (24944): accuracy: 0.6667, mnli_loss: 0.6991
09/07 07:32:24 PM: Update 25042: task mnli, batch 42 (24985): accuracy: 0.8720, mnli_loss: 0.3400
09/07 07:32:34 PM: Update 25084: task mnli, batch 84 (25027): accuracy: 0.8676, mnli_loss: 0.3456
09/07 07:32:44 PM: Update 25127: task mnli, batch 127 (25070): accuracy: 0.8602, mnli_loss: 0.3581
09/07 07:32:55 PM: Update 25168: task mnli, batch 168 (25111): accuracy: 0.8571, mnli_loss: 0.3738
09/07 07:33:05 PM: Update 25200: task mnli, batch 200 (25143): accuracy: 0.8573, mnli_loss: 0.3751
09/07 07:33:15 PM: Update 25243: task mnli, batch 243 (25186): accuracy: 0.8587, mnli_loss: 0.3738
09/07 07:33:25 PM: Update 25285: task mnli, batch 285 (25228): accuracy: 0.8542, mnli_loss: 0.3786
09/07 07:33:35 PM: Update 25324: task mnli, batch 324 (25267): accuracy: 0.8540, mnli_loss: 0.3811
09/07 07:33:45 PM: Update 25364: task mnli, batch 364 (25307): accuracy: 0.8520, mnli_loss: 0.3843
09/07 07:33:55 PM: Update 25405: task mnli, batch 405 (25348): accuracy: 0.8531, mnli_loss: 0.3847
09/07 07:34:06 PM: Update 25445: task mnli, batch 445 (25388): accuracy: 0.8522, mnli_loss: 0.3859
09/07 07:34:16 PM: Update 25487: task mnli, batch 487 (25430): accuracy: 0.8516, mnli_loss: 0.3861
09/07 07:34:26 PM: Update 25529: task mnli, batch 529 (25472): accuracy: 0.8507, mnli_loss: 0.3878
09/07 07:34:36 PM: Update 25570: task mnli, batch 570 (25513): accuracy: 0.8484, mnli_loss: 0.3938
09/07 07:34:48 PM: Update 25608: task mnli, batch 608 (25551): accuracy: 0.8469, mnli_loss: 0.3959
09/07 07:34:58 PM: Update 25648: task mnli, batch 648 (25591): accuracy: 0.8472, mnli_loss: 0.3958
09/07 07:35:09 PM: Update 25690: task mnli, batch 690 (25633): accuracy: 0.8473, mnli_loss: 0.3948
09/07 07:35:19 PM: Update 25729: task mnli, batch 729 (25672): accuracy: 0.8476, mnli_loss: 0.3962
09/07 07:35:29 PM: Update 25768: task mnli, batch 768 (25711): accuracy: 0.8476, mnli_loss: 0.3961
09/07 07:35:39 PM: Update 25808: task mnli, batch 808 (25751): accuracy: 0.8477, mnli_loss: 0.3965
09/07 07:35:49 PM: Update 25848: task mnli, batch 848 (25791): accuracy: 0.8475, mnli_loss: 0.3990
09/07 07:35:59 PM: Update 25890: task mnli, batch 890 (25833): accuracy: 0.8477, mnli_loss: 0.3980
09/07 07:36:09 PM: Update 25932: task mnli, batch 932 (25875): accuracy: 0.8482, mnli_loss: 0.3965
09/07 07:36:19 PM: Update 25974: task mnli, batch 974 (25917): accuracy: 0.8480, mnli_loss: 0.3979
09/07 07:36:26 PM: ***** Step 26000 / Validation 26 *****
09/07 07:36:26 PM: mnli: trained on 1000 batches, 0.061 epochs
09/07 07:36:26 PM: wnli: trained on 0 batches, 0.000 epochs
09/07 07:36:26 PM: Validating...
09/07 07:36:29 PM: Evaluate: task mnli, batch 55 (209): accuracy: 0.8144, mnli_loss: 0.4751
09/07 07:36:40 PM: Evaluate: task mnli, batch 205 (209): accuracy: 0.8118, mnli_loss: 0.5007
09/07 07:36:40 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.5833, wnli_loss: 0.6769
09/07 07:36:40 PM: Updating LR scheduler:
09/07 07:36:40 PM: 	Best result seen so far for macro_avg: 0.705
09/07 07:36:40 PM: 	# validation passes without improvement: 1
09/07 07:36:40 PM: mnli_loss: training: 0.398711 validation: 0.504830
09/07 07:36:40 PM: wnli_loss: training: 0.000000 validation: 0.699732
09/07 07:36:40 PM: macro_avg: validation: 0.672806
09/07 07:36:40 PM: micro_avg: validation: 0.806547
09/07 07:36:40 PM: mnli_accuracy: training: 0.848024 validation: 0.810400
09/07 07:36:40 PM: wnli_accuracy: validation: 0.535211
09/07 07:36:40 PM: Global learning rate: 1.25e-06
09/07 07:36:40 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 07:36:50 PM: Update 26026: task mnli, batch 26 (25969): accuracy: 0.8425, mnli_loss: 0.4004
09/07 07:37:00 PM: Update 26067: task mnli, batch 67 (26010): accuracy: 0.8444, mnli_loss: 0.3952
09/07 07:37:10 PM: Update 26108: task mnli, batch 108 (26051): accuracy: 0.8460, mnli_loss: 0.3952
09/07 07:37:20 PM: Update 26151: task mnli, batch 151 (26094): accuracy: 0.8504, mnli_loss: 0.3850
09/07 07:37:30 PM: Update 26194: task mnli, batch 194 (26137): accuracy: 0.8518, mnli_loss: 0.3905
09/07 07:37:40 PM: Update 26237: task mnli, batch 237 (26180): accuracy: 0.8519, mnli_loss: 0.3898
09/07 07:37:50 PM: Update 26279: task mnli, batch 279 (26222): accuracy: 0.8503, mnli_loss: 0.3907
09/07 07:38:01 PM: Update 26320: task mnli, batch 320 (26263): accuracy: 0.8487, mnli_loss: 0.3951
09/07 07:38:11 PM: Update 26361: task mnli, batch 361 (26304): accuracy: 0.8495, mnli_loss: 0.3943
09/07 07:38:21 PM: Update 26401: task mnli, batch 401 (26344): accuracy: 0.8510, mnli_loss: 0.3909
09/07 07:38:33 PM: Update 26442: task mnli, batch 442 (26385): accuracy: 0.8490, mnli_loss: 0.3933
09/07 07:38:43 PM: Update 26482: task mnli, batch 482 (26425): accuracy: 0.8495, mnli_loss: 0.3933
09/07 07:38:53 PM: Update 26524: task mnli, batch 524 (26467): accuracy: 0.8498, mnli_loss: 0.3930
09/07 07:39:03 PM: Update 26566: task mnli, batch 566 (26509): accuracy: 0.8499, mnli_loss: 0.3926
09/07 07:39:14 PM: Update 26609: task mnli, batch 609 (26552): accuracy: 0.8527, mnli_loss: 0.3876
09/07 07:39:24 PM: Update 26650: task mnli, batch 650 (26593): accuracy: 0.8540, mnli_loss: 0.3839
09/07 07:39:34 PM: Update 26693: task mnli, batch 693 (26636): accuracy: 0.8539, mnli_loss: 0.3830
09/07 07:39:44 PM: Update 26733: task mnli, batch 733 (26676): accuracy: 0.8528, mnli_loss: 0.3847
09/07 07:39:54 PM: Update 26773: task mnli, batch 773 (26716): accuracy: 0.8534, mnli_loss: 0.3831
09/07 07:40:04 PM: Update 26814: task mnli, batch 814 (26757): accuracy: 0.8533, mnli_loss: 0.3835
09/07 07:40:14 PM: Update 26856: task mnli, batch 856 (26799): accuracy: 0.8541, mnli_loss: 0.3816
09/07 07:40:15 PM: Update 26859: task wnli, batch 1 (58): accuracy: 0.5000, wnli_loss: 0.6673
09/07 07:40:24 PM: Update 26888: task mnli, batch 887 (26830): accuracy: 0.8539, mnli_loss: 0.3810
09/07 07:40:34 PM: Update 26928: task mnli, batch 927 (26870): accuracy: 0.8538, mnli_loss: 0.3810
09/07 07:40:44 PM: Update 26970: task mnli, batch 969 (26912): accuracy: 0.8535, mnli_loss: 0.3822
09/07 07:40:51 PM: ***** Step 27000 / Validation 27 *****
09/07 07:40:51 PM: mnli: trained on 999 batches, 0.061 epochs
09/07 07:40:51 PM: wnli: trained on 1 batches, 0.037 epochs
09/07 07:40:51 PM: Validating...
09/07 07:40:54 PM: Evaluate: task mnli, batch 43 (209): accuracy: 0.8227, mnli_loss: 0.4896
09/07 07:41:04 PM: Evaluate: task mnli, batch 178 (209): accuracy: 0.8167, mnli_loss: 0.4971
09/07 07:41:07 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.5833, wnli_loss: 0.6746
09/07 07:41:07 PM: Best result seen so far for mnli.
09/07 07:41:07 PM: Best result seen so far for micro.
09/07 07:41:07 PM: Updating LR scheduler:
09/07 07:41:07 PM: 	Best result seen so far for macro_avg: 0.705
09/07 07:41:07 PM: 	# validation passes without improvement: 2
09/07 07:41:07 PM: mnli_loss: training: 0.381811 validation: 0.508185
09/07 07:41:07 PM: wnli_loss: training: 0.667273 validation: 0.699917
09/07 07:41:07 PM: macro_avg: validation: 0.674706
09/07 07:41:07 PM: micro_avg: validation: 0.810294
09/07 07:41:07 PM: mnli_accuracy: training: 0.854041 validation: 0.814200
09/07 07:41:07 PM: wnli_accuracy: training: 0.500000 validation: 0.535211
09/07 07:41:07 PM: Global learning rate: 1.25e-06
09/07 07:41:07 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 07:41:14 PM: Update 27028: task mnli, batch 28 (26970): accuracy: 0.8676, mnli_loss: 0.3405
09/07 07:41:25 PM: Update 27069: task mnli, batch 69 (27011): accuracy: 0.8720, mnli_loss: 0.3530
09/07 07:41:35 PM: Update 27111: task mnli, batch 111 (27053): accuracy: 0.8727, mnli_loss: 0.3489
09/07 07:41:45 PM: Update 27153: task mnli, batch 153 (27095): accuracy: 0.8687, mnli_loss: 0.3538
09/07 07:41:55 PM: Update 27195: task mnli, batch 195 (27137): accuracy: 0.8632, mnli_loss: 0.3591
09/07 07:42:05 PM: Update 27237: task mnli, batch 237 (27179): accuracy: 0.8620, mnli_loss: 0.3576
09/07 07:42:17 PM: Update 27277: task mnli, batch 277 (27219): accuracy: 0.8599, mnli_loss: 0.3624
09/07 07:42:27 PM: Update 27318: task mnli, batch 318 (27260): accuracy: 0.8576, mnli_loss: 0.3688
09/07 07:42:37 PM: Update 27358: task mnli, batch 358 (27300): accuracy: 0.8538, mnli_loss: 0.3800
09/07 07:42:47 PM: Update 27400: task mnli, batch 400 (27342): accuracy: 0.8544, mnli_loss: 0.3836
09/07 07:42:58 PM: Update 27441: task mnli, batch 441 (27383): accuracy: 0.8524, mnli_loss: 0.3884
09/07 07:43:08 PM: Update 27482: task mnli, batch 482 (27424): accuracy: 0.8503, mnli_loss: 0.3906
09/07 07:43:18 PM: Update 27524: task mnli, batch 524 (27466): accuracy: 0.8514, mnli_loss: 0.3897
09/07 07:43:28 PM: Update 27566: task mnli, batch 566 (27508): accuracy: 0.8500, mnli_loss: 0.3923
09/07 07:43:38 PM: Update 27609: task mnli, batch 609 (27551): accuracy: 0.8502, mnli_loss: 0.3916
09/07 07:43:49 PM: Update 27651: task mnli, batch 651 (27593): accuracy: 0.8493, mnli_loss: 0.3932
09/07 07:43:59 PM: Update 27691: task mnli, batch 691 (27633): accuracy: 0.8474, mnli_loss: 0.3961
09/07 07:44:09 PM: Update 27724: task mnli, batch 724 (27666): accuracy: 0.8464, mnli_loss: 0.3981
09/07 07:44:16 PM: Update 27754: task wnli, batch 1 (59): accuracy: 0.5833, wnli_loss: 0.6633
09/07 07:44:19 PM: Update 27764: task mnli, batch 763 (27705): accuracy: 0.8445, mnli_loss: 0.4024
09/07 07:44:29 PM: Update 27805: task mnli, batch 804 (27746): accuracy: 0.8435, mnli_loss: 0.4046
09/07 07:44:39 PM: Update 27846: task mnli, batch 845 (27787): accuracy: 0.8437, mnli_loss: 0.4052
09/07 07:44:49 PM: Update 27889: task mnli, batch 888 (27830): accuracy: 0.8430, mnli_loss: 0.4065
09/07 07:44:59 PM: Update 27931: task mnli, batch 930 (27872): accuracy: 0.8418, mnli_loss: 0.4081
09/07 07:45:10 PM: Update 27975: task mnli, batch 974 (27916): accuracy: 0.8414, mnli_loss: 0.4090
09/07 07:45:16 PM: ***** Step 28000 / Validation 28 *****
09/07 07:45:16 PM: mnli: trained on 999 batches, 0.061 epochs
09/07 07:45:16 PM: wnli: trained on 1 batches, 0.037 epochs
09/07 07:45:16 PM: Validating...
09/07 07:45:20 PM: Evaluate: task mnli, batch 57 (209): accuracy: 0.8187, mnli_loss: 0.4762
09/07 07:45:30 PM: Evaluate: task mnli, batch 206 (209): accuracy: 0.8161, mnli_loss: 0.4949
09/07 07:45:30 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.5833, wnli_loss: 0.6776
09/07 07:45:30 PM: Best result seen so far for mnli.
09/07 07:45:30 PM: Best result seen so far for micro.
09/07 07:45:30 PM: Updating LR scheduler:
09/07 07:45:30 PM: 	Best result seen so far for macro_avg: 0.705
09/07 07:45:30 PM: 	# validation passes without improvement: 3
09/07 07:45:30 PM: mnli_loss: training: 0.410594 validation: 0.497951
09/07 07:45:30 PM: wnli_loss: training: 0.663295 validation: 0.700972
09/07 07:45:30 PM: macro_avg: validation: 0.675006
09/07 07:45:30 PM: micro_avg: validation: 0.810885
09/07 07:45:30 PM: mnli_accuracy: training: 0.840693 validation: 0.814800
09/07 07:45:30 PM: wnli_accuracy: training: 0.583333 validation: 0.535211
09/07 07:45:30 PM: Global learning rate: 1.25e-06
09/07 07:45:30 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 07:45:40 PM: Update 28036: task mnli, batch 36 (27977): accuracy: 0.8322, mnli_loss: 0.4450
09/07 07:45:50 PM: Update 28078: task mnli, batch 78 (28019): accuracy: 0.8312, mnli_loss: 0.4287
09/07 07:46:01 PM: Update 28112: task mnli, batch 112 (28053): accuracy: 0.8284, mnli_loss: 0.4386
09/07 07:46:11 PM: Update 28153: task mnli, batch 153 (28094): accuracy: 0.8240, mnli_loss: 0.4420
09/07 07:46:21 PM: Update 28194: task mnli, batch 194 (28135): accuracy: 0.8225, mnli_loss: 0.4472
09/07 07:46:31 PM: Update 28236: task mnli, batch 236 (28177): accuracy: 0.8241, mnli_loss: 0.4463
09/07 07:46:41 PM: Update 28278: task mnli, batch 278 (28219): accuracy: 0.8228, mnli_loss: 0.4477
09/07 07:46:51 PM: Update 28320: task mnli, batch 320 (28261): accuracy: 0.8242, mnli_loss: 0.4451
09/07 07:47:02 PM: Update 28363: task mnli, batch 363 (28304): accuracy: 0.8270, mnli_loss: 0.4408
09/07 07:47:12 PM: Update 28404: task mnli, batch 404 (28345): accuracy: 0.8288, mnli_loss: 0.4360
09/07 07:47:18 PM: Update 28426: task wnli, batch 1 (60): accuracy: 0.3333, wnli_loss: 0.7573
09/07 07:47:22 PM: Update 28443: task mnli, batch 442 (28383): accuracy: 0.8278, mnli_loss: 0.4375
09/07 07:47:32 PM: Update 28485: task mnli, batch 484 (28425): accuracy: 0.8287, mnli_loss: 0.4355
09/07 07:47:43 PM: Update 28526: task mnli, batch 525 (28466): accuracy: 0.8284, mnli_loss: 0.4372
09/07 07:47:51 PM: Update 28544: task wnli, batch 2 (61): accuracy: 0.5000, wnli_loss: 0.7068
09/07 07:47:53 PM: Update 28553: task mnli, batch 551 (28492): accuracy: 0.8284, mnli_loss: 0.4368
09/07 07:48:03 PM: Update 28596: task mnli, batch 594 (28535): accuracy: 0.8296, mnli_loss: 0.4329
09/07 07:48:13 PM: Update 28635: task mnli, batch 633 (28574): accuracy: 0.8295, mnli_loss: 0.4336
09/07 07:48:23 PM: Update 28676: task mnli, batch 674 (28615): accuracy: 0.8300, mnli_loss: 0.4342
09/07 07:48:33 PM: Update 28716: task mnli, batch 714 (28655): accuracy: 0.8297, mnli_loss: 0.4347
09/07 07:48:44 PM: Update 28759: task mnli, batch 757 (28698): accuracy: 0.8299, mnli_loss: 0.4339
09/07 07:48:54 PM: Update 28802: task mnli, batch 800 (28741): accuracy: 0.8292, mnli_loss: 0.4362
09/07 07:49:04 PM: Update 28843: task wnli, batch 3 (62): accuracy: 0.4722, wnli_loss: 0.7101
09/07 07:49:04 PM: Update 28844: task mnli, batch 841 (28782): accuracy: 0.8286, mnli_loss: 0.4364
09/07 07:49:14 PM: Update 28887: task mnli, batch 884 (28825): accuracy: 0.8287, mnli_loss: 0.4363
09/07 07:49:25 PM: Update 28928: task mnli, batch 925 (28866): accuracy: 0.8290, mnli_loss: 0.4355
09/07 07:49:35 PM: Update 28960: task mnli, batch 957 (28898): accuracy: 0.8291, mnli_loss: 0.4346
09/07 07:49:45 PM: ***** Step 29000 / Validation 29 *****
09/07 07:49:45 PM: mnli: trained on 997 batches, 0.061 epochs
09/07 07:49:45 PM: wnli: trained on 3 batches, 0.111 epochs
09/07 07:49:45 PM: Validating...
09/07 07:49:45 PM: Evaluate: task mnli, batch 1 (209): accuracy: 0.8333, mnli_loss: 0.3196
09/07 07:49:55 PM: Evaluate: task mnli, batch 147 (209): accuracy: 0.8115, mnli_loss: 0.4942
09/07 07:49:59 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.5417, wnli_loss: 0.6837
09/07 07:49:59 PM: Updating LR scheduler:
09/07 07:49:59 PM: 	Best result seen so far for macro_avg: 0.705
09/07 07:49:59 PM: 	# validation passes without improvement: 4
09/07 07:49:59 PM: mnli_loss: training: 0.434635 validation: 0.499381
09/07 07:49:59 PM: wnli_loss: training: 0.710144 validation: 0.700950
09/07 07:49:59 PM: macro_avg: validation: 0.665863
09/07 07:49:59 PM: micro_avg: validation: 0.806547
09/07 07:49:59 PM: mnli_accuracy: training: 0.829275 validation: 0.810600
09/07 07:49:59 PM: wnli_accuracy: training: 0.472222 validation: 0.521127
09/07 07:49:59 PM: Global learning rate: 1.25e-06
09/07 07:49:59 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 07:50:05 PM: Update 29021: task mnli, batch 21 (28959): accuracy: 0.8591, mnli_loss: 0.3722
09/07 07:50:15 PM: Update 29062: task mnli, batch 62 (29000): accuracy: 0.8387, mnli_loss: 0.4168
09/07 07:50:26 PM: Update 29102: task mnli, batch 102 (29040): accuracy: 0.8239, mnli_loss: 0.4446
09/07 07:50:36 PM: Update 29145: task mnli, batch 145 (29083): accuracy: 0.8270, mnli_loss: 0.4406
09/07 07:50:46 PM: Update 29188: task mnli, batch 188 (29126): accuracy: 0.8291, mnli_loss: 0.4288
09/07 07:50:56 PM: Update 29230: task mnli, batch 230 (29168): accuracy: 0.8270, mnli_loss: 0.4376
09/07 07:51:06 PM: Update 29272: task mnli, batch 272 (29210): accuracy: 0.8294, mnli_loss: 0.4356
09/07 07:51:16 PM: Update 29312: task mnli, batch 312 (29250): accuracy: 0.8320, mnli_loss: 0.4317
09/07 07:51:26 PM: Update 29353: task mnli, batch 353 (29291): accuracy: 0.8347, mnli_loss: 0.4243
09/07 07:51:36 PM: Update 29385: task mnli, batch 385 (29323): accuracy: 0.8326, mnli_loss: 0.4274
09/07 07:51:47 PM: Update 29426: task mnli, batch 426 (29364): accuracy: 0.8312, mnli_loss: 0.4322
09/07 07:51:57 PM: Update 29467: task mnli, batch 467 (29405): accuracy: 0.8296, mnli_loss: 0.4352
09/07 07:52:07 PM: Update 29509: task mnli, batch 509 (29447): accuracy: 0.8307, mnli_loss: 0.4336
09/07 07:52:17 PM: Update 29552: task mnli, batch 552 (29490): accuracy: 0.8298, mnli_loss: 0.4344
09/07 07:52:27 PM: Update 29595: task mnli, batch 595 (29533): accuracy: 0.8290, mnli_loss: 0.4354
09/07 07:52:37 PM: Update 29636: task mnli, batch 636 (29574): accuracy: 0.8287, mnli_loss: 0.4368
09/07 07:52:47 PM: Update 29676: task mnli, batch 676 (29614): accuracy: 0.8279, mnli_loss: 0.4387
09/07 07:52:58 PM: Update 29716: task mnli, batch 716 (29654): accuracy: 0.8278, mnli_loss: 0.4390
09/07 07:53:08 PM: Update 29758: task mnli, batch 758 (29696): accuracy: 0.8273, mnli_loss: 0.4405
09/07 07:53:18 PM: Update 29790: task mnli, batch 790 (29728): accuracy: 0.8276, mnli_loss: 0.4403
09/07 07:53:28 PM: Update 29832: task mnli, batch 832 (29770): accuracy: 0.8282, mnli_loss: 0.4392
09/07 07:53:38 PM: Update 29873: task mnli, batch 873 (29811): accuracy: 0.8295, mnli_loss: 0.4369
09/07 07:53:43 PM: Update 29894: task wnli, batch 1 (63): accuracy: 0.2917, wnli_loss: 0.7643
09/07 07:53:48 PM: Update 29914: task mnli, batch 913 (29851): accuracy: 0.8288, mnli_loss: 0.4384
09/07 07:53:58 PM: Update 29955: task mnli, batch 954 (29892): accuracy: 0.8289, mnli_loss: 0.4381
09/07 07:54:08 PM: Update 29997: task mnli, batch 996 (29934): accuracy: 0.8293, mnli_loss: 0.4366
09/07 07:54:09 PM: ***** Step 30000 / Validation 30 *****
09/07 07:54:09 PM: mnli: trained on 999 batches, 0.061 epochs
09/07 07:54:09 PM: wnli: trained on 1 batches, 0.037 epochs
09/07 07:54:09 PM: Validating...
09/07 07:54:18 PM: Evaluate: task mnli, batch 140 (209): accuracy: 0.8131, mnli_loss: 0.4918
09/07 07:54:23 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.5417, wnli_loss: 0.6886
09/07 07:54:23 PM: Updating LR scheduler:
09/07 07:54:23 PM: 	Best result seen so far for macro_avg: 0.705
09/07 07:54:23 PM: 	# validation passes without improvement: 0
09/07 07:54:23 PM: mnli_loss: training: 0.437198 validation: 0.500140
09/07 07:54:23 PM: wnli_loss: training: 0.764336 validation: 0.700089
09/07 07:54:23 PM: macro_avg: validation: 0.659121
09/07 07:54:23 PM: micro_avg: validation: 0.806941
09/07 07:54:23 PM: mnli_accuracy: training: 0.829090 validation: 0.811200
09/07 07:54:23 PM: wnli_accuracy: training: 0.291667 validation: 0.507042
09/07 07:54:23 PM: Global learning rate: 6.25e-07
09/07 07:54:23 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 07:54:29 PM: Update 30018: task mnli, batch 18 (29955): accuracy: 0.8403, mnli_loss: 0.4149
09/07 07:54:39 PM: Update 30060: task mnli, batch 60 (29997): accuracy: 0.8354, mnli_loss: 0.4163
09/07 07:54:49 PM: Update 30102: task mnli, batch 102 (30039): accuracy: 0.8366, mnli_loss: 0.4175
09/07 07:54:59 PM: Update 30142: task mnli, batch 142 (30079): accuracy: 0.8295, mnli_loss: 0.4268
09/07 07:55:09 PM: Update 30184: task mnli, batch 184 (30121): accuracy: 0.8308, mnli_loss: 0.4264
09/07 07:55:19 PM: Update 30213: task mnli, batch 213 (30150): accuracy: 0.8305, mnli_loss: 0.4325
09/07 07:55:29 PM: Update 30253: task mnli, batch 253 (30190): accuracy: 0.8292, mnli_loss: 0.4368
09/07 07:55:39 PM: Update 30293: task mnli, batch 293 (30230): accuracy: 0.8280, mnli_loss: 0.4403
09/07 07:55:49 PM: Update 30333: task mnli, batch 333 (30270): accuracy: 0.8284, mnli_loss: 0.4404
09/07 07:55:59 PM: Update 30373: task mnli, batch 373 (30310): accuracy: 0.8269, mnli_loss: 0.4422
09/07 07:56:09 PM: Update 30415: task mnli, batch 415 (30352): accuracy: 0.8262, mnli_loss: 0.4427
09/07 07:56:20 PM: Update 30456: task mnli, batch 456 (30393): accuracy: 0.8254, mnli_loss: 0.4485
09/07 07:56:30 PM: Update 30497: task mnli, batch 497 (30434): accuracy: 0.8256, mnli_loss: 0.4462
09/07 07:56:40 PM: Update 30539: task mnli, batch 539 (30476): accuracy: 0.8263, mnli_loss: 0.4441
09/07 07:56:50 PM: Update 30580: task mnli, batch 580 (30517): accuracy: 0.8256, mnli_loss: 0.4450
09/07 07:57:01 PM: Update 30618: task mnli, batch 618 (30555): accuracy: 0.8259, mnli_loss: 0.4451
09/07 07:57:11 PM: Update 30660: task mnli, batch 660 (30597): accuracy: 0.8257, mnli_loss: 0.4450
09/07 07:57:21 PM: Update 30700: task mnli, batch 700 (30637): accuracy: 0.8261, mnli_loss: 0.4437
09/07 07:57:31 PM: Update 30742: task mnli, batch 742 (30679): accuracy: 0.8273, mnli_loss: 0.4421
09/07 07:57:41 PM: Update 30784: task mnli, batch 784 (30721): accuracy: 0.8280, mnli_loss: 0.4409
09/07 07:57:51 PM: Update 30826: task mnli, batch 826 (30763): accuracy: 0.8292, mnli_loss: 0.4382
09/07 07:58:02 PM: Update 30868: task mnli, batch 868 (30805): accuracy: 0.8290, mnli_loss: 0.4391
09/07 07:58:11 PM: Update 30907: task wnli, batch 1 (64): accuracy: 0.6250, wnli_loss: 0.7374
09/07 07:58:12 PM: Update 30910: task mnli, batch 909 (30846): accuracy: 0.8296, mnli_loss: 0.4387
09/07 07:58:22 PM: Update 30951: task mnli, batch 950 (30887): accuracy: 0.8291, mnli_loss: 0.4381
09/07 07:58:32 PM: Update 30991: task mnli, batch 990 (30927): accuracy: 0.8292, mnli_loss: 0.4377
09/07 07:58:34 PM: ***** Step 31000 / Validation 31 *****
09/07 07:58:34 PM: mnli: trained on 999 batches, 0.061 epochs
09/07 07:58:34 PM: wnli: trained on 1 batches, 0.037 epochs
09/07 07:58:34 PM: Validating...
09/07 07:58:42 PM: Evaluate: task mnli, batch 115 (209): accuracy: 0.8076, mnli_loss: 0.4994
09/07 07:58:49 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.5417, wnli_loss: 0.6879
09/07 07:58:49 PM: Updating LR scheduler:
09/07 07:58:49 PM: 	Best result seen so far for macro_avg: 0.705
09/07 07:58:49 PM: 	# validation passes without improvement: 1
09/07 07:58:49 PM: mnli_loss: training: 0.437675 validation: 0.503137
09/07 07:58:49 PM: wnli_loss: training: 0.737424 validation: 0.699833
09/07 07:58:49 PM: macro_avg: validation: 0.658521
09/07 07:58:49 PM: micro_avg: validation: 0.805758
09/07 07:58:49 PM: mnli_accuracy: training: 0.829007 validation: 0.810000
09/07 07:58:49 PM: wnli_accuracy: training: 0.625000 validation: 0.507042
09/07 07:58:49 PM: Global learning rate: 6.25e-07
09/07 07:58:49 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 07:58:52 PM: Update 31011: task mnli, batch 11 (30947): accuracy: 0.8220, mnli_loss: 0.4357
09/07 07:59:03 PM: Update 31044: task mnli, batch 44 (30980): accuracy: 0.8053, mnli_loss: 0.4776
09/07 07:59:13 PM: Update 31084: task mnli, batch 84 (31020): accuracy: 0.8108, mnli_loss: 0.4748
09/07 07:59:23 PM: Update 31125: task mnli, batch 125 (31061): accuracy: 0.8145, mnli_loss: 0.4663
09/07 07:59:33 PM: Update 31166: task mnli, batch 166 (31102): accuracy: 0.8192, mnli_loss: 0.4554
09/07 07:59:43 PM: Update 31207: task mnli, batch 207 (31143): accuracy: 0.8194, mnli_loss: 0.4529
09/07 07:59:53 PM: Update 31248: task wnli, batch 1 (65): accuracy: 0.4167, wnli_loss: 0.7180
09/07 07:59:53 PM: Update 31249: task mnli, batch 248 (31184): accuracy: 0.8200, mnli_loss: 0.4517
09/07 08:00:04 PM: Update 31291: task mnli, batch 290 (31226): accuracy: 0.8180, mnli_loss: 0.4546
09/07 08:00:14 PM: Update 31332: task mnli, batch 331 (31267): accuracy: 0.8207, mnli_loss: 0.4501
09/07 08:00:24 PM: Update 31374: task mnli, batch 373 (31309): accuracy: 0.8225, mnli_loss: 0.4423
09/07 08:00:34 PM: Update 31416: task mnli, batch 415 (31351): accuracy: 0.8231, mnli_loss: 0.4421
09/07 08:00:45 PM: Update 31454: task mnli, batch 453 (31389): accuracy: 0.8254, mnli_loss: 0.4387
09/07 08:00:55 PM: Update 31496: task mnli, batch 495 (31431): accuracy: 0.8254, mnli_loss: 0.4406
09/07 08:01:05 PM: Update 31536: task mnli, batch 535 (31471): accuracy: 0.8237, mnli_loss: 0.4431
09/07 08:01:15 PM: Update 31578: task mnli, batch 577 (31513): accuracy: 0.8231, mnli_loss: 0.4420
09/07 08:01:25 PM: Update 31620: task mnli, batch 619 (31555): accuracy: 0.8233, mnli_loss: 0.4411
09/07 08:01:36 PM: Update 31662: task mnli, batch 661 (31597): accuracy: 0.8249, mnli_loss: 0.4407
09/07 08:01:46 PM: Update 31704: task mnli, batch 703 (31639): accuracy: 0.8256, mnli_loss: 0.4393
09/07 08:01:56 PM: Update 31746: task mnli, batch 745 (31681): accuracy: 0.8259, mnli_loss: 0.4412
09/07 08:02:06 PM: Update 31787: task mnli, batch 786 (31722): accuracy: 0.8258, mnli_loss: 0.4408
09/07 08:02:16 PM: Update 31829: task mnli, batch 828 (31764): accuracy: 0.8262, mnli_loss: 0.4413
09/07 08:02:26 PM: Update 31869: task mnli, batch 868 (31804): accuracy: 0.8263, mnli_loss: 0.4405
09/07 08:02:36 PM: Update 31903: task mnli, batch 902 (31838): accuracy: 0.8267, mnli_loss: 0.4400
09/07 08:02:47 PM: Update 31943: task mnli, batch 942 (31878): accuracy: 0.8267, mnli_loss: 0.4395
09/07 08:02:57 PM: Update 31985: task mnli, batch 984 (31920): accuracy: 0.8284, mnli_loss: 0.4367
09/07 08:03:00 PM: ***** Step 32000 / Validation 32 *****
09/07 08:03:00 PM: mnli: trained on 999 batches, 0.061 epochs
09/07 08:03:00 PM: wnli: trained on 1 batches, 0.037 epochs
09/07 08:03:00 PM: Validating...
09/07 08:03:07 PM: Evaluate: task mnli, batch 95 (209): accuracy: 0.8088, mnli_loss: 0.4866
09/07 08:03:14 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.5417, wnli_loss: 0.6860
09/07 08:03:14 PM: Updating LR scheduler:
09/07 08:03:14 PM: 	Best result seen so far for macro_avg: 0.705
09/07 08:03:14 PM: 	# validation passes without improvement: 2
09/07 08:03:14 PM: mnli_loss: training: 0.437496 validation: 0.499931
09/07 08:03:14 PM: wnli_loss: training: 0.717956 validation: 0.699143
09/07 08:03:14 PM: macro_avg: validation: 0.658021
09/07 08:03:14 PM: micro_avg: validation: 0.804772
09/07 08:03:14 PM: mnli_accuracy: training: 0.827739 validation: 0.809000
09/07 08:03:14 PM: wnli_accuracy: training: 0.416667 validation: 0.507042
09/07 08:03:14 PM: Global learning rate: 6.25e-07
09/07 08:03:14 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 08:03:17 PM: Update 32006: task mnli, batch 6 (31941): accuracy: 0.8681, mnli_loss: 0.3837
09/07 08:03:27 PM: Update 32048: task mnli, batch 48 (31983): accuracy: 0.8351, mnli_loss: 0.4337
09/07 08:03:37 PM: Update 32090: task mnli, batch 90 (32025): accuracy: 0.8296, mnli_loss: 0.4234
09/07 08:03:47 PM: Update 32130: task mnli, batch 130 (32065): accuracy: 0.8250, mnli_loss: 0.4324
09/07 08:03:57 PM: Update 32171: task mnli, batch 171 (32106): accuracy: 0.8287, mnli_loss: 0.4320
09/07 08:04:07 PM: Update 32212: task mnli, batch 212 (32147): accuracy: 0.8300, mnli_loss: 0.4339
09/07 08:04:17 PM: Update 32253: task mnli, batch 253 (32188): accuracy: 0.8277, mnli_loss: 0.4396
09/07 08:04:29 PM: Update 32288: task mnli, batch 288 (32223): accuracy: 0.8294, mnli_loss: 0.4362
09/07 08:04:39 PM: Update 32327: task mnli, batch 327 (32262): accuracy: 0.8282, mnli_loss: 0.4383
09/07 08:04:49 PM: Update 32367: task mnli, batch 367 (32302): accuracy: 0.8250, mnli_loss: 0.4447
09/07 08:04:59 PM: Update 32410: task mnli, batch 410 (32345): accuracy: 0.8266, mnli_loss: 0.4393
09/07 08:05:09 PM: Update 32452: task mnli, batch 452 (32387): accuracy: 0.8270, mnli_loss: 0.4388
09/07 08:05:20 PM: Update 32492: task mnli, batch 492 (32427): accuracy: 0.8279, mnli_loss: 0.4384
09/07 08:05:30 PM: Update 32535: task mnli, batch 535 (32470): accuracy: 0.8289, mnli_loss: 0.4362
09/07 08:05:40 PM: Update 32577: task mnli, batch 577 (32512): accuracy: 0.8299, mnli_loss: 0.4355
09/07 08:05:50 PM: Update 32619: task mnli, batch 619 (32554): accuracy: 0.8297, mnli_loss: 0.4351
09/07 08:06:00 PM: Update 32660: task mnli, batch 660 (32595): accuracy: 0.8305, mnli_loss: 0.4337
09/07 08:06:10 PM: Update 32701: task mnli, batch 701 (32636): accuracy: 0.8285, mnli_loss: 0.4382
09/07 08:06:20 PM: Update 32741: task mnli, batch 741 (32676): accuracy: 0.8285, mnli_loss: 0.4382
09/07 08:06:31 PM: Update 32782: task mnli, batch 782 (32717): accuracy: 0.8276, mnli_loss: 0.4389
09/07 08:06:38 PM: Update 32814: task wnli, batch 1 (66): accuracy: 0.5000, wnli_loss: 0.7200
09/07 08:06:41 PM: Update 32819: task mnli, batch 818 (32753): accuracy: 0.8267, mnli_loss: 0.4400
09/07 08:06:51 PM: Update 32860: task mnli, batch 859 (32794): accuracy: 0.8270, mnli_loss: 0.4390
09/07 08:07:02 PM: Update 32902: task mnli, batch 901 (32836): accuracy: 0.8282, mnli_loss: 0.4368
09/07 08:07:12 PM: Update 32944: task mnli, batch 943 (32878): accuracy: 0.8274, mnli_loss: 0.4380
09/07 08:07:22 PM: Update 32985: task mnli, batch 984 (32919): accuracy: 0.8279, mnli_loss: 0.4368
09/07 08:07:25 PM: ***** Step 33000 / Validation 33 *****
09/07 08:07:25 PM: mnli: trained on 999 batches, 0.061 epochs
09/07 08:07:25 PM: wnli: trained on 1 batches, 0.037 epochs
09/07 08:07:25 PM: Validating...
09/07 08:07:32 PM: Evaluate: task mnli, batch 95 (209): accuracy: 0.8132, mnli_loss: 0.4826
09/07 08:07:39 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.5417, wnli_loss: 0.6888
09/07 08:07:40 PM: Updating LR scheduler:
09/07 08:07:40 PM: 	Best result seen so far for macro_avg: 0.705
09/07 08:07:40 PM: 	# validation passes without improvement: 3
09/07 08:07:40 PM: mnli_loss: training: 0.436260 validation: 0.498036
09/07 08:07:40 PM: wnli_loss: training: 0.720006 validation: 0.699705
09/07 08:07:40 PM: macro_avg: validation: 0.658621
09/07 08:07:40 PM: micro_avg: validation: 0.805955
09/07 08:07:40 PM: mnli_accuracy: training: 0.827975 validation: 0.810200
09/07 08:07:40 PM: wnli_accuracy: training: 0.500000 validation: 0.507042
09/07 08:07:40 PM: Global learning rate: 6.25e-07
09/07 08:07:40 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 08:07:42 PM: Update 33006: task mnli, batch 6 (32940): accuracy: 0.8611, mnli_loss: 0.3850
09/07 08:07:52 PM: Update 33047: task mnli, batch 47 (32981): accuracy: 0.8378, mnli_loss: 0.4391
09/07 08:08:02 PM: Update 33088: task mnli, batch 88 (33022): accuracy: 0.8310, mnli_loss: 0.4483
09/07 08:08:12 PM: Update 33129: task mnli, batch 129 (33063): accuracy: 0.8337, mnli_loss: 0.4391
09/07 08:08:22 PM: Update 33172: task mnli, batch 172 (33106): accuracy: 0.8391, mnli_loss: 0.4284
09/07 08:08:32 PM: Update 33213: task mnli, batch 213 (33147): accuracy: 0.8382, mnli_loss: 0.4239
09/07 08:08:43 PM: Update 33242: task mnli, batch 242 (33176): accuracy: 0.8397, mnli_loss: 0.4212
09/07 08:08:53 PM: Update 33283: task mnli, batch 283 (33217): accuracy: 0.8420, mnli_loss: 0.4132
09/07 08:09:03 PM: Update 33324: task mnli, batch 324 (33258): accuracy: 0.8383, mnli_loss: 0.4197
09/07 08:09:13 PM: Update 33365: task mnli, batch 365 (33299): accuracy: 0.8362, mnli_loss: 0.4208
09/07 08:09:23 PM: Update 33410: task mnli, batch 410 (33344): accuracy: 0.8388, mnli_loss: 0.4152
09/07 08:09:25 PM: Update 33418: task wnli, batch 1 (67): accuracy: 0.5417, wnli_loss: 0.7127
09/07 08:09:34 PM: Update 33452: task mnli, batch 451 (33385): accuracy: 0.8376, mnli_loss: 0.4185
09/07 08:09:44 PM: Update 33493: task mnli, batch 492 (33426): accuracy: 0.8364, mnli_loss: 0.4196
09/07 08:09:49 PM: Update 33517: task wnli, batch 2 (68): accuracy: 0.5000, wnli_loss: 0.7087
09/07 08:09:54 PM: Update 33536: task mnli, batch 534 (33468): accuracy: 0.8366, mnli_loss: 0.4186
09/07 08:10:04 PM: Update 33578: task mnli, batch 576 (33510): accuracy: 0.8362, mnli_loss: 0.4192
09/07 08:10:14 PM: Update 33621: task mnli, batch 619 (33553): accuracy: 0.8349, mnli_loss: 0.4207
09/07 08:10:24 PM: Update 33655: task mnli, batch 653 (33587): accuracy: 0.8346, mnli_loss: 0.4216
09/07 08:10:35 PM: Update 33696: task mnli, batch 694 (33628): accuracy: 0.8339, mnli_loss: 0.4232
09/07 08:10:45 PM: Update 33738: task mnli, batch 736 (33670): accuracy: 0.8339, mnli_loss: 0.4228
09/07 08:10:55 PM: Update 33780: task mnli, batch 778 (33712): accuracy: 0.8339, mnli_loss: 0.4225
09/07 08:11:05 PM: Update 33822: task mnli, batch 820 (33754): accuracy: 0.8344, mnli_loss: 0.4215
09/07 08:11:15 PM: Update 33863: task mnli, batch 861 (33795): accuracy: 0.8341, mnli_loss: 0.4216
09/07 08:11:25 PM: Update 33904: task mnli, batch 902 (33836): accuracy: 0.8338, mnli_loss: 0.4221
09/07 08:11:35 PM: Update 33944: task mnli, batch 942 (33876): accuracy: 0.8336, mnli_loss: 0.4222
09/07 08:11:41 PM: Update 33970: task wnli, batch 3 (69): accuracy: 0.4861, wnli_loss: 0.7127
09/07 08:11:45 PM: Update 33985: task mnli, batch 982 (33916): accuracy: 0.8337, mnli_loss: 0.4223
09/07 08:11:49 PM: ***** Step 34000 / Validation 34 *****
09/07 08:11:49 PM: mnli: trained on 997 batches, 0.061 epochs
09/07 08:11:49 PM: wnli: trained on 3 batches, 0.111 epochs
09/07 08:11:49 PM: Validating...
09/07 08:11:55 PM: Evaluate: task mnli, batch 96 (209): accuracy: 0.8147, mnli_loss: 0.4783
09/07 08:12:03 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.5417, wnli_loss: 0.6900
09/07 08:12:03 PM: Updating LR scheduler:
09/07 08:12:03 PM: 	Best result seen so far for macro_avg: 0.705
09/07 08:12:03 PM: 	# validation passes without improvement: 4
09/07 08:12:03 PM: mnli_loss: training: 0.422665 validation: 0.492581
09/07 08:12:03 PM: wnli_loss: training: 0.712669 validation: 0.699715
09/07 08:12:03 PM: macro_avg: validation: 0.652579
09/07 08:12:03 PM: micro_avg: validation: 0.807730
09/07 08:12:03 PM: mnli_accuracy: training: 0.833556 validation: 0.812200
09/07 08:12:03 PM: wnli_accuracy: training: 0.486111 validation: 0.492958
09/07 08:12:03 PM: Global learning rate: 6.25e-07
09/07 08:12:03 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 08:12:05 PM: Update 34006: task mnli, batch 6 (33937): accuracy: 0.8333, mnli_loss: 0.4004
09/07 08:12:16 PM: Update 34049: task mnli, batch 49 (33980): accuracy: 0.8197, mnli_loss: 0.4490
09/07 08:12:26 PM: Update 34081: task mnli, batch 81 (34012): accuracy: 0.8244, mnli_loss: 0.4433
09/07 08:12:36 PM: Update 34122: task mnli, batch 122 (34053): accuracy: 0.8318, mnli_loss: 0.4243
09/07 08:12:46 PM: Update 34163: task mnli, batch 163 (34094): accuracy: 0.8320, mnli_loss: 0.4231
09/07 08:12:52 PM: Update 34190: task wnli, batch 1 (70): accuracy: 0.5000, wnli_loss: 0.6914
09/07 08:12:56 PM: Update 34205: task mnli, batch 204 (34135): accuracy: 0.8333, mnli_loss: 0.4244
09/07 08:13:06 PM: Update 34247: task mnli, batch 246 (34177): accuracy: 0.8312, mnli_loss: 0.4339
09/07 08:13:16 PM: Update 34290: task mnli, batch 289 (34220): accuracy: 0.8323, mnli_loss: 0.4318
09/07 08:13:26 PM: Update 34332: task mnli, batch 331 (34262): accuracy: 0.8352, mnli_loss: 0.4252
09/07 08:13:37 PM: Update 34374: task mnli, batch 373 (34304): accuracy: 0.8352, mnli_loss: 0.4224
09/07 08:13:47 PM: Update 34414: task mnli, batch 413 (34344): accuracy: 0.8344, mnli_loss: 0.4224
09/07 08:13:57 PM: Update 34456: task mnli, batch 455 (34386): accuracy: 0.8354, mnli_loss: 0.4206
09/07 08:14:08 PM: Update 34491: task mnli, batch 490 (34421): accuracy: 0.8355, mnli_loss: 0.4223
09/07 08:14:18 PM: Update 34533: task mnli, batch 532 (34463): accuracy: 0.8356, mnli_loss: 0.4215
09/07 08:14:28 PM: Update 34574: task mnli, batch 573 (34504): accuracy: 0.8350, mnli_loss: 0.4225
09/07 08:14:38 PM: Update 34615: task mnli, batch 614 (34545): accuracy: 0.8339, mnli_loss: 0.4240
09/07 08:14:48 PM: Update 34658: task mnli, batch 657 (34588): accuracy: 0.8335, mnli_loss: 0.4243
09/07 08:14:58 PM: Update 34699: task mnli, batch 698 (34629): accuracy: 0.8334, mnli_loss: 0.4228
09/07 08:15:08 PM: Update 34740: task mnli, batch 739 (34670): accuracy: 0.8333, mnli_loss: 0.4233
09/07 08:15:18 PM: Update 34779: task mnli, batch 778 (34709): accuracy: 0.8334, mnli_loss: 0.4233
09/07 08:15:28 PM: Update 34821: task mnli, batch 820 (34751): accuracy: 0.8326, mnli_loss: 0.4250
09/07 08:15:37 PM: Update 34855: task wnli, batch 2 (71): accuracy: 0.4792, wnli_loss: 0.6887
09/07 08:15:38 PM: Update 34862: task mnli, batch 860 (34791): accuracy: 0.8336, mnli_loss: 0.4234
09/07 08:15:49 PM: Update 34903: task mnli, batch 901 (34832): accuracy: 0.8346, mnli_loss: 0.4221
09/07 08:15:59 PM: Update 34929: task mnli, batch 927 (34858): accuracy: 0.8349, mnli_loss: 0.4215
09/07 08:16:09 PM: Update 34970: task mnli, batch 968 (34899): accuracy: 0.8352, mnli_loss: 0.4205
09/07 08:16:16 PM: ***** Step 35000 / Validation 35 *****
09/07 08:16:16 PM: mnli: trained on 998 batches, 0.061 epochs
09/07 08:16:16 PM: wnli: trained on 2 batches, 0.074 epochs
09/07 08:16:16 PM: Validating...
09/07 08:16:19 PM: Evaluate: task mnli, batch 45 (209): accuracy: 0.8204, mnli_loss: 0.4657
09/07 08:16:30 PM: Evaluate: task mnli, batch 191 (209): accuracy: 0.8130, mnli_loss: 0.4857
09/07 08:16:31 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.5417, wnli_loss: 0.6915
09/07 08:16:31 PM: Updating LR scheduler:
09/07 08:16:31 PM: 	Best result seen so far for macro_avg: 0.705
09/07 08:16:31 PM: 	# validation passes without improvement: 0
09/07 08:16:31 PM: mnli_loss: training: 0.421372 validation: 0.492853
09/07 08:16:31 PM: wnli_loss: training: 0.688658 validation: 0.700159
09/07 08:16:31 PM: macro_avg: validation: 0.658821
09/07 08:16:31 PM: micro_avg: validation: 0.806350
09/07 08:16:31 PM: mnli_accuracy: training: 0.835089 validation: 0.810600
09/07 08:16:31 PM: wnli_accuracy: training: 0.479167 validation: 0.507042
09/07 08:16:31 PM: Global learning rate: 3.125e-07
09/07 08:16:31 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 08:16:40 PM: Update 35030: task mnli, batch 30 (34959): accuracy: 0.8542, mnli_loss: 0.3807
09/07 08:16:50 PM: Update 35070: task mnli, batch 70 (34999): accuracy: 0.8429, mnli_loss: 0.4063
09/07 08:17:00 PM: Update 35112: task mnli, batch 112 (35041): accuracy: 0.8464, mnli_loss: 0.3994
09/07 08:17:10 PM: Update 35152: task mnli, batch 152 (35081): accuracy: 0.8405, mnli_loss: 0.4073
09/07 08:17:20 PM: Update 35194: task mnli, batch 194 (35123): accuracy: 0.8458, mnli_loss: 0.3990
09/07 08:17:30 PM: Update 35238: task mnli, batch 238 (35167): accuracy: 0.8463, mnli_loss: 0.4010
09/07 08:17:41 PM: Update 35278: task mnli, batch 278 (35207): accuracy: 0.8473, mnli_loss: 0.3994
09/07 08:17:51 PM: Update 35319: task mnli, batch 319 (35248): accuracy: 0.8465, mnli_loss: 0.4017
09/07 08:18:01 PM: Update 35352: task mnli, batch 352 (35281): accuracy: 0.8460, mnli_loss: 0.4036
09/07 08:18:11 PM: Update 35394: task mnli, batch 394 (35323): accuracy: 0.8467, mnli_loss: 0.4035
09/07 08:18:21 PM: Update 35435: task mnli, batch 435 (35364): accuracy: 0.8455, mnli_loss: 0.4042
09/07 08:18:32 PM: Update 35474: task mnli, batch 474 (35403): accuracy: 0.8450, mnli_loss: 0.4049
09/07 08:18:42 PM: Update 35515: task mnli, batch 515 (35444): accuracy: 0.8434, mnli_loss: 0.4074
09/07 08:18:52 PM: Update 35557: task mnli, batch 557 (35486): accuracy: 0.8450, mnli_loss: 0.4058
09/07 08:19:02 PM: Update 35598: task mnli, batch 598 (35527): accuracy: 0.8456, mnli_loss: 0.4037
09/07 08:19:12 PM: Update 35638: task mnli, batch 638 (35567): accuracy: 0.8454, mnli_loss: 0.4044
09/07 08:19:23 PM: Update 35681: task mnli, batch 681 (35610): accuracy: 0.8451, mnli_loss: 0.4047
09/07 08:19:33 PM: Update 35721: task mnli, batch 721 (35650): accuracy: 0.8451, mnli_loss: 0.4051
09/07 08:19:43 PM: Update 35753: task mnli, batch 753 (35682): accuracy: 0.8440, mnli_loss: 0.4071
09/07 08:19:53 PM: Update 35793: task mnli, batch 793 (35722): accuracy: 0.8431, mnli_loss: 0.4090
09/07 08:20:03 PM: Update 35834: task mnli, batch 834 (35763): accuracy: 0.8434, mnli_loss: 0.4085
09/07 08:20:13 PM: Update 35878: task mnli, batch 878 (35807): accuracy: 0.8434, mnli_loss: 0.4066
09/07 08:20:23 PM: Update 35920: task mnli, batch 920 (35849): accuracy: 0.8436, mnli_loss: 0.4060
09/07 08:20:33 PM: Update 35961: task mnli, batch 961 (35890): accuracy: 0.8445, mnli_loss: 0.4045
09/07 08:20:43 PM: ***** Step 36000 / Validation 36 *****
09/07 08:20:43 PM: mnli: trained on 1000 batches, 0.061 epochs
09/07 08:20:43 PM: wnli: trained on 0 batches, 0.000 epochs
09/07 08:20:43 PM: Validating...
09/07 08:20:44 PM: Evaluate: task mnli, batch 7 (209): accuracy: 0.8214, mnli_loss: 0.4531
09/07 08:20:54 PM: Evaluate: task mnli, batch 156 (209): accuracy: 0.8133, mnli_loss: 0.4861
09/07 08:20:57 PM: Evaluate: task wnli, batch 1 (3): accuracy: 0.5417, wnli_loss: 0.6916
09/07 08:20:57 PM: Updating LR scheduler:
09/07 08:20:57 PM: 	Best result seen so far for macro_avg: 0.705
09/07 08:20:57 PM: 	# validation passes without improvement: 1
09/07 08:20:57 PM: Ran out of early stopping patience. Stopping training.
09/07 08:20:57 PM: mnli_loss: training: 0.403919 validation: 0.494028
09/07 08:20:57 PM: wnli_loss: training: 0.000000 validation: 0.700254
09/07 08:20:57 PM: macro_avg: validation: 0.659321
09/07 08:20:57 PM: micro_avg: validation: 0.807336
09/07 08:20:57 PM: mnli_accuracy: training: 0.845022 validation: 0.811600
09/07 08:20:57 PM: wnli_accuracy: validation: 0.507042
09/07 08:20:57 PM: Global learning rate: 3.125e-07
09/07 08:20:57 PM: Saving checkpoints to: diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 08:20:59 PM: Stopped training after 36 validation checks
09/07 08:20:59 PM: Trained mnli for 35929 batches or 2.196 epochs
09/07 08:20:59 PM: Trained wnli for 71 batches or 2.630 epochs
09/07 08:20:59 PM: ***** VALIDATION RESULTS *****
09/07 08:20:59 PM: mnli_accuracy (for best val pass 28): mnli_loss: 0.49795, wnli_loss: 0.70097, macro_avg: 0.67501, micro_avg: 0.81089, mnli_accuracy: 0.81480, wnli_accuracy: 0.53521
09/07 08:20:59 PM: wnli_accuracy (for best val pass 15): mnli_loss: 0.49616, wnli_loss: 0.69185, macro_avg: 0.70542, micro_avg: 0.80241, mnli_accuracy: 0.80520, wnli_accuracy: 0.60563
09/07 08:20:59 PM: micro_avg (for best val pass 28): mnli_loss: 0.49795, wnli_loss: 0.70097, macro_avg: 0.67501, micro_avg: 0.81089, mnli_accuracy: 0.81480, wnli_accuracy: 0.53521
09/07 08:20:59 PM: macro_avg (for best val pass 15): mnli_loss: 0.49616, wnli_loss: 0.69185, macro_avg: 0.70542, micro_avg: 0.80241, mnli_accuracy: 0.80520, wnli_accuracy: 0.60563
09/07 08:20:59 PM: Evaluating...
09/07 08:20:59 PM: Loaded model state from diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval/model_state_pretrain_val_15.best.th
09/07 08:20:59 PM: Evaluating on: glue-diagnostic, split: val
09/07 08:21:04 PM: Task 'glue-diagnostic': sorting predictions by 'idx'
09/07 08:21:04 PM: Finished evaluating on: glue-diagnostic
09/07 08:21:04 PM: Wrote predictions for task: glue-diagnostic
09/07 08:21:04 PM: Task 'glue-diagnostic': Wrote predictions to diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 08:21:04 PM: Wrote all preds for split 'val' to diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 08:21:04 PM: Evaluating on: glue-diagnostic, split: test
09/07 08:21:09 PM: Task 'glue-diagnostic': sorting predictions by 'idx'
09/07 08:21:09 PM: Finished evaluating on: glue-diagnostic
09/07 08:21:09 PM: Wrote predictions for task: glue-diagnostic
09/07 08:21:09 PM: Task 'glue-diagnostic': Wrote predictions to diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 08:21:09 PM: Wrote all preds for split 'test' to diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 08:21:09 PM: Writing results for split 'val' to diagnostic_run_2/mnli_wnli/results.tsv
09/07 08:21:09 PM: micro_avg: 0.000, macro_avg: 0.000, glue-diagnostic_lex_sem: 0.334, glue-diagnostic_lex_sem__Lexical entailment;Factivity: 0.000, glue-diagnostic_lex_sem__Lexical entailment;Quantifiers: 0.671, glue-diagnostic_lex_sem__Morphological negation: 0.500, glue-diagnostic_lex_sem__Symmetry/Collectivity: 0.000, glue-diagnostic_lex_sem__Redundancy: 0.592, glue-diagnostic_lex_sem__Lexical entailment: 0.282, glue-diagnostic_lex_sem__Factivity;Quantifiers: 0.000, glue-diagnostic_lex_sem__Factivity: 0.086, glue-diagnostic_lex_sem__Named entities: 0.209, glue-diagnostic_lex_sem__Quantifiers: 0.515, glue-diagnostic_pr_ar_str: 0.372, glue-diagnostic_pr_ar_str__Active/Passive;Prepositional phrases: 1.000, glue-diagnostic_pr_ar_str__Restrictivity;Relative clauses: -1.000, glue-diagnostic_pr_ar_str__Anaphora/Coreference: 0.306, glue-diagnostic_pr_ar_str__Core args: 0.318, glue-diagnostic_pr_ar_str__Restrictivity: -0.250, glue-diagnostic_pr_ar_str__Coordination scope;Prepositional phrases: 0.333, glue-diagnostic_pr_ar_str__Intersectivity: 0.207, glue-diagnostic_pr_ar_str__Nominalization;Genitives/Partitives: 0.000, glue-diagnostic_pr_ar_str__Anaphora/Coreference;Prepositional phrases: 0.000, glue-diagnostic_pr_ar_str__Relative clauses;Restrictivity: 0.577, glue-diagnostic_pr_ar_str__Core args;Anaphora/Coreference: 0.447, glue-diagnostic_pr_ar_str__Intersectivity;Ellipsis/Implicits: 0.000, glue-diagnostic_pr_ar_str__Restrictivity;Anaphora/Coreference: 0.000, glue-diagnostic_pr_ar_str__Ellipsis/Implicits;Anaphora/Coreference: -0.289, glue-diagnostic_pr_ar_str__Nominalization: 0.363, glue-diagnostic_pr_ar_str__Datives: 0.673, glue-diagnostic_pr_ar_str__Relative clauses: 0.106, glue-diagnostic_pr_ar_str__Prepositional phrases: 0.707, glue-diagnostic_pr_ar_str__Ellipsis/Implicits: 0.316, glue-diagnostic_pr_ar_str__Coordination scope: 0.390, glue-diagnostic_pr_ar_str__Active/Passive: 0.445, glue-diagnostic_pr_ar_str__Relative clauses;Anaphora/Coreference: 1.000, glue-diagnostic_pr_ar_str__Genitives/Partitives: 0.685, glue-diagnostic_logic: 0.181, glue-diagnostic_logic__Conjunction: 0.408, glue-diagnostic_logic__Conjunction;Negation: 0.250, glue-diagnostic_logic__Disjunction: -0.374, glue-diagnostic_logic__Upward monotone: 0.261, glue-diagnostic_logic__Downward monotone;Conditionals: -1.000, glue-diagnostic_logic__Conjunction;Upward monotone: 1.000, glue-diagnostic_logic__Disjunction;Conjunction: 0.000, glue-diagnostic_logic__Existential;Upward monotone: 1.000, glue-diagnostic_logic__Existential;Negation: 0.000, glue-diagnostic_logic__Universal;Negation: 0.000, glue-diagnostic_logic__Intervals/Numbers: -0.170, glue-diagnostic_logic__Double negation: 0.327, glue-diagnostic_logic__Existential: 0.400, glue-diagnostic_logic__Disjunction;Conditionals;Negation: 0.000, glue-diagnostic_logic__Non-monotone: 0.108, glue-diagnostic_logic__Temporal;Intervals/Numbers: 0.000, glue-diagnostic_logic__Disjunction;Non-monotone: 0.000, glue-diagnostic_logic__Downward monotone;Existential;Negation: -1.000, glue-diagnostic_logic__Intervals/Numbers;Non-monotone: 0.707, glue-diagnostic_logic__Downward monotone: -0.487, glue-diagnostic_logic__Double negation;Negation: 0.000, glue-diagnostic_logic__Negation: -0.020, glue-diagnostic_logic__Conditionals: 0.247, glue-diagnostic_logic__Universal;Conjunction: 0.000, glue-diagnostic_logic__Temporal: 0.116, glue-diagnostic_logic__Universal: 0.569, glue-diagnostic_logic__Negation;Conditionals: 0.000, glue-diagnostic_logic__Disjunction;Negation: -0.316, glue-diagnostic_logic__Temporal;Conjunction: 0.000, glue-diagnostic_knowledge: 0.210, glue-diagnostic_knowledge__World knowledge: 0.181, glue-diagnostic_knowledge__Common sense: 0.245, glue-diagnostic_all_mcc: 0.306, glue-diagnostic_accuracy: 0.554
09/07 08:21:09 PM: Loaded model state from diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval/model_state_pretrain_val_15.best.th
09/07 08:21:09 PM: Evaluating on: mnli, split: val
09/07 08:21:39 PM: 	Task mnli: batch 447
09/07 08:22:05 PM: Task 'mnli': sorting predictions by 'idx'
09/07 08:22:05 PM: Finished evaluating on: mnli
09/07 08:22:05 PM: Wrote predictions for task: mnli
09/07 08:22:05 PM: Task 'mnli': Wrote predictions to diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 08:22:05 PM: Wrote all preds for split 'val' to diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 08:22:05 PM: Evaluating on: mnli, split: test
09/07 08:22:35 PM: 	Task mnli: batch 426
09/07 08:23:02 PM: Task 'mnli': sorting predictions by 'idx'
09/07 08:23:02 PM: Finished evaluating on: mnli
09/07 08:23:02 PM: There are 19643 examples in MNLI, 19643 were expected
09/07 08:23:02 PM: Wrote predictions for task: mnli
09/07 08:23:02 PM: Task 'mnli': Wrote predictions to diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 08:23:02 PM: Wrote all preds for split 'test' to diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 08:23:02 PM: Writing results for split 'val' to diagnostic_run_2/mnli_wnli/results.tsv
09/07 08:23:02 PM: micro_avg: 0.809, macro_avg: 0.809, mnli_accuracy: 0.809
09/07 08:23:03 PM: Loaded model state from diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval/model_state_pretrain_val_15.best.th
09/07 08:23:03 PM: Evaluating on: wnli, split: val
09/07 08:23:03 PM: Task 'wnli': sorting predictions by 'idx'
09/07 08:23:03 PM: Finished evaluating on: wnli
09/07 08:23:03 PM: Wrote predictions for task: wnli
09/07 08:23:03 PM: Task 'wnli': Wrote predictions to diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 08:23:03 PM: Wrote all preds for split 'val' to diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 08:23:03 PM: Evaluating on: wnli, split: test
09/07 08:23:03 PM: Task 'wnli': sorting predictions by 'idx'
09/07 08:23:03 PM: Finished evaluating on: wnli
09/07 08:23:03 PM: Wrote predictions for task: wnli
09/07 08:23:03 PM: Task 'wnli': Wrote predictions to diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 08:23:03 PM: Wrote all preds for split 'test' to diagnostic_run_2/mnli_wnli/mnli_wnli_diagnostic_eval
09/07 08:23:03 PM: Writing results for split 'val' to diagnostic_run_2/mnli_wnli/results.tsv
09/07 08:23:03 PM: micro_avg: 0.606, macro_avg: 0.606, wnli_accuracy: 0.606
09/07 08:23:03 PM: Done!
