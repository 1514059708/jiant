// Config modifications to apply during all final *BERT* runs.

// This imports the defaults, which can be overridden below.
include "defaults.conf"

// _All_ final runs will share preproc and tasks.
exp_name = final-bert

tokenizer = "bert-base-cased"
bert_model_name = "bert-base-cased"
bert_embeddings_mode = "top"   // how to use the outputs of the BERT module
                                // set as "top", we use only the top-layer activation
                                // other options: "only" uses the lexical layer (first layer)
                                //                "cat" uses lexical layer + top layer
bert_fine_tune = 1
elmo = 0
elmo_chars_only = 0

classifier = log_reg // following BERT paper

dropout = 0.1 // following BERT paper
optimizer = bert_adam
lr_patience = 4
patience = 20
max_vals = 10000

// For all generation tasks.
max_word_v_size = 20000
max_targ_word_v_size = 20000

// For MTL.
weighting_method = power_0.75
scaling_method = uniform

write_preds = val  // Somewhat slow, but should be useful for analysis.
