// Config modifications to apply during all final *BERT* runs.

// This imports the defaults, which can be overridden below.
include "defaults.conf"

// _All_ final runs will share preproc and tasks.
max_seq_len = 80
tokenizer = "bert-large-cased"
bert_model_name = "bert-large-cased"
exp_name = "bert-large-cased"
bert_embeddings_mode = "none"   // how to use the outputs of the BERT module
                                // set as "top", we use only the top-layer activation
                                // other options: "only" uses the lexical layer (first layer)
                                //                "cat" uses lexical layer + top layer
sent_enc = "null"
sep_embs_for_skip = 1
bert_fine_tune = 1
elmo = 0
elmo_chars_only = 0 
pair_attn = 0 // shouldn't be needed but JIC
s2s = {
    attention = none
}
classifier = log_reg // following BERT paper
pretrain_tasks = "cola"  // empty: don't run main training phase
target_tasks = "probing_metadata"

do_pretrain = 1
do_target_task_training = 0
do_full_eval = 0

dropout = 0.1 // following BERT paper
optimizer = bert_adam
batch_size = 16
max_epochs = 3
lr = .00001
min_lr = .0000001
lr_patience = 4
patience = 20
max_vals = 10000

// For all generation tasks.
max_word_v_size = 20000
max_targ_word_v_size = 20000

// For MTL.
weighting_method = power_0.75
scaling_method = uniform

write_preds = val  // Somewhat slow, but should be useful for analysis.

cola = {}
cola_classifier_dropout = 0.1
cola_val_interval = 100
cola_lr = 0.00001

