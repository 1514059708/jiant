09/05 02:17:48 PM: Git branch: implement_data_parallel
09/05 02:17:48 PM: Git SHA: b9db7dcc389b2e41da8b5fb5af6cef3eb9962ce9
09/05 02:17:48 PM: Parsed args: 
{
  "batch_size": 24,
  "classifier": "log_reg",
  "do_target_task_training": 0,
  "dropout": 0.1,
  "dropout_embs": 0.1,
  "input_module": "bert-base-cased",
  "local_log_path": "diagnostic_run_2/my-experiment/mnli_copa_diagnostic/log.log",
  "lr": "1e-5",
  "lr_patience": 4,
  "max_epochs": 3,
  "max_vals": 10000,
  "min_lr": 0.0,
  "optimizer": "bert_adam",
  "patience": 20,
  "pretrain_tasks": "mnli,copa",
  "pytorch_transformers_output_mode": "top",
  "random_seed": 42,
  "remote_log_name": "my-experiment__mnli_copa_diagnostic",
  "run_dir": "diagnostic_run_2/my-experiment/mnli_copa_diagnostic",
  "run_name": "mnli_copa_diagnostic",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "glue-diagnostic",
  "transfer_paradigm": "finetune",
  "write_preds": "val,test"
}
09/05 02:17:48 PM: Saved config to diagnostic_run_2/my-experiment/mnli_copa_diagnostic/params.conf
09/05 02:17:48 PM: Using random seed 42
09/05 02:17:49 PM: Using GPU 0
09/05 02:17:49 PM: Loading tasks...
09/05 02:17:49 PM: Writing pre-preprocessed tasks to diagnostic_run_2/my-experiment/
09/05 02:17:49 PM: 	Loaded existing task copa
09/05 02:17:49 PM: 	Task 'copa': |train|=400 |val|=100 |test|=500
09/05 02:17:49 PM: 	Loaded existing task glue-diagnostic
09/05 02:17:49 PM: 	Task 'glue-diagnostic': |train|=1104 |val|=1104 |test|=1104
09/05 02:17:52 PM: 	Loaded existing task mnli
09/05 02:17:52 PM: 	Task 'mnli': |train|=392702 |val|=19647 |test|=19643
09/05 02:17:52 PM: 	Finished loading tasks: copa glue-diagnostic mnli.
09/05 02:17:52 PM: Loading token dictionary from diagnostic_run_2/my-experiment/vocab.
09/05 02:17:52 PM: 	Loaded vocab from diagnostic_run_2/my-experiment/vocab
09/05 02:17:52 PM: 	Vocab namespace bert_cased: size 28998
09/05 02:17:52 PM: 	Vocab namespace tokens: size 25700
09/05 02:17:52 PM: 	Vocab namespace chars: size 139
09/05 02:17:52 PM: 	Finished building vocab.
09/05 02:17:52 PM: 	Task 'copa', split 'train': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/copa__train_data
09/05 02:17:52 PM: 	Task 'copa', split 'val': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/copa__val_data
09/05 02:17:52 PM: 	Task 'copa', split 'test': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/copa__test_data
09/05 02:17:52 PM: 	Task 'glue-diagnostic', split 'train': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/glue-diagnostic__train_data
09/05 02:17:52 PM: 	Task 'glue-diagnostic', split 'val': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/glue-diagnostic__val_data
09/05 02:17:52 PM: 	Task 'glue-diagnostic', split 'test': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/glue-diagnostic__test_data
09/05 02:17:52 PM: 	Task 'mnli', split 'train': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/mnli__train_data
09/05 02:17:52 PM: 	Task 'mnli', split 'val': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/mnli__val_data
09/05 02:17:52 PM: 	Task 'mnli', split 'test': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/mnli__test_data
09/05 02:17:52 PM: 	Finished indexing tasks
09/05 02:17:52 PM: 	Creating trimmed pretraining-only version of copa train.
09/05 02:17:52 PM: 	Creating trimmed target-only version of glue-diagnostic train.
09/05 02:17:52 PM: 	Creating trimmed pretraining-only version of mnli train.
09/05 02:17:52 PM: 	  Training on mnli, copa
09/05 02:17:52 PM: 	  Evaluating on glue-diagnostic
09/05 02:17:52 PM: 	Finished loading tasks in 3.630s
09/05 02:17:52 PM: 	 Tasks: ['copa', 'glue-diagnostic', 'mnli']
09/05 02:17:52 PM: Building model...
09/05 02:17:52 PM: Using BERT model (bert-base-cased).
09/05 02:17:52 PM: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at diagnostic_run_2/my-experiment/pytorch_transformers_cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.d7a3af18ce3a2ab7c0f48f04dc8daff45ed9a3ed333b9e9a79d012a0dedf87a6
09/05 02:17:52 PM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

09/05 02:17:52 PM: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at diagnostic_run_2/my-experiment/pytorch_transformers_cache/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
09/05 02:17:55 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at diagnostic_run_2/my-experiment/pytorch_transformers_cache/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
09/05 02:17:55 PM: Initializing parameters
09/05 02:17:55 PM: Done initializing parameters; the following parameters are using their default initialization from their code
09/05 02:17:55 PM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/05 02:17:55 PM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/05 02:17:55 PM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/05 02:17:55 PM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/05 02:17:55 PM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/05 02:17:55 PM:    _text_field_embedder.model.pooler.dense.bias
09/05 02:17:55 PM:    _text_field_embedder.model.pooler.dense.weight
09/05 02:17:55 PM: 	Task 'mnli' params: {
  "cls_type": "log_reg",
  "d_hid": 512,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 1,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "mnli"
}
09/05 02:17:55 PM: 	Task 'glue-diagnostic' params: {
  "cls_type": "log_reg",
  "d_hid": 512,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 1,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "mnli"
}
09/05 02:17:55 PM: 	Task 'copa' params: {
  "cls_type": "log_reg",
  "d_hid": 512,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 1,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "copa"
}
09/05 02:17:55 PM: batch_first = True
09/05 02:17:55 PM: stateful = False
09/05 02:17:55 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/05 02:17:55 PM: CURRENTLY DEFINED PARAMETERS: 
09/05 02:17:55 PM: input_size = 1536
09/05 02:17:55 PM: hidden_size = 512
09/05 02:17:55 PM: num_layers = 1
09/05 02:17:55 PM: bidirectional = True
09/05 02:17:55 PM: batch_first = True
09/05 02:17:55 PM: Initializing parameters
09/05 02:17:55 PM: Done initializing parameters; the following parameters are using their default initialization from their code
09/05 02:17:55 PM:    _modeling_layer._module.bias_hh_l0
09/05 02:17:55 PM:    _modeling_layer._module.bias_hh_l0_reverse
09/05 02:17:55 PM:    _modeling_layer._module.bias_ih_l0
09/05 02:17:55 PM:    _modeling_layer._module.bias_ih_l0_reverse
09/05 02:17:55 PM:    _modeling_layer._module.weight_hh_l0
09/05 02:17:55 PM:    _modeling_layer._module.weight_hh_l0_reverse
09/05 02:17:55 PM:    _modeling_layer._module.weight_ih_l0
09/05 02:17:55 PM:    _modeling_layer._module.weight_ih_l0_reverse
09/05 02:17:55 PM: Name of the task is different than the classifier it should use
09/05 02:17:59 PM: Model specification:
09/05 02:17:59 PM: DataParallel(
  (module): MultiTaskModel(
    (sent_encoder): SentenceEncoder(
      (_text_field_embedder): BertEmbedderModule(
        (model): BertModel(
          (embeddings): BertEmbeddings(
            (word_embeddings): Embedding(28996, 768, padding_idx=0)
            (position_embeddings): Embedding(512, 768)
            (token_type_embeddings): Embedding(2, 768)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (encoder): BertEncoder(
            (layer): ModuleList(
              (0): BertLayer(
                (attention): BertAttention(
                  (self): BertSelfAttention(
                    (query): Linear(in_features=768, out_features=768, bias=True)
                    (key): Linear(in_features=768, out_features=768, bias=True)
                    (value): Linear(in_features=768, out_features=768, bias=True)
                    (dropout): Dropout(p=0.1)
                  )
                  (output): BertSelfOutput(
                    (dense): Linear(in_features=768, out_features=768, bias=True)
                    (LayerNorm): BertLayerNorm()
                    (dropout): Dropout(p=0.1)
                  )
                )
                (intermediate): BertIntermediate(
                  (dense): Linear(in_features=768, out_features=3072, bias=True)
                )
                (output): BertOutput(
                  (dense): Linear(in_features=3072, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (1): BertLayer(
                (attention): BertAttention(
                  (self): BertSelfAttention(
                    (query): Linear(in_features=768, out_features=768, bias=True)
                    (key): Linear(in_features=768, out_features=768, bias=True)
                    (value): Linear(in_features=768, out_features=768, bias=True)
                    (dropout): Dropout(p=0.1)
                  )
                  (output): BertSelfOutput(
                    (dense): Linear(in_features=768, out_features=768, bias=True)
                    (LayerNorm): BertLayerNorm()
                    (dropout): Dropout(p=0.1)
                  )
                )
                (intermediate): BertIntermediate(
                  (dense): Linear(in_features=768, out_features=3072, bias=True)
                )
                (output): BertOutput(
                  (dense): Linear(in_features=3072, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (2): BertLayer(
                (attention): BertAttention(
                  (self): BertSelfAttention(
                    (query): Linear(in_features=768, out_features=768, bias=True)
                    (key): Linear(in_features=768, out_features=768, bias=True)
                    (value): Linear(in_features=768, out_features=768, bias=True)
                    (dropout): Dropout(p=0.1)
                  )
                  (output): BertSelfOutput(
                    (dense): Linear(in_features=768, out_features=768, bias=True)
                    (LayerNorm): BertLayerNorm()
                    (dropout): Dropout(p=0.1)
                  )
                )
                (intermediate): BertIntermediate(
                  (dense): Linear(in_features=768, out_features=3072, bias=True)
                )
                (output): BertOutput(
                  (dense): Linear(in_features=3072, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (3): BertLayer(
                (attention): BertAttention(
                  (self): BertSelfAttention(
                    (query): Linear(in_features=768, out_features=768, bias=True)
                    (key): Linear(in_features=768, out_features=768, bias=True)
                    (value): Linear(in_features=768, out_features=768, bias=True)
                    (dropout): Dropout(p=0.1)
                  )
                  (output): BertSelfOutput(
                    (dense): Linear(in_features=768, out_features=768, bias=True)
                    (LayerNorm): BertLayerNorm()
                    (dropout): Dropout(p=0.1)
                  )
                )
                (intermediate): BertIntermediate(
                  (dense): Linear(in_features=768, out_features=3072, bias=True)
                )
                (output): BertOutput(
                  (dense): Linear(in_features=3072, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (4): BertLayer(
                (attention): BertAttention(
                  (self): BertSelfAttention(
                    (query): Linear(in_features=768, out_features=768, bias=True)
                    (key): Linear(in_features=768, out_features=768, bias=True)
                    (value): Linear(in_features=768, out_features=768, bias=True)
                    (dropout): Dropout(p=0.1)
                  )
                  (output): BertSelfOutput(
                    (dense): Linear(in_features=768, out_features=768, bias=True)
                    (LayerNorm): BertLayerNorm()
                    (dropout): Dropout(p=0.1)
                  )
                )
                (intermediate): BertIntermediate(
                  (dense): Linear(in_features=768, out_features=3072, bias=True)
                )
                (output): BertOutput(
                  (dense): Linear(in_features=3072, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (5): BertLayer(
                (attention): BertAttention(
                  (self): BertSelfAttention(
                    (query): Linear(in_features=768, out_features=768, bias=True)
                    (key): Linear(in_features=768, out_features=768, bias=True)
                    (value): Linear(in_features=768, out_features=768, bias=True)
                    (dropout): Dropout(p=0.1)
                  )
                  (output): BertSelfOutput(
                    (dense): Linear(in_features=768, out_features=768, bias=True)
                    (LayerNorm): BertLayerNorm()
                    (dropout): Dropout(p=0.1)
                  )
                )
                (intermediate): BertIntermediate(
                  (dense): Linear(in_features=768, out_features=3072, bias=True)
                )
                (output): BertOutput(
                  (dense): Linear(in_features=3072, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (6): BertLayer(
                (attention): BertAttention(
                  (self): BertSelfAttention(
                    (query): Linear(in_features=768, out_features=768, bias=True)
                    (key): Linear(in_features=768, out_features=768, bias=True)
                    (value): Linear(in_features=768, out_features=768, bias=True)
                    (dropout): Dropout(p=0.1)
                  )
                  (output): BertSelfOutput(
                    (dense): Linear(in_features=768, out_features=768, bias=True)
                    (LayerNorm): BertLayerNorm()
                    (dropout): Dropout(p=0.1)
                  )
                )
                (intermediate): BertIntermediate(
                  (dense): Linear(in_features=768, out_features=3072, bias=True)
                )
                (output): BertOutput(
                  (dense): Linear(in_features=3072, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (7): BertLayer(
                (attention): BertAttention(
                  (self): BertSelfAttention(
                    (query): Linear(in_features=768, out_features=768, bias=True)
                    (key): Linear(in_features=768, out_features=768, bias=True)
                    (value): Linear(in_features=768, out_features=768, bias=True)
                    (dropout): Dropout(p=0.1)
                  )
                  (output): BertSelfOutput(
                    (dense): Linear(in_features=768, out_features=768, bias=True)
                    (LayerNorm): BertLayerNorm()
                    (dropout): Dropout(p=0.1)
                  )
                )
                (intermediate): BertIntermediate(
                  (dense): Linear(in_features=768, out_features=3072, bias=True)
                )
                (output): BertOutput(
                  (dense): Linear(in_features=3072, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (8): BertLayer(
                (attention): BertAttention(
                  (self): BertSelfAttention(
                    (query): Linear(in_features=768, out_features=768, bias=True)
                    (key): Linear(in_features=768, out_features=768, bias=True)
                    (value): Linear(in_features=768, out_features=768, bias=True)
                    (dropout): Dropout(p=0.1)
                  )
                  (output): BertSelfOutput(
                    (dense): Linear(in_features=768, out_features=768, bias=True)
                    (LayerNorm): BertLayerNorm()
                    (dropout): Dropout(p=0.1)
                  )
                )
                (intermediate): BertIntermediate(
                  (dense): Linear(in_features=768, out_features=3072, bias=True)
                )
                (output): BertOutput(
                  (dense): Linear(in_features=3072, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (9): BertLayer(
                (attention): BertAttention(
                  (self): BertSelfAttention(
                    (query): Linear(in_features=768, out_features=768, bias=True)
                    (key): Linear(in_features=768, out_features=768, bias=True)
                    (value): Linear(in_features=768, out_features=768, bias=True)
                    (dropout): Dropout(p=0.1)
                  )
                  (output): BertSelfOutput(
                    (dense): Linear(in_features=768, out_features=768, bias=True)
                    (LayerNorm): BertLayerNorm()
                    (dropout): Dropout(p=0.1)
                  )
                )
                (intermediate): BertIntermediate(
                  (dense): Linear(in_features=768, out_features=3072, bias=True)
                )
                (output): BertOutput(
                  (dense): Linear(in_features=3072, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (10): BertLayer(
                (attention): BertAttention(
                  (self): BertSelfAttention(
                    (query): Linear(in_features=768, out_features=768, bias=True)
                    (key): Linear(in_features=768, out_features=768, bias=True)
                    (value): Linear(in_features=768, out_features=768, bias=True)
                    (dropout): Dropout(p=0.1)
                  )
                  (output): BertSelfOutput(
                    (dense): Linear(in_features=768, out_features=768, bias=True)
                    (LayerNorm): BertLayerNorm()
                    (dropout): Dropout(p=0.1)
                  )
                )
                (intermediate): BertIntermediate(
                  (dense): Linear(in_features=768, out_features=3072, bias=True)
                )
                (output): BertOutput(
                  (dense): Linear(in_features=3072, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (11): BertLayer(
                (attention): BertAttention(
                  (self): BertSelfAttention(
                    (query): Linear(in_features=768, out_features=768, bias=True)
                    (key): Linear(in_features=768, out_features=768, bias=True)
                    (value): Linear(in_features=768, out_features=768, bias=True)
                    (dropout): Dropout(p=0.1)
                  )
                  (output): BertSelfOutput(
                    (dense): Linear(in_features=768, out_features=768, bias=True)
                    (LayerNorm): BertLayerNorm()
                    (dropout): Dropout(p=0.1)
                  )
                )
                (intermediate): BertIntermediate(
                  (dense): Linear(in_features=768, out_features=3072, bias=True)
                )
                (output): BertOutput(
                  (dense): Linear(in_features=3072, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
            )
          )
          (pooler): BertPooler(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (activation): Tanh()
          )
        )
      )
      (_highway_layer): TimeDistributed(
        (_module): Highway(
          (_layers): ModuleList()
        )
      )
      (_phrase_layer): NullPhraseLayer()
      (_dropout): Dropout(p=0.1)
    )
    (mnli_mdl): SingleClassifier(
      (pooler): Pooler()
      (classifier): Classifier(
        (classifier): Linear(in_features=768, out_features=3, bias=True)
      )
    )
    (copa_mdl): SingleClassifier(
      (pooler): Pooler()
      (classifier): Classifier(
        (classifier): Linear(in_features=768, out_features=1, bias=True)
      )
    )
  )
)
09/05 02:17:59 PM: Model parameters:
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Trainable parameter, count 22268928 with torch.Size([28996, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Trainable parameter, count 393216 with torch.Size([512, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.pooler.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:17:59 PM: 	module.sent_encoder._text_field_embedder.model.pooler.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:17:59 PM: 	module.mnli_mdl.classifier.classifier.weight: Trainable parameter, count 2304 with torch.Size([3, 768])
09/05 02:17:59 PM: 	module.mnli_mdl.classifier.classifier.bias: Trainable parameter, count 3 with torch.Size([3])
09/05 02:17:59 PM: 	module.copa_mdl.classifier.classifier.weight: Trainable parameter, count 768 with torch.Size([1, 768])
09/05 02:17:59 PM: 	module.copa_mdl.classifier.classifier.bias: Trainable parameter, count 1 with torch.Size([1])
09/05 02:17:59 PM: Total number of parameters: 108313348 (1.08313e+08)
09/05 02:17:59 PM: Number of trainable parameters: 108313348 (1.08313e+08)
09/05 02:17:59 PM: Finished building model in 6.912s
09/05 02:17:59 PM: Will run the following steps for this experiment:
Training model on tasks: mnli,copa 
Evaluating model on tasks: glue-diagnostic 

09/05 02:17:59 PM: Training...
09/05 02:17:59 PM: patience = 20
09/05 02:17:59 PM: val_interval = 1000
09/05 02:17:59 PM: max_vals = 10000
09/05 02:17:59 PM: cuda_device = [0]
09/05 02:17:59 PM: grad_norm = 5.0
09/05 02:17:59 PM: grad_clipping = None
09/05 02:17:59 PM: lr_decay = 0.99
09/05 02:17:59 PM: min_lr = 1e-07
09/05 02:17:59 PM: keep_all_checkpoints = 0
09/05 02:17:59 PM: val_data_limit = 5000
09/05 02:17:59 PM: max_epochs = 3
09/05 02:17:59 PM: dec_val_scale = 250
09/05 02:17:59 PM: training_data_fraction = 1
09/05 02:17:59 PM: use_cuda = 1
09/05 02:17:59 PM: type = bert_adam
09/05 02:17:59 PM: parameter_groups = None
09/05 02:17:59 PM: Number of trainable parameters: 108313348
09/05 02:17:59 PM: infer_type_and_cast = True
09/05 02:17:59 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/05 02:17:59 PM: CURRENTLY DEFINED PARAMETERS: 
09/05 02:17:59 PM: lr = 1e-5
09/05 02:17:59 PM: t_total = 1000
09/05 02:17:59 PM: warmup = 0.1
09/05 02:17:59 PM: type = reduce_on_plateau
09/05 02:17:59 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/05 02:17:59 PM: CURRENTLY DEFINED PARAMETERS: 
09/05 02:17:59 PM: mode = max
09/05 02:17:59 PM: factor = 0.5
09/05 02:17:59 PM: patience = 4
09/05 02:17:59 PM: threshold = 0.0001
09/05 02:17:59 PM: threshold_mode = abs
09/05 02:17:59 PM: verbose = True
09/05 02:17:59 PM: type = bert_adam
09/05 02:17:59 PM: parameter_groups = None
09/05 02:17:59 PM: Number of trainable parameters: 108313348
09/05 02:17:59 PM: infer_type_and_cast = True
09/05 02:17:59 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/05 02:17:59 PM: CURRENTLY DEFINED PARAMETERS: 
09/05 02:17:59 PM: lr = 1e-5
09/05 02:17:59 PM: t_total = 50000
09/05 02:17:59 PM: warmup = 0.1
09/05 02:17:59 PM: type = reduce_on_plateau
09/05 02:17:59 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/05 02:17:59 PM: CURRENTLY DEFINED PARAMETERS: 
09/05 02:17:59 PM: mode = max
09/05 02:17:59 PM: factor = 0.5
09/05 02:17:59 PM: patience = 4
09/05 02:17:59 PM: threshold = 0.0001
09/05 02:17:59 PM: threshold_mode = abs
09/05 02:17:59 PM: verbose = True
09/05 02:17:59 PM: type = bert_adam
09/05 02:17:59 PM: parameter_groups = None
09/05 02:17:59 PM: Number of trainable parameters: 108313348
09/05 02:17:59 PM: infer_type_and_cast = True
09/05 02:17:59 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/05 02:17:59 PM: CURRENTLY DEFINED PARAMETERS: 
09/05 02:17:59 PM: lr = 1e-5
09/05 02:17:59 PM: t_total = 50000
09/05 02:17:59 PM: warmup = 0.1
09/05 02:17:59 PM: type = reduce_on_plateau
09/05 02:17:59 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/05 02:17:59 PM: CURRENTLY DEFINED PARAMETERS: 
09/05 02:17:59 PM: mode = max
09/05 02:17:59 PM: factor = 0.5
09/05 02:17:59 PM: patience = 4
09/05 02:17:59 PM: threshold = 0.0001
09/05 02:17:59 PM: threshold_mode = abs
09/05 02:17:59 PM: verbose = True
09/05 02:17:59 PM: load_model=1 but there is not checkpoint.                         Starting training without restoring from a checkpoint.
09/05 02:17:59 PM: Training examples per task, before any subsampling: {'copa': 400, 'mnli': 392702}
09/05 02:17:59 PM: Using weighting method: proportional, with normalized sample weights [0.001 0.999] 
09/05 02:17:59 PM: Beginning training with stopping criteria based on metric: macro_avg
09/05 02:18:09 PM: Update 31: task mnli, batch 31 (31): accuracy: 0.3003, mnli_loss: 1.1248
09/05 02:18:19 PM: Update 71: task mnli, batch 71 (71): accuracy: 0.3066, mnli_loss: 1.1230
09/05 02:18:29 PM: Update 111: task mnli, batch 111 (111): accuracy: 0.3227, mnli_loss: 1.1165
09/05 02:18:32 PM: Fatal error in main():
Traceback (most recent call last):
  File "main.py", line 16, in <module>
    main(sys.argv[1:])
  File "/beegfs/yp913/jiant_cleanup/jiant/__main__.py", line 533, in main
    phase="pretrain",
  File "/beegfs/yp913/jiant_cleanup/jiant/trainer.py", line 604, in train
    output_dict = self._forward(batch, task=task)
  File "/beegfs/yp913/jiant_cleanup/jiant/trainer.py", line 1050, in _forward
    model_out = self._model.forward(task, tensor_batch)
  File "/beegfs/yp913/anaconda3/envs/jiant/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 150, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/beegfs/yp913/anaconda3/envs/jiant/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/beegfs/yp913/jiant_cleanup/jiant/models.py", line 850, in forward
    out = self._mc_forward(batch, task, predict)
  File "/beegfs/yp913/jiant_cleanup/jiant/models.py", line 1160, in _mc_forward
    torch.tensor(get_batch_size(batch, keyword="choice0")), self._cuda_device
  File "/beegfs/yp913/anaconda3/envs/jiant/lib/python3.6/site-packages/allennlp/nn/util.py", line 41, in move_to_device
    if cuda_device < 0 or not has_tensor(obj):
TypeError: '<' not supported between instances of 'list' and 'int'
09/05 02:20:20 PM: Git branch: master
09/05 02:20:20 PM: Git SHA: 883e7176a66d891d9d0238a6a08338d8f200af17
09/05 02:20:20 PM: Parsed args: 
{
  "batch_size": 24,
  "classifier": "log_reg",
  "do_target_task_training": 0,
  "dropout": 0.1,
  "dropout_embs": 0.1,
  "input_module": "bert-base-cased",
  "local_log_path": "diagnostic_run_2/my-experiment/mnli_copa_diagnostic/log.log",
  "lr": "1e-5",
  "lr_patience": 4,
  "max_epochs": 3,
  "max_vals": 10000,
  "min_lr": 0.0,
  "optimizer": "bert_adam",
  "patience": 20,
  "pretrain_tasks": "mnli,copa",
  "pytorch_transformers_output_mode": "top",
  "random_seed": 42,
  "remote_log_name": "my-experiment__mnli_copa_diagnostic",
  "run_dir": "diagnostic_run_2/my-experiment/mnli_copa_diagnostic",
  "run_name": "mnli_copa_diagnostic",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "glue-diagnostic",
  "transfer_paradigm": "finetune",
  "write_preds": "val,test"
}
09/05 02:20:20 PM: Saved config to diagnostic_run_2/my-experiment/mnli_copa_diagnostic/params.conf
09/05 02:20:20 PM: Using random seed 42
09/05 02:20:20 PM: Using GPU 0
09/05 02:20:20 PM: Loading tasks...
09/05 02:20:20 PM: Writing pre-preprocessed tasks to diagnostic_run_2/my-experiment/
09/05 02:20:20 PM: 	Loaded existing task copa
09/05 02:20:20 PM: 	Task 'copa': |train|=400 |val|=100 |test|=500
09/05 02:20:20 PM: 	Loaded existing task glue-diagnostic
09/05 02:20:20 PM: 	Task 'glue-diagnostic': |train|=1104 |val|=1104 |test|=1104
09/05 02:20:24 PM: 	Loaded existing task mnli
09/05 02:20:24 PM: 	Task 'mnli': |train|=392702 |val|=19647 |test|=19643
09/05 02:20:24 PM: 	Finished loading tasks: copa glue-diagnostic mnli.
09/05 02:20:24 PM: Loading token dictionary from diagnostic_run_2/my-experiment/vocab.
09/05 02:20:24 PM: 	Loaded vocab from diagnostic_run_2/my-experiment/vocab
09/05 02:20:24 PM: 	Vocab namespace bert_cased: size 28998
09/05 02:20:24 PM: 	Vocab namespace tokens: size 25700
09/05 02:20:24 PM: 	Vocab namespace chars: size 139
09/05 02:20:24 PM: 	Finished building vocab.
09/05 02:20:24 PM: 	Task 'copa', split 'train': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/copa__train_data
09/05 02:20:24 PM: 	Task 'copa', split 'val': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/copa__val_data
09/05 02:20:24 PM: 	Task 'copa', split 'test': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/copa__test_data
09/05 02:20:24 PM: 	Task 'glue-diagnostic', split 'train': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/glue-diagnostic__train_data
09/05 02:20:24 PM: 	Task 'glue-diagnostic', split 'val': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/glue-diagnostic__val_data
09/05 02:20:24 PM: 	Task 'glue-diagnostic', split 'test': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/glue-diagnostic__test_data
09/05 02:20:24 PM: 	Task 'mnli', split 'train': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/mnli__train_data
09/05 02:20:24 PM: 	Task 'mnli', split 'val': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/mnli__val_data
09/05 02:20:24 PM: 	Task 'mnli', split 'test': Found preprocessed copy in diagnostic_run_2/my-experiment/preproc/mnli__test_data
09/05 02:20:24 PM: 	Finished indexing tasks
09/05 02:20:24 PM: 	Creating trimmed pretraining-only version of copa train.
09/05 02:20:24 PM: 	Creating trimmed target-only version of glue-diagnostic train.
09/05 02:20:24 PM: 	Creating trimmed pretraining-only version of mnli train.
09/05 02:20:24 PM: 	  Training on mnli, copa
09/05 02:20:24 PM: 	  Evaluating on glue-diagnostic
09/05 02:20:24 PM: 	Finished loading tasks in 3.603s
09/05 02:20:24 PM: 	 Tasks: ['copa', 'glue-diagnostic', 'mnli']
09/05 02:20:24 PM: Building model...
09/05 02:20:24 PM: Using BERT model (bert-base-cased).
09/05 02:20:24 PM: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at diagnostic_run_2/my-experiment/pytorch_transformers_cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.d7a3af18ce3a2ab7c0f48f04dc8daff45ed9a3ed333b9e9a79d012a0dedf87a6
09/05 02:20:24 PM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

09/05 02:20:24 PM: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at diagnostic_run_2/my-experiment/pytorch_transformers_cache/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
09/05 02:20:27 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at diagnostic_run_2/my-experiment/pytorch_transformers_cache/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
09/05 02:20:27 PM: Initializing parameters
09/05 02:20:27 PM: Done initializing parameters; the following parameters are using their default initialization from their code
09/05 02:20:27 PM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/05 02:20:27 PM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/05 02:20:27 PM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/05 02:20:27 PM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/05 02:20:27 PM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/05 02:20:27 PM:    _text_field_embedder.model.pooler.dense.bias
09/05 02:20:27 PM:    _text_field_embedder.model.pooler.dense.weight
09/05 02:20:27 PM: 	Task 'copa' params: {
  "cls_type": "log_reg",
  "d_hid": 512,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 1,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "copa"
}
09/05 02:20:27 PM: 	Task 'mnli' params: {
  "cls_type": "log_reg",
  "d_hid": 512,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 1,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "mnli"
}
09/05 02:20:27 PM: 	Task 'glue-diagnostic' params: {
  "cls_type": "log_reg",
  "d_hid": 512,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 1,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "mnli"
}
09/05 02:20:27 PM: batch_first = True
09/05 02:20:27 PM: stateful = False
09/05 02:20:27 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/05 02:20:27 PM: CURRENTLY DEFINED PARAMETERS: 
09/05 02:20:27 PM: input_size = 1536
09/05 02:20:27 PM: hidden_size = 512
09/05 02:20:27 PM: num_layers = 1
09/05 02:20:27 PM: bidirectional = True
09/05 02:20:27 PM: batch_first = True
09/05 02:20:27 PM: Initializing parameters
09/05 02:20:27 PM: Done initializing parameters; the following parameters are using their default initialization from their code
09/05 02:20:27 PM:    _modeling_layer._module.bias_hh_l0
09/05 02:20:27 PM:    _modeling_layer._module.bias_hh_l0_reverse
09/05 02:20:27 PM:    _modeling_layer._module.bias_ih_l0
09/05 02:20:27 PM:    _modeling_layer._module.bias_ih_l0_reverse
09/05 02:20:27 PM:    _modeling_layer._module.weight_hh_l0
09/05 02:20:27 PM:    _modeling_layer._module.weight_hh_l0_reverse
09/05 02:20:27 PM:    _modeling_layer._module.weight_ih_l0
09/05 02:20:27 PM:    _modeling_layer._module.weight_ih_l0_reverse
09/05 02:20:27 PM: Name of the task is different than the classifier it should use
09/05 02:20:31 PM: Model specification:
09/05 02:20:31 PM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.1)
  )
  (copa_mdl): SingleClassifier(
    (pooler): Pooler()
    (classifier): Classifier(
      (classifier): Linear(in_features=768, out_features=1, bias=True)
    )
  )
  (mnli_mdl): SingleClassifier(
    (pooler): Pooler()
    (classifier): Classifier(
      (classifier): Linear(in_features=768, out_features=3, bias=True)
    )
  )
)
09/05 02:20:31 PM: Model parameters:
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Trainable parameter, count 22268928 with torch.Size([28996, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Trainable parameter, count 393216 with torch.Size([512, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
09/05 02:20:31 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Trainable parameter, count 768 with torch.Size([768])
09/05 02:20:31 PM: 	copa_mdl.classifier.classifier.weight: Trainable parameter, count 768 with torch.Size([1, 768])
09/05 02:20:31 PM: 	copa_mdl.classifier.classifier.bias: Trainable parameter, count 1 with torch.Size([1])
09/05 02:20:31 PM: 	mnli_mdl.classifier.classifier.weight: Trainable parameter, count 2304 with torch.Size([3, 768])
09/05 02:20:31 PM: 	mnli_mdl.classifier.classifier.bias: Trainable parameter, count 3 with torch.Size([3])
09/05 02:20:31 PM: Total number of parameters: 108313348 (1.08313e+08)
09/05 02:20:31 PM: Number of trainable parameters: 108313348 (1.08313e+08)
09/05 02:20:31 PM: Finished building model in 7.104s
09/05 02:20:31 PM: Will run the following steps for this experiment:
Training model on tasks: mnli,copa 
Evaluating model on tasks: glue-diagnostic 

09/05 02:20:31 PM: Training...
09/05 02:20:31 PM: patience = 20
09/05 02:20:31 PM: val_interval = 1000
09/05 02:20:31 PM: max_vals = 10000
09/05 02:20:31 PM: cuda_device = 0
09/05 02:20:31 PM: grad_norm = 5.0
09/05 02:20:31 PM: grad_clipping = None
09/05 02:20:31 PM: lr_decay = 0.99
09/05 02:20:31 PM: min_lr = 1e-07
09/05 02:20:31 PM: keep_all_checkpoints = 0
09/05 02:20:31 PM: val_data_limit = 5000
09/05 02:20:31 PM: max_epochs = 3
09/05 02:20:31 PM: dec_val_scale = 250
09/05 02:20:31 PM: training_data_fraction = 1
09/05 02:20:31 PM: type = bert_adam
09/05 02:20:31 PM: parameter_groups = None
09/05 02:20:31 PM: Number of trainable parameters: 108313348
09/05 02:20:31 PM: infer_type_and_cast = True
09/05 02:20:31 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/05 02:20:31 PM: CURRENTLY DEFINED PARAMETERS: 
09/05 02:20:31 PM: lr = 1e-5
09/05 02:20:31 PM: t_total = 1000
09/05 02:20:31 PM: warmup = 0.1
09/05 02:20:31 PM: type = reduce_on_plateau
09/05 02:20:31 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/05 02:20:31 PM: CURRENTLY DEFINED PARAMETERS: 
09/05 02:20:31 PM: mode = max
09/05 02:20:31 PM: factor = 0.5
09/05 02:20:31 PM: patience = 4
09/05 02:20:31 PM: threshold = 0.0001
09/05 02:20:31 PM: threshold_mode = abs
09/05 02:20:31 PM: verbose = True
09/05 02:20:31 PM: type = bert_adam
09/05 02:20:31 PM: parameter_groups = None
09/05 02:20:31 PM: Number of trainable parameters: 108313348
09/05 02:20:31 PM: infer_type_and_cast = True
09/05 02:20:31 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/05 02:20:31 PM: CURRENTLY DEFINED PARAMETERS: 
09/05 02:20:31 PM: lr = 1e-5
09/05 02:20:31 PM: t_total = 50000
09/05 02:20:31 PM: warmup = 0.1
09/05 02:20:31 PM: type = reduce_on_plateau
09/05 02:20:31 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/05 02:20:31 PM: CURRENTLY DEFINED PARAMETERS: 
09/05 02:20:31 PM: mode = max
09/05 02:20:31 PM: factor = 0.5
09/05 02:20:31 PM: patience = 4
09/05 02:20:31 PM: threshold = 0.0001
09/05 02:20:31 PM: threshold_mode = abs
09/05 02:20:31 PM: verbose = True
09/05 02:20:31 PM: type = bert_adam
09/05 02:20:31 PM: parameter_groups = None
09/05 02:20:31 PM: Number of trainable parameters: 108313348
09/05 02:20:31 PM: infer_type_and_cast = True
09/05 02:20:31 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/05 02:20:31 PM: CURRENTLY DEFINED PARAMETERS: 
09/05 02:20:31 PM: lr = 1e-5
09/05 02:20:31 PM: t_total = 50000
09/05 02:20:31 PM: warmup = 0.1
09/05 02:20:31 PM: type = reduce_on_plateau
09/05 02:20:31 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/05 02:20:31 PM: CURRENTLY DEFINED PARAMETERS: 
09/05 02:20:31 PM: mode = max
09/05 02:20:31 PM: factor = 0.5
09/05 02:20:31 PM: patience = 4
09/05 02:20:31 PM: threshold = 0.0001
09/05 02:20:31 PM: threshold_mode = abs
09/05 02:20:31 PM: verbose = True
09/05 02:20:31 PM: load_model=1 but there is not checkpoint.                         Starting training without restoring from a checkpoint.
09/05 02:20:31 PM: Training examples per task, before any subsampling: {'copa': 400, 'mnli': 392702}
09/05 02:20:31 PM: Using weighting method: proportional, with normalized sample weights [0.001 0.999] 
09/05 02:20:31 PM: Beginning training with stopping criteria based on metric: macro_avg
09/05 02:20:41 PM: Update 31: task mnli, batch 31 (31): accuracy: 0.3954, mnli_loss: 1.1452
09/05 02:20:51 PM: Update 71: task mnli, batch 71 (71): accuracy: 0.3750, mnli_loss: 1.1473
09/05 02:21:01 PM: Update 111: task mnli, batch 111 (111): accuracy: 0.3656, mnli_loss: 1.1440
09/05 02:21:05 PM: Update 125: task copa, batch 1 (1): accuracy: 0.6250, copa_loss: 0.6671
09/05 02:21:11 PM: Update 151: task mnli, batch 150 (150): accuracy: 0.3705, mnli_loss: 1.1390
09/05 02:21:22 PM: Update 188: task mnli, batch 187 (187): accuracy: 0.3725, mnli_loss: 1.1337
09/05 02:21:32 PM: Update 228: task mnli, batch 227 (227): accuracy: 0.3735, mnli_loss: 1.1280
09/05 02:21:42 PM: Update 267: task mnli, batch 266 (266): accuracy: 0.3755, mnli_loss: 1.1235
09/05 02:21:52 PM: Update 306: task mnli, batch 305 (305): accuracy: 0.3792, mnli_loss: 1.1183
09/05 02:22:02 PM: Update 345: task mnli, batch 344 (344): accuracy: 0.3808, mnli_loss: 1.1161
09/05 02:22:12 PM: Update 385: task mnli, batch 384 (384): accuracy: 0.3827, mnli_loss: 1.1129
09/05 02:22:15 PM: Update 395: task copa, batch 2 (2): accuracy: 0.5750, copa_loss: 0.6832
09/05 02:22:23 PM: Update 420: task mnli, batch 418 (418): accuracy: 0.3856, mnli_loss: 1.1093
09/05 02:22:33 PM: Update 458: task mnli, batch 456 (456): accuracy: 0.3867, mnli_loss: 1.1062
09/05 02:22:44 PM: Update 498: task mnli, batch 496 (496): accuracy: 0.3903, mnli_loss: 1.1026
09/05 02:22:54 PM: Update 536: task mnli, batch 534 (534): accuracy: 0.3934, mnli_loss: 1.0991
09/05 02:23:04 PM: Update 574: task mnli, batch 572 (572): accuracy: 0.3986, mnli_loss: 1.0953
09/05 02:23:14 PM: Update 612: task mnli, batch 610 (610): accuracy: 0.4008, mnli_loss: 1.0923
09/05 02:23:24 PM: Update 653: task mnli, batch 651 (651): accuracy: 0.4049, mnli_loss: 1.0888
09/05 02:23:34 PM: Update 695: task mnli, batch 693 (693): accuracy: 0.4118, mnli_loss: 1.0831
09/05 02:23:44 PM: Update 734: task mnli, batch 732 (732): accuracy: 0.4174, mnli_loss: 1.0788
09/05 02:23:54 PM: Update 772: task mnli, batch 770 (770): accuracy: 0.4235, mnli_loss: 1.0730
09/05 02:24:05 PM: Update 808: task mnli, batch 806 (806): accuracy: 0.4295, mnli_loss: 1.0673
09/05 02:24:15 PM: Update 839: task mnli, batch 837 (837): accuracy: 0.4354, mnli_loss: 1.0615
09/05 02:24:25 PM: Update 878: task mnli, batch 876 (876): accuracy: 0.4433, mnli_loss: 1.0537
09/05 02:24:35 PM: Update 916: task mnli, batch 914 (914): accuracy: 0.4480, mnli_loss: 1.0482
09/05 02:24:45 PM: Update 954: task mnli, batch 952 (952): accuracy: 0.4540, mnli_loss: 1.0417
09/05 02:24:55 PM: Update 991: task mnli, batch 989 (989): accuracy: 0.4591, mnli_loss: 1.0360
09/05 02:24:57 PM: ***** Step 1000 / Validation 1 *****
09/05 02:24:57 PM: copa: trained on 2 batches, 0.118 epochs
09/05 02:24:57 PM: mnli: trained on 998 batches, 0.061 epochs
09/05 02:24:57 PM: Validating...
09/05 02:24:58 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.5417, copa_loss: 0.6839
09/05 02:25:05 PM: Evaluate: task mnli, batch 89 (209): accuracy: 0.6081, mnli_loss: 0.8546
09/05 02:25:14 PM: Best result seen so far for copa.
09/05 02:25:14 PM: Best result seen so far for mnli.
09/05 02:25:14 PM: Best result seen so far for micro.
09/05 02:25:14 PM: Best result seen so far for macro.
09/05 02:25:14 PM: Updating LR scheduler:
09/05 02:25:14 PM: 	Best result seen so far for macro_avg: 0.544
09/05 02:25:14 PM: 	# validation passes without improvement: 0
09/05 02:25:14 PM: copa_loss: training: 0.683199 validation: 0.695575
09/05 02:25:14 PM: mnli_loss: training: 1.034416 validation: 0.867364
09/05 02:25:14 PM: macro_avg: validation: 0.544400
09/05 02:25:14 PM: micro_avg: validation: 0.596667
09/05 02:25:14 PM: copa_accuracy: training: 0.575000 validation: 0.490000
09/05 02:25:14 PM: mnli_accuracy: training: 0.460507 validation: 0.598800
09/05 02:25:14 PM: Global learning rate: 1e-05
09/05 02:25:14 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 02:25:15 PM: Update 1002: task mnli, batch 2 (1000): accuracy: 0.5833, mnli_loss: 0.9208
09/05 02:25:25 PM: Update 1042: task mnli, batch 42 (1040): accuracy: 0.5933, mnli_loss: 0.9110
09/05 02:25:36 PM: Update 1081: task mnli, batch 81 (1079): accuracy: 0.6070, mnli_loss: 0.8711
09/05 02:25:46 PM: Update 1121: task mnli, batch 121 (1119): accuracy: 0.5995, mnli_loss: 0.8719
09/05 02:25:56 PM: Update 1160: task mnli, batch 160 (1158): accuracy: 0.6036, mnli_loss: 0.8645
09/05 02:26:06 PM: Update 1199: task mnli, batch 199 (1197): accuracy: 0.6099, mnli_loss: 0.8570
09/05 02:26:12 PM: Update 1222: task copa, batch 1 (3): accuracy: 0.5417, copa_loss: 0.7184
09/05 02:26:16 PM: Update 1237: task mnli, batch 236 (1234): accuracy: 0.6079, mnli_loss: 0.8603
09/05 02:26:27 PM: Update 1268: task mnli, batch 267 (1265): accuracy: 0.6069, mnli_loss: 0.8604
09/05 02:26:37 PM: Update 1306: task mnli, batch 305 (1303): accuracy: 0.6102, mnli_loss: 0.8572
09/05 02:26:47 PM: Update 1343: task mnli, batch 342 (1340): accuracy: 0.6116, mnli_loss: 0.8525
09/05 02:26:57 PM: Update 1381: task mnli, batch 380 (1378): accuracy: 0.6136, mnli_loss: 0.8513
09/05 02:27:07 PM: Update 1420: task mnli, batch 419 (1417): accuracy: 0.6183, mnli_loss: 0.8446
09/05 02:27:17 PM: Update 1459: task mnli, batch 458 (1456): accuracy: 0.6217, mnli_loss: 0.8377
09/05 02:27:28 PM: Update 1498: task mnli, batch 497 (1495): accuracy: 0.6240, mnli_loss: 0.8346
09/05 02:27:38 PM: Update 1538: task mnli, batch 537 (1535): accuracy: 0.6286, mnli_loss: 0.8278
09/05 02:27:48 PM: Update 1578: task mnli, batch 577 (1575): accuracy: 0.6303, mnli_loss: 0.8243
09/05 02:27:58 PM: Update 1616: task mnli, batch 615 (1613): accuracy: 0.6333, mnli_loss: 0.8211
09/05 02:28:08 PM: Update 1655: task mnli, batch 654 (1652): accuracy: 0.6363, mnli_loss: 0.8174
09/05 02:28:18 PM: Update 1686: task mnli, batch 685 (1683): accuracy: 0.6372, mnli_loss: 0.8165
09/05 02:28:29 PM: Update 1726: task mnli, batch 725 (1723): accuracy: 0.6387, mnli_loss: 0.8132
09/05 02:28:39 PM: Update 1765: task mnli, batch 764 (1762): accuracy: 0.6410, mnli_loss: 0.8108
09/05 02:28:49 PM: Update 1803: task mnli, batch 802 (1800): accuracy: 0.6437, mnli_loss: 0.8076
09/05 02:28:59 PM: Update 1840: task mnli, batch 839 (1837): accuracy: 0.6461, mnli_loss: 0.8045
09/05 02:29:09 PM: Update 1878: task mnli, batch 877 (1875): accuracy: 0.6467, mnli_loss: 0.8037
09/05 02:29:19 PM: Update 1918: task mnli, batch 917 (1915): accuracy: 0.6479, mnli_loss: 0.8017
09/05 02:29:29 PM: Update 1957: task mnli, batch 956 (1954): accuracy: 0.6480, mnli_loss: 0.8017
09/05 02:29:40 PM: Update 1995: task mnli, batch 994 (1992): accuracy: 0.6490, mnli_loss: 0.8005
09/05 02:29:41 PM: ***** Step 2000 / Validation 2 *****
09/05 02:29:41 PM: copa: trained on 1 batches, 0.059 epochs
09/05 02:29:41 PM: mnli: trained on 999 batches, 0.061 epochs
09/05 02:29:41 PM: Validating...
09/05 02:29:41 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.4583, copa_loss: 0.6888
09/05 02:29:50 PM: Evaluate: task mnli, batch 112 (209): accuracy: 0.6923, mnli_loss: 0.7097
09/05 02:29:57 PM: Best result seen so far for mnli.
09/05 02:29:57 PM: Best result seen so far for micro.
09/05 02:29:57 PM: Best result seen so far for macro.
09/05 02:29:57 PM: Updating LR scheduler:
09/05 02:29:57 PM: 	Best result seen so far for macro_avg: 0.569
09/05 02:29:57 PM: 	# validation passes without improvement: 0
09/05 02:29:57 PM: copa_loss: training: 0.718394 validation: 0.695329
09/05 02:29:57 PM: mnli_loss: training: 0.799986 validation: 0.705603
09/05 02:29:57 PM: macro_avg: validation: 0.568700
09/05 02:29:57 PM: micro_avg: validation: 0.692353
09/05 02:29:57 PM: copa_accuracy: training: 0.541667 validation: 0.440000
09/05 02:29:57 PM: mnli_accuracy: training: 0.649290 validation: 0.697400
09/05 02:29:57 PM: Global learning rate: 1e-05
09/05 02:29:57 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 02:30:00 PM: Update 2008: task mnli, batch 8 (2005): accuracy: 0.7552, mnli_loss: 0.6235
09/05 02:30:10 PM: Update 2046: task mnli, batch 46 (2043): accuracy: 0.6966, mnli_loss: 0.7159
09/05 02:30:20 PM: Update 2085: task mnli, batch 85 (2082): accuracy: 0.6990, mnli_loss: 0.7150
09/05 02:30:30 PM: Update 2114: task mnli, batch 114 (2111): accuracy: 0.6939, mnli_loss: 0.7234
09/05 02:30:40 PM: Update 2152: task mnli, batch 152 (2149): accuracy: 0.6997, mnli_loss: 0.7116
09/05 02:30:50 PM: Update 2191: task mnli, batch 191 (2188): accuracy: 0.7002, mnli_loss: 0.7093
09/05 02:31:00 PM: Update 2229: task mnli, batch 229 (2226): accuracy: 0.6986, mnli_loss: 0.7132
09/05 02:31:10 PM: Update 2268: task mnli, batch 268 (2265): accuracy: 0.6963, mnli_loss: 0.7156
09/05 02:31:21 PM: Update 2306: task mnli, batch 306 (2303): accuracy: 0.6968, mnli_loss: 0.7151
09/05 02:31:31 PM: Update 2346: task mnli, batch 346 (2343): accuracy: 0.6985, mnli_loss: 0.7119
09/05 02:31:41 PM: Update 2386: task mnli, batch 386 (2383): accuracy: 0.6994, mnli_loss: 0.7116
09/05 02:31:51 PM: Update 2424: task mnli, batch 424 (2421): accuracy: 0.7001, mnli_loss: 0.7089
09/05 02:32:01 PM: Update 2462: task mnli, batch 462 (2459): accuracy: 0.7017, mnli_loss: 0.7052
09/05 02:32:11 PM: Update 2501: task mnli, batch 501 (2498): accuracy: 0.7028, mnli_loss: 0.7048
09/05 02:32:21 PM: Update 2529: task mnli, batch 529 (2526): accuracy: 0.7039, mnli_loss: 0.7039
09/05 02:32:32 PM: Update 2567: task mnli, batch 567 (2564): accuracy: 0.7062, mnli_loss: 0.7012
09/05 02:32:42 PM: Update 2606: task mnli, batch 606 (2603): accuracy: 0.7071, mnli_loss: 0.7007
09/05 02:32:52 PM: Update 2645: task mnli, batch 645 (2642): accuracy: 0.7057, mnli_loss: 0.7017
09/05 02:33:02 PM: Update 2682: task mnli, batch 682 (2679): accuracy: 0.7052, mnli_loss: 0.7027
09/05 02:33:12 PM: Update 2721: task mnli, batch 721 (2718): accuracy: 0.7047, mnli_loss: 0.7035
09/05 02:33:22 PM: Update 2760: task mnli, batch 760 (2757): accuracy: 0.7042, mnli_loss: 0.7032
09/05 02:33:32 PM: Update 2799: task mnli, batch 799 (2796): accuracy: 0.7044, mnli_loss: 0.7032
09/05 02:33:42 PM: Update 2837: task mnli, batch 837 (2834): accuracy: 0.7046, mnli_loss: 0.7035
09/05 02:33:52 PM: Update 2876: task mnli, batch 876 (2873): accuracy: 0.7048, mnli_loss: 0.7026
09/05 02:34:03 PM: Update 2913: task mnli, batch 913 (2910): accuracy: 0.7051, mnli_loss: 0.7018
09/05 02:34:13 PM: Update 2943: task mnli, batch 943 (2940): accuracy: 0.7057, mnli_loss: 0.7018
09/05 02:34:23 PM: Update 2980: task mnli, batch 980 (2977): accuracy: 0.7059, mnli_loss: 0.7004
09/05 02:34:28 PM: ***** Step 3000 / Validation 3 *****
09/05 02:34:28 PM: copa: trained on 0 batches, 0.000 epochs
09/05 02:34:28 PM: mnli: trained on 1000 batches, 0.061 epochs
09/05 02:34:28 PM: Validating...
09/05 02:34:28 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.4167, copa_loss: 0.6988
09/05 02:34:33 PM: Evaluate: task mnli, batch 57 (209): accuracy: 0.7376, mnli_loss: 0.6415
09/05 02:34:43 PM: Evaluate: task mnli, batch 193 (209): accuracy: 0.7366, mnli_loss: 0.6486
09/05 02:34:44 PM: Best result seen so far for mnli.
09/05 02:34:44 PM: Best result seen so far for micro.
09/05 02:34:44 PM: Updating LR scheduler:
09/05 02:34:44 PM: 	Best result seen so far for macro_avg: 0.569
09/05 02:34:44 PM: 	# validation passes without improvement: 1
09/05 02:34:44 PM: copa_loss: training: 0.000000 validation: 0.711416
09/05 02:34:44 PM: mnli_loss: training: 0.699661 validation: 0.650954
09/05 02:34:44 PM: macro_avg: validation: 0.567000
09/05 02:34:44 PM: micro_avg: validation: 0.727451
09/05 02:34:44 PM: copa_accuracy: validation: 0.400000
09/05 02:34:44 PM: mnli_accuracy: training: 0.706373 validation: 0.734000
09/05 02:34:44 PM: Global learning rate: 1e-05
09/05 02:34:44 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 02:34:53 PM: Update 3033: task mnli, batch 33 (3030): accuracy: 0.6818, mnli_loss: 0.7388
09/05 02:35:03 PM: Update 3072: task mnli, batch 72 (3069): accuracy: 0.6933, mnli_loss: 0.7188
09/05 02:35:13 PM: Update 3111: task mnli, batch 111 (3108): accuracy: 0.7046, mnli_loss: 0.7021
09/05 02:35:23 PM: Update 3148: task mnli, batch 148 (3145): accuracy: 0.7083, mnli_loss: 0.6888
09/05 02:35:34 PM: Update 3188: task mnli, batch 188 (3185): accuracy: 0.7077, mnli_loss: 0.6851
09/05 02:35:44 PM: Update 3226: task mnli, batch 226 (3223): accuracy: 0.7111, mnli_loss: 0.6813
09/05 02:35:54 PM: Update 3264: task mnli, batch 264 (3261): accuracy: 0.7156, mnli_loss: 0.6755
09/05 02:36:04 PM: Update 3302: task mnli, batch 302 (3299): accuracy: 0.7169, mnli_loss: 0.6733
09/05 02:36:16 PM: Update 3340: task mnli, batch 340 (3337): accuracy: 0.7161, mnli_loss: 0.6752
09/05 02:36:26 PM: Update 3379: task mnli, batch 379 (3376): accuracy: 0.7185, mnli_loss: 0.6721
09/05 02:36:36 PM: Update 3418: task mnli, batch 418 (3415): accuracy: 0.7199, mnli_loss: 0.6699
09/05 02:36:46 PM: Update 3455: task mnli, batch 455 (3452): accuracy: 0.7216, mnli_loss: 0.6706
09/05 02:36:56 PM: Update 3493: task mnli, batch 493 (3490): accuracy: 0.7215, mnli_loss: 0.6723
09/05 02:37:06 PM: Update 3532: task mnli, batch 532 (3529): accuracy: 0.7230, mnli_loss: 0.6689
09/05 02:37:16 PM: Update 3571: task mnli, batch 571 (3568): accuracy: 0.7233, mnli_loss: 0.6684
09/05 02:37:27 PM: Update 3610: task mnli, batch 610 (3607): accuracy: 0.7238, mnli_loss: 0.6672
09/05 02:37:37 PM: Update 3650: task mnli, batch 650 (3647): accuracy: 0.7251, mnli_loss: 0.6676
09/05 02:37:47 PM: Update 3687: task mnli, batch 687 (3684): accuracy: 0.7257, mnli_loss: 0.6663
09/05 02:37:57 PM: Update 3725: task mnli, batch 725 (3722): accuracy: 0.7252, mnli_loss: 0.6668
09/05 02:38:07 PM: Update 3757: task mnli, batch 757 (3754): accuracy: 0.7257, mnli_loss: 0.6654
09/05 02:38:18 PM: Update 3795: task mnli, batch 795 (3792): accuracy: 0.7260, mnli_loss: 0.6642
09/05 02:38:28 PM: Update 3832: task mnli, batch 832 (3829): accuracy: 0.7258, mnli_loss: 0.6638
09/05 02:38:38 PM: Update 3871: task mnli, batch 871 (3868): accuracy: 0.7263, mnli_loss: 0.6629
09/05 02:38:48 PM: Update 3910: task mnli, batch 910 (3907): accuracy: 0.7284, mnli_loss: 0.6602
09/05 02:38:59 PM: Update 3949: task mnli, batch 949 (3946): accuracy: 0.7297, mnli_loss: 0.6583
09/05 02:39:09 PM: Update 3990: task mnli, batch 990 (3987): accuracy: 0.7313, mnli_loss: 0.6548
09/05 02:39:11 PM: ***** Step 4000 / Validation 4 *****
09/05 02:39:11 PM: copa: trained on 0 batches, 0.000 epochs
09/05 02:39:11 PM: mnli: trained on 1000 batches, 0.061 epochs
09/05 02:39:11 PM: Validating...
09/05 02:39:11 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.3333, copa_loss: 0.7129
09/05 02:39:19 PM: Evaluate: task mnli, batch 96 (209): accuracy: 0.7383, mnli_loss: 0.6259
09/05 02:39:28 PM: Best result seen so far for mnli.
09/05 02:39:28 PM: Best result seen so far for micro.
09/05 02:39:28 PM: Best result seen so far for macro.
09/05 02:39:28 PM: Updating LR scheduler:
09/05 02:39:28 PM: 	Best result seen so far for macro_avg: 0.582
09/05 02:39:28 PM: 	# validation passes without improvement: 0
09/05 02:39:28 PM: copa_loss: training: 0.000000 validation: 0.707132
09/05 02:39:28 PM: mnli_loss: training: 0.654586 validation: 0.615893
09/05 02:39:28 PM: macro_avg: validation: 0.582300
09/05 02:39:28 PM: micro_avg: validation: 0.738235
09/05 02:39:28 PM: copa_accuracy: validation: 0.420000
09/05 02:39:28 PM: mnli_accuracy: training: 0.731571 validation: 0.744600
09/05 02:39:28 PM: Global learning rate: 1e-05
09/05 02:39:28 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 02:39:29 PM: Update 4001: task mnli, batch 1 (3998): accuracy: 0.6250, mnli_loss: 0.6387
09/05 02:39:39 PM: Update 4038: task mnli, batch 38 (4035): accuracy: 0.7204, mnli_loss: 0.6694
09/05 02:39:49 PM: Update 4075: task mnli, batch 75 (4072): accuracy: 0.7228, mnli_loss: 0.6603
09/05 02:39:59 PM: Update 4114: task mnli, batch 114 (4111): accuracy: 0.7248, mnli_loss: 0.6513
09/05 02:40:09 PM: Update 4153: task mnli, batch 153 (4150): accuracy: 0.7312, mnli_loss: 0.6412
09/05 02:40:19 PM: Update 4184: task mnli, batch 184 (4181): accuracy: 0.7300, mnli_loss: 0.6415
09/05 02:40:30 PM: Update 4223: task mnli, batch 223 (4220): accuracy: 0.7330, mnli_loss: 0.6413
09/05 02:40:40 PM: Update 4261: task mnli, batch 261 (4258): accuracy: 0.7331, mnli_loss: 0.6409
09/05 02:40:50 PM: Update 4299: task mnli, batch 299 (4296): accuracy: 0.7337, mnli_loss: 0.6411
09/05 02:41:00 PM: Update 4336: task mnli, batch 336 (4333): accuracy: 0.7335, mnli_loss: 0.6410
09/05 02:41:10 PM: Update 4376: task mnli, batch 376 (4373): accuracy: 0.7358, mnli_loss: 0.6408
09/05 02:41:20 PM: Update 4414: task mnli, batch 414 (4411): accuracy: 0.7364, mnli_loss: 0.6395
09/05 02:41:30 PM: Update 4452: task mnli, batch 452 (4449): accuracy: 0.7378, mnli_loss: 0.6375
09/05 02:41:41 PM: Update 4492: task mnli, batch 492 (4489): accuracy: 0.7381, mnli_loss: 0.6372
09/05 02:41:51 PM: Update 4531: task mnli, batch 531 (4528): accuracy: 0.7402, mnli_loss: 0.6339
09/05 02:41:54 PM: Update 4541: task copa, batch 1 (4): accuracy: 0.4583, copa_loss: 0.7342
09/05 02:42:01 PM: Update 4568: task mnli, batch 567 (4564): accuracy: 0.7414, mnli_loss: 0.6321
09/05 02:42:11 PM: Update 4600: task mnli, batch 599 (4596): accuracy: 0.7423, mnli_loss: 0.6309
09/05 02:42:21 PM: Update 4637: task mnli, batch 636 (4633): accuracy: 0.7419, mnli_loss: 0.6302
09/05 02:42:31 PM: Update 4677: task mnli, batch 676 (4673): accuracy: 0.7430, mnli_loss: 0.6270
09/05 02:42:41 PM: Update 4716: task mnli, batch 715 (4712): accuracy: 0.7459, mnli_loss: 0.6228
09/05 02:42:51 PM: Update 4753: task mnli, batch 752 (4749): accuracy: 0.7453, mnli_loss: 0.6235
09/05 02:43:02 PM: Update 4791: task mnli, batch 790 (4787): accuracy: 0.7454, mnli_loss: 0.6225
09/05 02:43:12 PM: Update 4831: task mnli, batch 830 (4827): accuracy: 0.7453, mnli_loss: 0.6225
09/05 02:43:22 PM: Update 4870: task mnli, batch 869 (4866): accuracy: 0.7452, mnli_loss: 0.6221
09/05 02:43:32 PM: Update 4908: task mnli, batch 907 (4904): accuracy: 0.7437, mnli_loss: 0.6241
09/05 02:43:42 PM: Update 4945: task mnli, batch 944 (4941): accuracy: 0.7439, mnli_loss: 0.6239
09/05 02:43:52 PM: Update 4983: task mnli, batch 982 (4979): accuracy: 0.7436, mnli_loss: 0.6256
09/05 02:43:57 PM: ***** Step 5000 / Validation 5 *****
09/05 02:43:57 PM: copa: trained on 1 batches, 0.059 epochs
09/05 02:43:57 PM: mnli: trained on 999 batches, 0.061 epochs
09/05 02:43:57 PM: Validating...
09/05 02:43:57 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.5000, copa_loss: 0.6961
09/05 02:44:02 PM: Evaluate: task mnli, batch 71 (209): accuracy: 0.7594, mnli_loss: 0.5923
09/05 02:44:12 PM: Evaluate: task mnli, batch 207 (209): accuracy: 0.7597, mnli_loss: 0.5823
09/05 02:44:12 PM: Best result seen so far for mnli.
09/05 02:44:12 PM: Best result seen so far for micro.
09/05 02:44:12 PM: Best result seen so far for macro.
09/05 02:44:12 PM: Updating LR scheduler:
09/05 02:44:12 PM: 	Best result seen so far for macro_avg: 0.609
09/05 02:44:12 PM: 	# validation passes without improvement: 0
09/05 02:44:12 PM: copa_loss: training: 0.734230 validation: 0.693883
09/05 02:44:12 PM: mnli_loss: training: 0.625314 validation: 0.584635
09/05 02:44:12 PM: macro_avg: validation: 0.609300
09/05 02:44:12 PM: micro_avg: validation: 0.752745
09/05 02:44:12 PM: copa_accuracy: training: 0.458333 validation: 0.460000
09/05 02:44:12 PM: mnli_accuracy: training: 0.743907 validation: 0.758600
09/05 02:44:12 PM: Global learning rate: 1e-05
09/05 02:44:12 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 02:44:22 PM: Update 5028: task mnli, batch 28 (5024): accuracy: 0.7440, mnli_loss: 0.6268
09/05 02:44:33 PM: Update 5068: task mnli, batch 68 (5064): accuracy: 0.7500, mnli_loss: 0.6109
09/05 02:44:43 PM: Update 5107: task mnli, batch 107 (5103): accuracy: 0.7512, mnli_loss: 0.6055
09/05 02:44:53 PM: Update 5146: task mnli, batch 146 (5142): accuracy: 0.7463, mnli_loss: 0.6103
09/05 02:45:03 PM: Update 5185: task mnli, batch 185 (5181): accuracy: 0.7486, mnli_loss: 0.6088
09/05 02:45:13 PM: Update 5223: task mnli, batch 223 (5219): accuracy: 0.7524, mnli_loss: 0.6035
09/05 02:45:23 PM: Update 5261: task mnli, batch 261 (5257): accuracy: 0.7513, mnli_loss: 0.6034
09/05 02:45:33 PM: Update 5299: task mnli, batch 299 (5295): accuracy: 0.7529, mnli_loss: 0.6052
09/05 02:45:44 PM: Update 5338: task mnli, batch 338 (5334): accuracy: 0.7515, mnli_loss: 0.6073
09/05 02:45:54 PM: Update 5376: task mnli, batch 376 (5372): accuracy: 0.7513, mnli_loss: 0.6093
09/05 02:46:04 PM: Update 5413: task mnli, batch 413 (5409): accuracy: 0.7488, mnli_loss: 0.6115
09/05 02:46:14 PM: Update 5441: task mnli, batch 441 (5437): accuracy: 0.7493, mnli_loss: 0.6088
09/05 02:46:24 PM: Update 5481: task mnli, batch 481 (5477): accuracy: 0.7509, mnli_loss: 0.6077
09/05 02:46:34 PM: Update 5520: task mnli, batch 520 (5516): accuracy: 0.7506, mnli_loss: 0.6073
09/05 02:46:44 PM: Update 5558: task mnli, batch 558 (5554): accuracy: 0.7530, mnli_loss: 0.6037
09/05 02:46:55 PM: Update 5597: task mnli, batch 597 (5593): accuracy: 0.7542, mnli_loss: 0.6027
09/05 02:47:05 PM: Update 5637: task mnli, batch 637 (5633): accuracy: 0.7537, mnli_loss: 0.6039
09/05 02:47:15 PM: Update 5674: task mnli, batch 674 (5670): accuracy: 0.7542, mnli_loss: 0.6019
09/05 02:47:25 PM: Update 5712: task mnli, batch 712 (5708): accuracy: 0.7536, mnli_loss: 0.6031
09/05 02:47:35 PM: Update 5749: task mnli, batch 749 (5745): accuracy: 0.7535, mnli_loss: 0.6033
09/05 02:47:45 PM: Update 5787: task mnli, batch 787 (5783): accuracy: 0.7545, mnli_loss: 0.6002
09/05 02:47:55 PM: Update 5824: task mnli, batch 824 (5820): accuracy: 0.7545, mnli_loss: 0.6012
09/05 02:48:05 PM: Update 5854: task mnli, batch 854 (5850): accuracy: 0.7542, mnli_loss: 0.6010
09/05 02:48:15 PM: Update 5893: task mnli, batch 893 (5889): accuracy: 0.7541, mnli_loss: 0.6024
09/05 02:48:19 PM: Update 5908: task copa, batch 1 (5): accuracy: 0.5000, copa_loss: 0.6977
09/05 02:48:25 PM: Update 5930: task mnli, batch 929 (5925): accuracy: 0.7540, mnli_loss: 0.6019
09/05 02:48:36 PM: Update 5970: task mnli, batch 969 (5965): accuracy: 0.7546, mnli_loss: 0.6018
09/05 02:48:43 PM: ***** Step 6000 / Validation 6 *****
09/05 02:48:43 PM: copa: trained on 1 batches, 0.059 epochs
09/05 02:48:43 PM: mnli: trained on 999 batches, 0.061 epochs
09/05 02:48:43 PM: Validating...
09/05 02:48:43 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.5833, copa_loss: 0.6918
09/05 02:48:46 PM: Evaluate: task mnli, batch 29 (209): accuracy: 0.7601, mnli_loss: 0.5657
09/05 02:48:56 PM: Evaluate: task mnli, batch 166 (209): accuracy: 0.7743, mnli_loss: 0.5510
09/05 02:48:59 PM: Best result seen so far for copa.
09/05 02:48:59 PM: Best result seen so far for mnli.
09/05 02:48:59 PM: Best result seen so far for micro.
09/05 02:48:59 PM: Best result seen so far for macro.
09/05 02:48:59 PM: Updating LR scheduler:
09/05 02:48:59 PM: 	Best result seen so far for macro_avg: 0.637
09/05 02:48:59 PM: 	# validation passes without improvement: 0
09/05 02:48:59 PM: copa_loss: training: 0.697720 validation: 0.691204
09/05 02:48:59 PM: mnli_loss: training: 0.600784 validation: 0.554737
09/05 02:48:59 PM: macro_avg: validation: 0.637200
09/05 02:48:59 PM: micro_avg: validation: 0.769020
09/05 02:48:59 PM: copa_accuracy: training: 0.500000 validation: 0.500000
09/05 02:48:59 PM: mnli_accuracy: training: 0.755177 validation: 0.774400
09/05 02:48:59 PM: Global learning rate: 1e-05
09/05 02:48:59 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 02:49:06 PM: Update 6023: task mnli, batch 23 (6018): accuracy: 0.7482, mnli_loss: 0.6304
09/05 02:49:16 PM: Update 6062: task mnli, batch 62 (6057): accuracy: 0.7782, mnli_loss: 0.5778
09/05 02:49:26 PM: Update 6100: task mnli, batch 100 (6095): accuracy: 0.7758, mnli_loss: 0.5756
09/05 02:49:34 PM: Update 6128: task copa, batch 1 (6): accuracy: 0.6250, copa_loss: 0.6837
09/05 02:49:37 PM: Update 6138: task mnli, batch 137 (6132): accuracy: 0.7646, mnli_loss: 0.5840
09/05 02:49:47 PM: Update 6174: task mnli, batch 173 (6168): accuracy: 0.7587, mnli_loss: 0.5896
09/05 02:49:57 PM: Update 6213: task mnli, batch 212 (6207): accuracy: 0.7604, mnli_loss: 0.5885
09/05 02:50:07 PM: Update 6255: task mnli, batch 254 (6249): accuracy: 0.7649, mnli_loss: 0.5788
09/05 02:50:17 PM: Update 6285: task mnli, batch 284 (6279): accuracy: 0.7638, mnli_loss: 0.5792
09/05 02:50:27 PM: Update 6324: task mnli, batch 323 (6318): accuracy: 0.7632, mnli_loss: 0.5767
09/05 02:50:38 PM: Update 6364: task mnli, batch 363 (6358): accuracy: 0.7646, mnli_loss: 0.5760
09/05 02:50:48 PM: Update 6403: task mnli, batch 402 (6397): accuracy: 0.7630, mnli_loss: 0.5804
09/05 02:50:58 PM: Update 6441: task mnli, batch 440 (6435): accuracy: 0.7626, mnli_loss: 0.5824
09/05 02:51:08 PM: Update 6479: task mnli, batch 478 (6473): accuracy: 0.7623, mnli_loss: 0.5846
09/05 02:51:18 PM: Update 6518: task mnli, batch 517 (6512): accuracy: 0.7623, mnli_loss: 0.5840
09/05 02:51:25 PM: Update 6542: task copa, batch 2 (7): accuracy: 0.6458, copa_loss: 0.6808
09/05 02:51:28 PM: Update 6557: task mnli, batch 555 (6550): accuracy: 0.7636, mnli_loss: 0.5836
09/05 02:51:38 PM: Update 6593: task mnli, batch 591 (6586): accuracy: 0.7614, mnli_loss: 0.5882
09/05 02:51:49 PM: Update 6631: task mnli, batch 629 (6624): accuracy: 0.7631, mnli_loss: 0.5867
09/05 02:51:59 PM: Update 6669: task mnli, batch 667 (6662): accuracy: 0.7630, mnli_loss: 0.5862
09/05 02:52:09 PM: Update 6700: task mnli, batch 698 (6693): accuracy: 0.7642, mnli_loss: 0.5842
09/05 02:52:19 PM: Update 6737: task mnli, batch 735 (6730): accuracy: 0.7628, mnli_loss: 0.5861
09/05 02:52:29 PM: Update 6776: task mnli, batch 774 (6769): accuracy: 0.7633, mnli_loss: 0.5844
09/05 02:52:39 PM: Update 6813: task mnli, batch 811 (6806): accuracy: 0.7630, mnli_loss: 0.5852
09/05 02:52:49 PM: Update 6851: task mnli, batch 849 (6844): accuracy: 0.7630, mnli_loss: 0.5848
09/05 02:52:59 PM: Update 6890: task mnli, batch 888 (6883): accuracy: 0.7636, mnli_loss: 0.5838
09/05 02:53:09 PM: Update 6930: task mnli, batch 928 (6923): accuracy: 0.7640, mnli_loss: 0.5821
09/05 02:53:20 PM: Update 6968: task mnli, batch 966 (6961): accuracy: 0.7640, mnli_loss: 0.5820
09/05 02:53:28 PM: ***** Step 7000 / Validation 7 *****
09/05 02:53:28 PM: copa: trained on 2 batches, 0.118 epochs
09/05 02:53:28 PM: mnli: trained on 998 batches, 0.061 epochs
09/05 02:53:28 PM: Validating...
09/05 02:53:28 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.5833, copa_loss: 0.6981
09/05 02:53:30 PM: Evaluate: task mnli, batch 20 (209): accuracy: 0.7625, mnli_loss: 0.5627
09/05 02:53:40 PM: Evaluate: task mnli, batch 157 (209): accuracy: 0.7797, mnli_loss: 0.5491
09/05 02:53:43 PM: Best result seen so far for copa.
09/05 02:53:43 PM: Best result seen so far for mnli.
09/05 02:53:43 PM: Best result seen so far for micro.
09/05 02:53:43 PM: Best result seen so far for macro.
09/05 02:53:43 PM: Updating LR scheduler:
09/05 02:53:43 PM: 	Best result seen so far for macro_avg: 0.650
09/05 02:53:43 PM: 	# validation passes without improvement: 0
09/05 02:53:43 PM: copa_loss: training: 0.680837 validation: 0.690143
09/05 02:53:43 PM: mnli_loss: training: 0.581323 validation: 0.551235
09/05 02:53:43 PM: macro_avg: validation: 0.650300
09/05 02:53:43 PM: micro_avg: validation: 0.775490
09/05 02:53:43 PM: copa_accuracy: training: 0.645833 validation: 0.520000
09/05 02:53:43 PM: mnli_accuracy: training: 0.764622 validation: 0.780600
09/05 02:53:43 PM: Global learning rate: 1e-05
09/05 02:53:43 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 02:53:50 PM: Update 7021: task mnli, batch 21 (7014): accuracy: 0.7619, mnli_loss: 0.5541
09/05 02:54:00 PM: Update 7059: task mnli, batch 59 (7052): accuracy: 0.7620, mnli_loss: 0.5832
09/05 02:54:13 PM: Update 7097: task mnli, batch 97 (7090): accuracy: 0.7724, mnli_loss: 0.5709
09/05 02:54:23 PM: Update 7134: task mnli, batch 134 (7127): accuracy: 0.7724, mnli_loss: 0.5680
09/05 02:54:33 PM: Update 7173: task mnli, batch 173 (7166): accuracy: 0.7688, mnli_loss: 0.5714
09/05 02:54:43 PM: Update 7212: task mnli, batch 212 (7205): accuracy: 0.7650, mnli_loss: 0.5786
09/05 02:54:53 PM: Update 7251: task mnli, batch 251 (7244): accuracy: 0.7658, mnli_loss: 0.5797
09/05 02:55:00 PM: Update 7276: task copa, batch 1 (8): accuracy: 0.5000, copa_loss: 0.6872
09/05 02:55:03 PM: Update 7290: task mnli, batch 289 (7282): accuracy: 0.7670, mnli_loss: 0.5796
09/05 02:55:14 PM: Update 7329: task mnli, batch 328 (7321): accuracy: 0.7665, mnli_loss: 0.5800
09/05 02:55:24 PM: Update 7366: task mnli, batch 365 (7358): accuracy: 0.7671, mnli_loss: 0.5760
09/05 02:55:28 PM: Update 7383: task copa, batch 2 (9): accuracy: 0.5417, copa_loss: 0.6601
09/05 02:55:34 PM: Update 7404: task mnli, batch 402 (7395): accuracy: 0.7693, mnli_loss: 0.5725
09/05 02:55:44 PM: Update 7443: task mnli, batch 441 (7434): accuracy: 0.7711, mnli_loss: 0.5689
09/05 02:55:54 PM: Update 7481: task mnli, batch 479 (7472): accuracy: 0.7730, mnli_loss: 0.5655
09/05 02:55:58 PM: Update 7497: task copa, batch 3 (10): accuracy: 0.5278, copa_loss: 0.6807
09/05 02:56:06 PM: Update 7518: task mnli, batch 514 (7507): accuracy: 0.7723, mnli_loss: 0.5672
09/05 02:56:16 PM: Update 7558: task mnli, batch 554 (7547): accuracy: 0.7712, mnli_loss: 0.5679
09/05 02:56:26 PM: Update 7593: task copa, batch 5 (12): accuracy: 0.5583, copa_loss: 0.6733
09/05 02:56:26 PM: Update 7595: task mnli, batch 590 (7583): accuracy: 0.7711, mnli_loss: 0.5683
09/05 02:56:37 PM: Update 7632: task mnli, batch 626 (7619): accuracy: 0.7718, mnli_loss: 0.5672
09/05 02:56:47 PM: Update 7671: task mnli, batch 665 (7658): accuracy: 0.7724, mnli_loss: 0.5659
09/05 02:56:57 PM: Update 7711: task mnli, batch 705 (7698): accuracy: 0.7720, mnli_loss: 0.5662
09/05 02:57:07 PM: Update 7748: task mnli, batch 742 (7735): accuracy: 0.7737, mnli_loss: 0.5634
09/05 02:57:17 PM: Update 7786: task mnli, batch 780 (7773): accuracy: 0.7746, mnli_loss: 0.5613
09/05 02:57:27 PM: Update 7826: task mnli, batch 820 (7813): accuracy: 0.7750, mnli_loss: 0.5598
09/05 02:57:37 PM: Update 7863: task mnli, batch 857 (7850): accuracy: 0.7754, mnli_loss: 0.5599
09/05 02:57:47 PM: Update 7901: task mnli, batch 895 (7888): accuracy: 0.7753, mnli_loss: 0.5603
09/05 02:57:58 PM: Update 7937: task mnli, batch 931 (7924): accuracy: 0.7754, mnli_loss: 0.5598
09/05 02:58:08 PM: Update 7973: task mnli, batch 967 (7960): accuracy: 0.7747, mnli_loss: 0.5611
09/05 02:58:15 PM: ***** Step 8000 / Validation 8 *****
09/05 02:58:15 PM: copa: trained on 6 batches, 0.353 epochs
09/05 02:58:15 PM: mnli: trained on 994 batches, 0.061 epochs
09/05 02:58:15 PM: Validating...
09/05 02:58:15 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.7917, copa_loss: 0.6515
09/05 02:58:19 PM: Evaluate: task mnli, batch 40 (209): accuracy: 0.7958, mnli_loss: 0.5179
09/05 02:58:29 PM: Evaluate: task mnli, batch 177 (209): accuracy: 0.7874, mnli_loss: 0.5216
09/05 02:58:31 PM: Best result seen so far for copa.
09/05 02:58:31 PM: Best result seen so far for mnli.
09/05 02:58:31 PM: Best result seen so far for micro.
09/05 02:58:31 PM: Best result seen so far for macro.
09/05 02:58:31 PM: Updating LR scheduler:
09/05 02:58:31 PM: 	Best result seen so far for macro_avg: 0.729
09/05 02:58:31 PM: 	# validation passes without improvement: 0
09/05 02:58:31 PM: copa_loss: training: 0.666213 validation: 0.664723
09/05 02:58:31 PM: mnli_loss: training: 0.560193 validation: 0.528879
09/05 02:58:31 PM: macro_avg: validation: 0.728500
09/05 02:58:31 PM: micro_avg: validation: 0.784706
09/05 02:58:31 PM: copa_accuracy: training: 0.576389 validation: 0.670000
09/05 02:58:31 PM: mnli_accuracy: training: 0.775176 validation: 0.787000
09/05 02:58:31 PM: Global learning rate: 1e-05
09/05 02:58:31 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 02:58:39 PM: Update 8025: task mnli, batch 25 (8012): accuracy: 0.7600, mnli_loss: 0.5822
09/05 02:58:49 PM: Update 8064: task mnli, batch 64 (8051): accuracy: 0.7773, mnli_loss: 0.5472
09/05 02:58:59 PM: Update 8104: task mnli, batch 104 (8091): accuracy: 0.7716, mnli_loss: 0.5585
09/05 02:59:09 PM: Update 8141: task mnli, batch 141 (8128): accuracy: 0.7725, mnli_loss: 0.5629
09/05 02:59:19 PM: Update 8180: task mnli, batch 180 (8167): accuracy: 0.7720, mnli_loss: 0.5607
09/05 02:59:29 PM: Update 8219: task mnli, batch 219 (8206): accuracy: 0.7675, mnli_loss: 0.5672
09/05 02:59:39 PM: Update 8257: task mnli, batch 257 (8244): accuracy: 0.7704, mnli_loss: 0.5612
09/05 02:59:49 PM: Update 8295: task mnli, batch 295 (8282): accuracy: 0.7695, mnli_loss: 0.5657
09/05 03:00:00 PM: Update 8333: task mnli, batch 333 (8320): accuracy: 0.7691, mnli_loss: 0.5680
09/05 03:00:10 PM: Update 8364: task mnli, batch 364 (8351): accuracy: 0.7686, mnli_loss: 0.5678
09/05 03:00:20 PM: Update 8402: task mnli, batch 402 (8389): accuracy: 0.7695, mnli_loss: 0.5651
09/05 03:00:30 PM: Update 8443: task mnli, batch 443 (8430): accuracy: 0.7720, mnli_loss: 0.5610
09/05 03:00:40 PM: Update 8481: task mnli, batch 481 (8468): accuracy: 0.7732, mnli_loss: 0.5574
09/05 03:00:50 PM: Update 8519: task mnli, batch 519 (8506): accuracy: 0.7724, mnli_loss: 0.5592
09/05 03:01:00 PM: Update 8557: task mnli, batch 557 (8544): accuracy: 0.7723, mnli_loss: 0.5599
09/05 03:01:11 PM: Update 8595: task mnli, batch 595 (8582): accuracy: 0.7730, mnli_loss: 0.5581
09/05 03:01:21 PM: Update 8633: task mnli, batch 633 (8620): accuracy: 0.7727, mnli_loss: 0.5600
09/05 03:01:31 PM: Update 8672: task mnli, batch 672 (8659): accuracy: 0.7723, mnli_loss: 0.5607
09/05 03:01:41 PM: Update 8711: task mnli, batch 711 (8698): accuracy: 0.7720, mnli_loss: 0.5600
09/05 03:01:51 PM: Update 8748: task mnli, batch 748 (8735): accuracy: 0.7712, mnli_loss: 0.5614
09/05 03:02:01 PM: Update 8778: task mnli, batch 778 (8765): accuracy: 0.7710, mnli_loss: 0.5618
09/05 03:02:11 PM: Update 8815: task mnli, batch 815 (8802): accuracy: 0.7697, mnli_loss: 0.5645
09/05 03:02:21 PM: Update 8854: task mnli, batch 854 (8841): accuracy: 0.7699, mnli_loss: 0.5647
09/05 03:02:31 PM: Update 8893: task mnli, batch 893 (8880): accuracy: 0.7703, mnli_loss: 0.5644
09/05 03:02:42 PM: Update 8933: task mnli, batch 933 (8920): accuracy: 0.7706, mnli_loss: 0.5635
09/05 03:02:52 PM: Update 8972: task mnli, batch 972 (8959): accuracy: 0.7712, mnli_loss: 0.5627
09/05 03:02:59 PM: ***** Step 9000 / Validation 9 *****
09/05 03:02:59 PM: copa: trained on 0 batches, 0.000 epochs
09/05 03:02:59 PM: mnli: trained on 1000 batches, 0.061 epochs
09/05 03:02:59 PM: Validating...
09/05 03:02:59 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.7500, copa_loss: 0.6582
09/05 03:03:02 PM: Evaluate: task mnli, batch 31 (209): accuracy: 0.7944, mnli_loss: 0.5222
09/05 03:03:12 PM: Evaluate: task mnli, batch 158 (209): accuracy: 0.7956, mnli_loss: 0.5100
09/05 03:03:15 PM: Best result seen so far for copa.
09/05 03:03:15 PM: Best result seen so far for mnli.
09/05 03:03:15 PM: Best result seen so far for micro.
09/05 03:03:15 PM: Best result seen so far for macro.
09/05 03:03:15 PM: Updating LR scheduler:
09/05 03:03:15 PM: 	Best result seen so far for macro_avg: 0.742
09/05 03:03:15 PM: 	# validation passes without improvement: 0
09/05 03:03:15 PM: copa_loss: training: 0.000000 validation: 0.665177
09/05 03:03:15 PM: mnli_loss: training: 0.562458 validation: 0.515910
09/05 03:03:15 PM: macro_avg: validation: 0.741500
09/05 03:03:15 PM: micro_avg: validation: 0.790980
09/05 03:03:15 PM: copa_accuracy: validation: 0.690000
09/05 03:03:15 PM: mnli_accuracy: training: 0.771765 validation: 0.793000
09/05 03:03:15 PM: Global learning rate: 1e-05
09/05 03:03:15 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 03:03:22 PM: Update 9021: task mnli, batch 21 (9008): accuracy: 0.7639, mnli_loss: 0.5701
09/05 03:03:32 PM: Update 9059: task mnli, batch 59 (9046): accuracy: 0.7634, mnli_loss: 0.5492
09/05 03:03:42 PM: Update 9095: task mnli, batch 95 (9082): accuracy: 0.7719, mnli_loss: 0.5469
09/05 03:03:52 PM: Update 9134: task mnli, batch 134 (9121): accuracy: 0.7715, mnli_loss: 0.5567
09/05 03:04:03 PM: Update 9172: task mnli, batch 172 (9159): accuracy: 0.7776, mnli_loss: 0.5500
09/05 03:04:13 PM: Update 9203: task mnli, batch 203 (9190): accuracy: 0.7800, mnli_loss: 0.5493
09/05 03:04:23 PM: Update 9240: task mnli, batch 240 (9227): accuracy: 0.7768, mnli_loss: 0.5552
09/05 03:04:33 PM: Update 9280: task mnli, batch 280 (9267): accuracy: 0.7791, mnli_loss: 0.5485
09/05 03:04:43 PM: Update 9316: task mnli, batch 316 (9303): accuracy: 0.7779, mnli_loss: 0.5500
09/05 03:04:45 PM: Update 9324: task copa, batch 1 (14): accuracy: 0.7083, copa_loss: 0.6603
09/05 03:04:53 PM: Update 9354: task mnli, batch 353 (9340): accuracy: 0.7769, mnli_loss: 0.5503
09/05 03:05:03 PM: Update 9392: task mnli, batch 391 (9378): accuracy: 0.7784, mnli_loss: 0.5480
09/05 03:05:14 PM: Update 9430: task mnli, batch 429 (9416): accuracy: 0.7788, mnli_loss: 0.5465
09/05 03:05:24 PM: Update 9469: task mnli, batch 468 (9455): accuracy: 0.7790, mnli_loss: 0.5460
09/05 03:05:34 PM: Update 9509: task mnli, batch 508 (9495): accuracy: 0.7777, mnli_loss: 0.5492
09/05 03:05:44 PM: Update 9548: task mnli, batch 547 (9534): accuracy: 0.7780, mnli_loss: 0.5509
09/05 03:05:54 PM: Update 9587: task mnli, batch 586 (9573): accuracy: 0.7780, mnli_loss: 0.5520
09/05 03:06:04 PM: Update 9617: task mnli, batch 616 (9603): accuracy: 0.7792, mnli_loss: 0.5505
09/05 03:06:15 PM: Update 9656: task mnli, batch 655 (9642): accuracy: 0.7803, mnli_loss: 0.5485
09/05 03:06:25 PM: Update 9695: task mnli, batch 694 (9681): accuracy: 0.7805, mnli_loss: 0.5488
09/05 03:06:35 PM: Update 9732: task mnli, batch 731 (9718): accuracy: 0.7813, mnli_loss: 0.5466
09/05 03:06:45 PM: Update 9771: task mnli, batch 770 (9757): accuracy: 0.7810, mnli_loss: 0.5463
09/05 03:06:55 PM: Update 9808: task mnli, batch 807 (9794): accuracy: 0.7803, mnli_loss: 0.5465
09/05 03:07:05 PM: Update 9846: task mnli, batch 845 (9832): accuracy: 0.7805, mnli_loss: 0.5472
09/05 03:07:16 PM: Update 9885: task mnli, batch 884 (9871): accuracy: 0.7807, mnli_loss: 0.5467
09/05 03:07:26 PM: Update 9924: task mnli, batch 923 (9910): accuracy: 0.7798, mnli_loss: 0.5483
09/05 03:07:36 PM: Update 9963: task mnli, batch 962 (9949): accuracy: 0.7798, mnli_loss: 0.5481
09/05 03:07:45 PM: ***** Step 10000 / Validation 10 *****
09/05 03:07:45 PM: copa: trained on 1 batches, 0.059 epochs
09/05 03:07:45 PM: mnli: trained on 999 batches, 0.061 epochs
09/05 03:07:45 PM: Validating...
09/05 03:07:45 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.7917, copa_loss: 0.6465
09/05 03:07:46 PM: Evaluate: task mnli, batch 2 (209): accuracy: 0.7917, mnli_loss: 0.5344
09/05 03:07:56 PM: Evaluate: task mnli, batch 139 (209): accuracy: 0.7944, mnli_loss: 0.5090
09/05 03:08:01 PM: Updating LR scheduler:
09/05 03:08:01 PM: 	Best result seen so far for macro_avg: 0.742
09/05 03:08:01 PM: 	# validation passes without improvement: 1
09/05 03:08:01 PM: copa_loss: training: 0.660329 validation: 0.655152
09/05 03:08:01 PM: mnli_loss: training: 0.547581 validation: 0.515797
09/05 03:08:01 PM: macro_avg: validation: 0.736200
09/05 03:08:01 PM: micro_avg: validation: 0.790196
09/05 03:08:01 PM: copa_accuracy: training: 0.708333 validation: 0.680000
09/05 03:08:01 PM: mnli_accuracy: training: 0.780092 validation: 0.792400
09/05 03:08:01 PM: Global learning rate: 1e-05
09/05 03:08:01 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 03:08:06 PM: Update 10017: task mnli, batch 17 (10003): accuracy: 0.7966, mnli_loss: 0.5130
09/05 03:08:16 PM: Update 10048: task mnli, batch 48 (10034): accuracy: 0.7675, mnli_loss: 0.5811
09/05 03:08:27 PM: Update 10087: task mnli, batch 87 (10073): accuracy: 0.7692, mnli_loss: 0.5640
09/05 03:08:37 PM: Update 10125: task mnli, batch 125 (10111): accuracy: 0.7757, mnli_loss: 0.5516
09/05 03:08:47 PM: Update 10164: task mnli, batch 164 (10150): accuracy: 0.7859, mnli_loss: 0.5376
09/05 03:08:57 PM: Update 10203: task mnli, batch 203 (10189): accuracy: 0.7856, mnli_loss: 0.5352
09/05 03:09:07 PM: Update 10239: task mnli, batch 239 (10225): accuracy: 0.7867, mnli_loss: 0.5334
09/05 03:09:17 PM: Update 10277: task mnli, batch 277 (10263): accuracy: 0.7863, mnli_loss: 0.5336
09/05 03:09:27 PM: Update 10315: task mnli, batch 315 (10301): accuracy: 0.7887, mnli_loss: 0.5279
09/05 03:09:38 PM: Update 10354: task mnli, batch 354 (10340): accuracy: 0.7886, mnli_loss: 0.5277
09/05 03:09:48 PM: Update 10394: task mnli, batch 394 (10380): accuracy: 0.7891, mnli_loss: 0.5299
09/05 03:09:58 PM: Update 10434: task mnli, batch 434 (10420): accuracy: 0.7925, mnli_loss: 0.5252
09/05 03:10:08 PM: Update 10463: task mnli, batch 463 (10449): accuracy: 0.7928, mnli_loss: 0.5245
09/05 03:10:19 PM: Update 10501: task mnli, batch 501 (10487): accuracy: 0.7915, mnli_loss: 0.5263
09/05 03:10:29 PM: Update 10539: task mnli, batch 539 (10525): accuracy: 0.7922, mnli_loss: 0.5244
09/05 03:10:33 PM: Update 10555: task copa, batch 1 (15): accuracy: 0.5417, copa_loss: 0.6869
09/05 03:10:39 PM: Update 10578: task mnli, batch 577 (10563): accuracy: 0.7937, mnli_loss: 0.5211
09/05 03:10:49 PM: Update 10616: task mnli, batch 615 (10601): accuracy: 0.7934, mnli_loss: 0.5199
09/05 03:10:59 PM: Update 10654: task mnli, batch 653 (10639): accuracy: 0.7930, mnli_loss: 0.5196
09/05 03:11:09 PM: Update 10693: task mnli, batch 692 (10678): accuracy: 0.7918, mnli_loss: 0.5218
09/05 03:11:19 PM: Update 10732: task mnli, batch 731 (10717): accuracy: 0.7911, mnli_loss: 0.5230
09/05 03:11:30 PM: Update 10769: task mnli, batch 768 (10754): accuracy: 0.7906, mnli_loss: 0.5235
09/05 03:11:40 PM: Update 10808: task mnli, batch 807 (10793): accuracy: 0.7904, mnli_loss: 0.5244
09/05 03:11:50 PM: Update 10847: task mnli, batch 846 (10832): accuracy: 0.7907, mnli_loss: 0.5233
09/05 03:12:00 PM: Update 10877: task mnli, batch 876 (10862): accuracy: 0.7896, mnli_loss: 0.5251
09/05 03:12:10 PM: Update 10914: task mnli, batch 913 (10899): accuracy: 0.7896, mnli_loss: 0.5243
09/05 03:12:20 PM: Update 10952: task mnli, batch 951 (10937): accuracy: 0.7897, mnli_loss: 0.5248
09/05 03:12:30 PM: Update 10992: task mnli, batch 991 (10977): accuracy: 0.7907, mnli_loss: 0.5233
09/05 03:12:32 PM: ***** Step 11000 / Validation 11 *****
09/05 03:12:32 PM: copa: trained on 1 batches, 0.059 epochs
09/05 03:12:32 PM: mnli: trained on 999 batches, 0.061 epochs
09/05 03:12:32 PM: Validating...
09/05 03:12:32 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.7083, copa_loss: 0.6285
09/05 03:12:40 PM: Evaluate: task mnli, batch 101 (209): accuracy: 0.7941, mnli_loss: 0.5061
09/05 03:12:48 PM: Best result seen so far for mnli.
09/05 03:12:48 PM: Best result seen so far for micro.
09/05 03:12:48 PM: Updating LR scheduler:
09/05 03:12:48 PM: 	Best result seen so far for macro_avg: 0.742
09/05 03:12:48 PM: 	# validation passes without improvement: 2
09/05 03:12:48 PM: copa_loss: training: 0.686868 validation: 0.642086
09/05 03:12:48 PM: mnli_loss: training: 0.523571 validation: 0.513250
09/05 03:12:48 PM: macro_avg: validation: 0.718000
09/05 03:12:48 PM: micro_avg: validation: 0.792941
09/05 03:12:48 PM: copa_accuracy: training: 0.541667 validation: 0.640000
09/05 03:12:48 PM: mnli_accuracy: training: 0.790414 validation: 0.796000
09/05 03:12:48 PM: Global learning rate: 1e-05
09/05 03:12:48 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 03:12:50 PM: Update 11006: task mnli, batch 6 (10991): accuracy: 0.8056, mnli_loss: 0.5011
09/05 03:13:00 PM: Update 11044: task mnli, batch 44 (11029): accuracy: 0.7794, mnli_loss: 0.5376
09/05 03:13:11 PM: Update 11083: task mnli, batch 83 (11068): accuracy: 0.7882, mnli_loss: 0.5239
09/05 03:13:21 PM: Update 11120: task mnli, batch 120 (11105): accuracy: 0.7833, mnli_loss: 0.5300
09/05 03:13:31 PM: Update 11159: task mnli, batch 159 (11144): accuracy: 0.7875, mnli_loss: 0.5203
09/05 03:13:41 PM: Update 11197: task mnli, batch 197 (11182): accuracy: 0.7851, mnli_loss: 0.5256
09/05 03:13:51 PM: Update 11235: task mnli, batch 235 (11220): accuracy: 0.7853, mnli_loss: 0.5286
09/05 03:14:01 PM: Update 11274: task mnli, batch 274 (11259): accuracy: 0.7871, mnli_loss: 0.5274
09/05 03:14:11 PM: Update 11306: task mnli, batch 306 (11291): accuracy: 0.7865, mnli_loss: 0.5248
09/05 03:14:22 PM: Update 11346: task mnli, batch 346 (11331): accuracy: 0.7907, mnli_loss: 0.5192
09/05 03:14:32 PM: Update 11385: task mnli, batch 385 (11370): accuracy: 0.7912, mnli_loss: 0.5188
09/05 03:14:35 PM: Update 11396: task copa, batch 1 (16): accuracy: 0.6667, copa_loss: 0.6119
09/05 03:14:42 PM: Update 11424: task mnli, batch 423 (11408): accuracy: 0.7912, mnli_loss: 0.5192
09/05 03:14:52 PM: Update 11464: task mnli, batch 463 (11448): accuracy: 0.7910, mnli_loss: 0.5203
09/05 03:15:02 PM: Update 11503: task mnli, batch 502 (11487): accuracy: 0.7906, mnli_loss: 0.5218
09/05 03:15:12 PM: Update 11541: task mnli, batch 540 (11525): accuracy: 0.7905, mnli_loss: 0.5229
09/05 03:15:22 PM: Update 11578: task mnli, batch 577 (11562): accuracy: 0.7911, mnli_loss: 0.5219
09/05 03:15:32 PM: Update 11615: task mnli, batch 614 (11599): accuracy: 0.7903, mnli_loss: 0.5240
09/05 03:15:43 PM: Update 11652: task mnli, batch 651 (11636): accuracy: 0.7896, mnli_loss: 0.5253
09/05 03:15:53 PM: Update 11690: task mnli, batch 689 (11674): accuracy: 0.7888, mnli_loss: 0.5252
09/05 03:16:03 PM: Update 11722: task mnli, batch 721 (11706): accuracy: 0.7887, mnli_loss: 0.5279
09/05 03:16:13 PM: Update 11761: task mnli, batch 760 (11745): accuracy: 0.7882, mnli_loss: 0.5286
09/05 03:16:23 PM: Update 11799: task mnli, batch 798 (11783): accuracy: 0.7885, mnli_loss: 0.5277
09/05 03:16:34 PM: Update 11838: task mnli, batch 837 (11822): accuracy: 0.7884, mnli_loss: 0.5281
09/05 03:16:44 PM: Update 11876: task mnli, batch 875 (11860): accuracy: 0.7888, mnli_loss: 0.5276
09/05 03:16:54 PM: Update 11914: task mnli, batch 913 (11898): accuracy: 0.7889, mnli_loss: 0.5278
09/05 03:17:04 PM: Update 11953: task mnli, batch 952 (11937): accuracy: 0.7879, mnli_loss: 0.5286
09/05 03:17:14 PM: Update 11991: task mnli, batch 990 (11975): accuracy: 0.7877, mnli_loss: 0.5282
09/05 03:17:16 PM: ***** Step 12000 / Validation 12 *****
09/05 03:17:16 PM: copa: trained on 1 batches, 0.059 epochs
09/05 03:17:16 PM: mnli: trained on 999 batches, 0.061 epochs
09/05 03:17:16 PM: Validating...
09/05 03:17:17 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.7917, copa_loss: 0.6231
09/05 03:17:24 PM: Evaluate: task mnli, batch 100 (209): accuracy: 0.8071, mnli_loss: 0.4791
09/05 03:17:32 PM: Best result seen so far for copa.
09/05 03:17:32 PM: Best result seen so far for mnli.
09/05 03:17:32 PM: Best result seen so far for micro.
09/05 03:17:32 PM: Best result seen so far for macro.
09/05 03:17:32 PM: Updating LR scheduler:
09/05 03:17:32 PM: 	Best result seen so far for macro_avg: 0.753
09/05 03:17:32 PM: 	# validation passes without improvement: 0
09/05 03:17:32 PM: copa_loss: training: 0.611921 validation: 0.642130
09/05 03:17:32 PM: mnli_loss: training: 0.528354 validation: 0.487550
09/05 03:17:32 PM: macro_avg: validation: 0.753000
09/05 03:17:32 PM: micro_avg: validation: 0.803922
09/05 03:17:32 PM: copa_accuracy: training: 0.666667 validation: 0.700000
09/05 03:17:32 PM: mnli_accuracy: training: 0.787563 validation: 0.806000
09/05 03:17:32 PM: Global learning rate: 1e-05
09/05 03:17:32 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 03:17:34 PM: Update 12005: task mnli, batch 5 (11989): accuracy: 0.7667, mnli_loss: 0.6103
09/05 03:17:45 PM: Update 12044: task mnli, batch 44 (12028): accuracy: 0.7983, mnli_loss: 0.5008
09/05 03:17:55 PM: Update 12081: task mnli, batch 81 (12065): accuracy: 0.7999, mnli_loss: 0.4958
09/05 03:18:05 PM: Update 12111: task mnli, batch 111 (12095): accuracy: 0.7986, mnli_loss: 0.5047
09/05 03:18:15 PM: Update 12148: task mnli, batch 148 (12132): accuracy: 0.7971, mnli_loss: 0.5023
09/05 03:18:25 PM: Update 12187: task mnli, batch 187 (12171): accuracy: 0.7980, mnli_loss: 0.5025
09/05 03:18:36 PM: Update 12225: task mnli, batch 225 (12209): accuracy: 0.7982, mnli_loss: 0.5044
09/05 03:18:46 PM: Update 12261: task mnli, batch 261 (12245): accuracy: 0.8008, mnli_loss: 0.5026
09/05 03:18:56 PM: Update 12299: task mnli, batch 299 (12283): accuracy: 0.7985, mnli_loss: 0.5062
09/05 03:19:06 PM: Update 12337: task mnli, batch 337 (12321): accuracy: 0.7960, mnli_loss: 0.5118
09/05 03:19:16 PM: Update 12376: task mnli, batch 376 (12360): accuracy: 0.7960, mnli_loss: 0.5103
09/05 03:19:26 PM: Update 12416: task mnli, batch 416 (12400): accuracy: 0.7955, mnli_loss: 0.5104
09/05 03:19:36 PM: Update 12456: task mnli, batch 456 (12440): accuracy: 0.7937, mnli_loss: 0.5148
09/05 03:19:47 PM: Update 12496: task mnli, batch 496 (12480): accuracy: 0.7946, mnli_loss: 0.5141
09/05 03:19:57 PM: Update 12528: task mnli, batch 528 (12512): accuracy: 0.7943, mnli_loss: 0.5156
09/05 03:20:07 PM: Update 12566: task mnli, batch 566 (12550): accuracy: 0.7955, mnli_loss: 0.5132
09/05 03:20:17 PM: Update 12604: task mnli, batch 604 (12588): accuracy: 0.7965, mnli_loss: 0.5123
09/05 03:20:27 PM: Update 12643: task mnli, batch 643 (12627): accuracy: 0.7963, mnli_loss: 0.5129
09/05 03:20:37 PM: Update 12681: task mnli, batch 681 (12665): accuracy: 0.7963, mnli_loss: 0.5132
09/05 03:20:48 PM: Update 12718: task mnli, batch 718 (12702): accuracy: 0.7976, mnli_loss: 0.5105
09/05 03:20:58 PM: Update 12758: task mnli, batch 758 (12742): accuracy: 0.7986, mnli_loss: 0.5085
09/05 03:21:08 PM: Update 12795: task mnli, batch 795 (12779): accuracy: 0.7985, mnli_loss: 0.5090
09/05 03:21:18 PM: Update 12834: task mnli, batch 834 (12818): accuracy: 0.7985, mnli_loss: 0.5093
09/05 03:21:28 PM: Update 12874: task mnli, batch 874 (12858): accuracy: 0.7998, mnli_loss: 0.5070
09/05 03:21:37 PM: Update 12908: task copa, batch 1 (17): accuracy: 0.7083, copa_loss: 0.6178
09/05 03:21:38 PM: Update 12910: task mnli, batch 909 (12893): accuracy: 0.7989, mnli_loss: 0.5087
09/05 03:21:49 PM: Update 12945: task mnli, batch 944 (12928): accuracy: 0.7991, mnli_loss: 0.5073
09/05 03:21:59 PM: Update 12982: task mnli, batch 981 (12965): accuracy: 0.7982, mnli_loss: 0.5091
09/05 03:22:04 PM: ***** Step 13000 / Validation 13 *****
09/05 03:22:04 PM: copa: trained on 1 batches, 0.059 epochs
09/05 03:22:04 PM: mnli: trained on 999 batches, 0.061 epochs
09/05 03:22:04 PM: Validating...
09/05 03:22:04 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8333, copa_loss: 0.6152
09/05 03:22:09 PM: Evaluate: task mnli, batch 66 (209): accuracy: 0.7992, mnli_loss: 0.4825
09/05 03:22:19 PM: Evaluate: task mnli, batch 202 (209): accuracy: 0.7941, mnli_loss: 0.5029
09/05 03:22:20 PM: Updating LR scheduler:
09/05 03:22:20 PM: 	Best result seen so far for macro_avg: 0.753
09/05 03:22:20 PM: 	# validation passes without improvement: 1
09/05 03:22:20 PM: copa_loss: training: 0.617845 validation: 0.644802
09/05 03:22:20 PM: mnli_loss: training: 0.509758 validation: 0.505601
09/05 03:22:20 PM: macro_avg: validation: 0.741100
09/05 03:22:20 PM: micro_avg: validation: 0.790196
09/05 03:22:20 PM: copa_accuracy: training: 0.708333 validation: 0.690000
09/05 03:22:20 PM: mnli_accuracy: training: 0.797887 validation: 0.792200
09/05 03:22:20 PM: Global learning rate: 1e-05
09/05 03:22:20 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 03:22:30 PM: Update 13036: task mnli, batch 36 (13019): accuracy: 0.8299, mnli_loss: 0.4357
09/05 03:22:40 PM: Update 13074: task mnli, batch 74 (13057): accuracy: 0.8069, mnli_loss: 0.5005
09/05 03:22:50 PM: Update 13113: task mnli, batch 113 (13096): accuracy: 0.7968, mnli_loss: 0.5145
09/05 03:23:00 PM: Update 13149: task mnli, batch 149 (13132): accuracy: 0.7967, mnli_loss: 0.5101
09/05 03:23:10 PM: Update 13188: task mnli, batch 188 (13171): accuracy: 0.7972, mnli_loss: 0.5131
09/05 03:23:20 PM: Update 13226: task mnli, batch 226 (13209): accuracy: 0.7939, mnli_loss: 0.5184
09/05 03:23:30 PM: Update 13264: task mnli, batch 264 (13247): accuracy: 0.7929, mnli_loss: 0.5233
09/05 03:23:41 PM: Update 13304: task mnli, batch 304 (13287): accuracy: 0.7941, mnli_loss: 0.5216
09/05 03:23:51 PM: Update 13342: task mnli, batch 342 (13325): accuracy: 0.7930, mnli_loss: 0.5258
09/05 03:24:01 PM: Update 13372: task mnli, batch 372 (13355): accuracy: 0.7926, mnli_loss: 0.5266
09/05 03:24:11 PM: Update 13410: task mnli, batch 410 (13393): accuracy: 0.7897, mnli_loss: 0.5285
09/05 03:24:21 PM: Update 13451: task mnli, batch 451 (13434): accuracy: 0.7907, mnli_loss: 0.5253
09/05 03:24:31 PM: Update 13489: task mnli, batch 489 (13472): accuracy: 0.7894, mnli_loss: 0.5273
09/05 03:24:41 PM: Update 13529: task mnli, batch 529 (13512): accuracy: 0.7919, mnli_loss: 0.5226
09/05 03:24:52 PM: Update 13567: task mnli, batch 567 (13550): accuracy: 0.7913, mnli_loss: 0.5244
09/05 03:25:02 PM: Update 13603: task mnli, batch 603 (13586): accuracy: 0.7898, mnli_loss: 0.5254
09/05 03:25:07 PM: Update 13623: task copa, batch 1 (18): accuracy: 0.5000, copa_loss: 0.6891
09/05 03:25:12 PM: Update 13642: task mnli, batch 641 (13624): accuracy: 0.7912, mnli_loss: 0.5216
09/05 03:25:22 PM: Update 13679: task mnli, batch 678 (13661): accuracy: 0.7927, mnli_loss: 0.5184
09/05 03:25:32 PM: Update 13717: task mnli, batch 716 (13699): accuracy: 0.7934, mnli_loss: 0.5178
09/05 03:25:42 PM: Update 13755: task mnli, batch 754 (13737): accuracy: 0.7937, mnli_loss: 0.5171
09/05 03:25:52 PM: Update 13787: task mnli, batch 786 (13769): accuracy: 0.7929, mnli_loss: 0.5193
09/05 03:26:03 PM: Update 13824: task mnli, batch 823 (13806): accuracy: 0.7923, mnli_loss: 0.5198
09/05 03:26:13 PM: Update 13861: task mnli, batch 860 (13843): accuracy: 0.7917, mnli_loss: 0.5199
09/05 03:26:23 PM: Update 13899: task mnli, batch 898 (13881): accuracy: 0.7910, mnli_loss: 0.5202
09/05 03:26:33 PM: Update 13938: task mnli, batch 937 (13920): accuracy: 0.7913, mnli_loss: 0.5193
09/05 03:26:43 PM: Update 13977: task mnli, batch 976 (13959): accuracy: 0.7906, mnli_loss: 0.5204
09/05 03:26:49 PM: ***** Step 14000 / Validation 14 *****
09/05 03:26:49 PM: copa: trained on 1 batches, 0.059 epochs
09/05 03:26:49 PM: mnli: trained on 999 batches, 0.061 epochs
09/05 03:26:49 PM: Validating...
09/05 03:26:49 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8750, copa_loss: 0.6195
09/05 03:26:53 PM: Evaluate: task mnli, batch 54 (209): accuracy: 0.8025, mnli_loss: 0.4806
09/05 03:27:03 PM: Evaluate: task mnli, batch 189 (209): accuracy: 0.7987, mnli_loss: 0.5035
09/05 03:27:05 PM: Best result seen so far for copa.
09/05 03:27:05 PM: Best result seen so far for macro.
09/05 03:27:05 PM: Updating LR scheduler:
09/05 03:27:05 PM: 	Best result seen so far for macro_avg: 0.763
09/05 03:27:05 PM: 	# validation passes without improvement: 0
09/05 03:27:05 PM: copa_loss: training: 0.689132 validation: 0.649749
09/05 03:27:05 PM: mnli_loss: training: 0.520642 validation: 0.507730
09/05 03:27:05 PM: macro_avg: validation: 0.763100
09/05 03:27:05 PM: micro_avg: validation: 0.794902
09/05 03:27:05 PM: copa_accuracy: training: 0.500000 validation: 0.730000
09/05 03:27:05 PM: mnli_accuracy: training: 0.790234 validation: 0.796200
09/05 03:27:05 PM: Global learning rate: 1e-05
09/05 03:27:05 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 03:27:13 PM: Update 14028: task mnli, batch 28 (14010): accuracy: 0.7827, mnli_loss: 0.5292
09/05 03:27:23 PM: Update 14065: task mnli, batch 65 (14047): accuracy: 0.8064, mnli_loss: 0.5226
09/05 03:27:33 PM: Update 14105: task mnli, batch 105 (14087): accuracy: 0.8071, mnli_loss: 0.5109
09/05 03:27:44 PM: Update 14144: task mnli, batch 144 (14126): accuracy: 0.7998, mnli_loss: 0.5298
09/05 03:27:54 PM: Update 14183: task mnli, batch 183 (14165): accuracy: 0.8003, mnli_loss: 0.5266
09/05 03:28:04 PM: Update 14212: task mnli, batch 212 (14194): accuracy: 0.8014, mnli_loss: 0.5227
09/05 03:28:13 PM: Update 14249: task copa, batch 1 (19): accuracy: 0.7500, copa_loss: 0.6169
09/05 03:28:14 PM: Update 14251: task mnli, batch 250 (14232): accuracy: 0.8016, mnli_loss: 0.5176
09/05 03:28:24 PM: Update 14288: task mnli, batch 287 (14269): accuracy: 0.8001, mnli_loss: 0.5177
09/05 03:28:34 PM: Update 14327: task mnli, batch 326 (14308): accuracy: 0.7990, mnli_loss: 0.5147
09/05 03:28:41 PM: Update 14353: task copa, batch 2 (20): accuracy: 0.7708, copa_loss: 0.6054
09/05 03:28:44 PM: Update 14366: task mnli, batch 364 (14346): accuracy: 0.7993, mnli_loss: 0.5127
09/05 03:28:55 PM: Update 14402: task mnli, batch 400 (14382): accuracy: 0.7977, mnli_loss: 0.5135
09/05 03:29:05 PM: Update 14441: task mnli, batch 439 (14421): accuracy: 0.7991, mnli_loss: 0.5090
09/05 03:29:15 PM: Update 14480: task mnli, batch 478 (14460): accuracy: 0.7991, mnli_loss: 0.5091
09/05 03:29:25 PM: Update 14520: task mnli, batch 518 (14500): accuracy: 0.7972, mnli_loss: 0.5127
09/05 03:29:35 PM: Update 14558: task mnli, batch 556 (14538): accuracy: 0.7966, mnli_loss: 0.5136
09/05 03:29:46 PM: Update 14595: task mnli, batch 593 (14575): accuracy: 0.7968, mnli_loss: 0.5125
09/05 03:29:56 PM: Update 14626: task mnli, batch 624 (14606): accuracy: 0.7973, mnli_loss: 0.5120
09/05 03:30:06 PM: Update 14665: task mnli, batch 663 (14645): accuracy: 0.7968, mnli_loss: 0.5114
09/05 03:30:16 PM: Update 14703: task mnli, batch 701 (14683): accuracy: 0.7963, mnli_loss: 0.5134
09/05 03:30:26 PM: Update 14741: task mnli, batch 739 (14721): accuracy: 0.7966, mnli_loss: 0.5124
09/05 03:30:36 PM: Update 14781: task mnli, batch 779 (14761): accuracy: 0.7974, mnli_loss: 0.5114
09/05 03:30:47 PM: Update 14818: task mnli, batch 816 (14798): accuracy: 0.7968, mnli_loss: 0.5120
09/05 03:30:57 PM: Update 14857: task mnli, batch 855 (14837): accuracy: 0.7967, mnli_loss: 0.5112
09/05 03:31:07 PM: Update 14895: task mnli, batch 893 (14875): accuracy: 0.7966, mnli_loss: 0.5109
09/05 03:31:17 PM: Update 14932: task mnli, batch 930 (14912): accuracy: 0.7969, mnli_loss: 0.5095
09/05 03:31:27 PM: Update 14971: task mnli, batch 969 (14951): accuracy: 0.7976, mnli_loss: 0.5091
09/05 03:31:35 PM: ***** Step 15000 / Validation 15 *****
09/05 03:31:35 PM: copa: trained on 2 batches, 0.118 epochs
09/05 03:31:35 PM: mnli: trained on 998 batches, 0.061 epochs
09/05 03:31:35 PM: Validating...
09/05 03:31:35 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8750, copa_loss: 0.6004
09/05 03:31:37 PM: Evaluate: task mnli, batch 27 (209): accuracy: 0.8056, mnli_loss: 0.4824
09/05 03:31:47 PM: Evaluate: task mnli, batch 163 (209): accuracy: 0.8090, mnli_loss: 0.4877
09/05 03:31:50 PM: Best result seen so far for mnli.
09/05 03:31:50 PM: Best result seen so far for micro.
09/05 03:31:50 PM: Best result seen so far for macro.
09/05 03:31:50 PM: Updating LR scheduler:
09/05 03:31:50 PM: 	Best result seen so far for macro_avg: 0.764
09/05 03:31:50 PM: 	# validation passes without improvement: 0
09/05 03:31:50 PM: copa_loss: training: 0.605383 validation: 0.643477
09/05 03:31:50 PM: mnli_loss: training: 0.509295 validation: 0.491260
09/05 03:31:50 PM: macro_avg: validation: 0.763800
09/05 03:31:50 PM: micro_avg: validation: 0.805882
09/05 03:31:50 PM: copa_accuracy: training: 0.770833 validation: 0.720000
09/05 03:31:50 PM: mnli_accuracy: training: 0.797752 validation: 0.807600
09/05 03:31:50 PM: Global learning rate: 1e-05
09/05 03:31:50 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 03:31:57 PM: Update 15023: task mnli, batch 23 (15003): accuracy: 0.8152, mnli_loss: 0.4995
09/05 03:32:07 PM: Update 15053: task mnli, batch 53 (15033): accuracy: 0.7967, mnli_loss: 0.5174
09/05 03:32:17 PM: Update 15092: task mnli, batch 92 (15072): accuracy: 0.7964, mnli_loss: 0.5143
09/05 03:32:28 PM: Update 15129: task mnli, batch 129 (15109): accuracy: 0.7921, mnli_loss: 0.5193
09/05 03:32:38 PM: Update 15169: task mnli, batch 169 (15149): accuracy: 0.7989, mnli_loss: 0.5075
09/05 03:32:48 PM: Update 15207: task mnli, batch 207 (15187): accuracy: 0.7994, mnli_loss: 0.5065
09/05 03:32:58 PM: Update 15245: task mnli, batch 245 (15225): accuracy: 0.7926, mnli_loss: 0.5166
09/05 03:33:08 PM: Update 15284: task mnli, batch 284 (15264): accuracy: 0.7929, mnli_loss: 0.5179
09/05 03:33:18 PM: Update 15322: task mnli, batch 322 (15302): accuracy: 0.7937, mnli_loss: 0.5146
09/05 03:33:28 PM: Update 15360: task mnli, batch 360 (15340): accuracy: 0.7924, mnli_loss: 0.5186
09/05 03:33:38 PM: Update 15398: task mnli, batch 398 (15378): accuracy: 0.7918, mnli_loss: 0.5197
09/05 03:33:49 PM: Update 15438: task mnli, batch 438 (15418): accuracy: 0.7925, mnli_loss: 0.5171
09/05 03:33:59 PM: Update 15468: task mnli, batch 468 (15448): accuracy: 0.7910, mnli_loss: 0.5193
09/05 03:34:09 PM: Update 15506: task mnli, batch 506 (15486): accuracy: 0.7913, mnli_loss: 0.5189
09/05 03:34:19 PM: Update 15544: task mnli, batch 544 (15524): accuracy: 0.7917, mnli_loss: 0.5165
09/05 03:34:29 PM: Update 15583: task mnli, batch 583 (15563): accuracy: 0.7929, mnli_loss: 0.5149
09/05 03:34:39 PM: Update 15621: task mnli, batch 621 (15601): accuracy: 0.7920, mnli_loss: 0.5152
09/05 03:34:50 PM: Update 15661: task mnli, batch 661 (15641): accuracy: 0.7928, mnli_loss: 0.5117
09/05 03:35:00 PM: Update 15700: task mnli, batch 700 (15680): accuracy: 0.7927, mnli_loss: 0.5113
09/05 03:35:10 PM: Update 15738: task mnli, batch 738 (15718): accuracy: 0.7935, mnli_loss: 0.5106
09/05 03:35:20 PM: Update 15775: task mnli, batch 775 (15755): accuracy: 0.7932, mnli_loss: 0.5106
09/05 03:35:30 PM: Update 15814: task mnli, batch 814 (15794): accuracy: 0.7929, mnli_loss: 0.5108
09/05 03:35:40 PM: Update 15852: task mnli, batch 852 (15832): accuracy: 0.7935, mnli_loss: 0.5096
09/05 03:35:51 PM: Update 15880: task mnli, batch 880 (15860): accuracy: 0.7938, mnli_loss: 0.5103
09/05 03:36:01 PM: Update 15917: task mnli, batch 917 (15897): accuracy: 0.7930, mnli_loss: 0.5124
09/05 03:36:11 PM: Update 15955: task mnli, batch 955 (15935): accuracy: 0.7929, mnli_loss: 0.5128
09/05 03:36:21 PM: Update 15994: task mnli, batch 994 (15974): accuracy: 0.7927, mnli_loss: 0.5133
09/05 03:36:23 PM: ***** Step 16000 / Validation 16 *****
09/05 03:36:23 PM: copa: trained on 0 batches, 0.000 epochs
09/05 03:36:23 PM: mnli: trained on 1000 batches, 0.061 epochs
09/05 03:36:23 PM: Validating...
09/05 03:36:23 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8333, copa_loss: 0.6014
09/05 03:36:31 PM: Evaluate: task mnli, batch 109 (209): accuracy: 0.8100, mnli_loss: 0.4778
09/05 03:36:38 PM: Updating LR scheduler:
09/05 03:36:38 PM: 	Best result seen so far for macro_avg: 0.764
09/05 03:36:38 PM: 	# validation passes without improvement: 1
09/05 03:36:38 PM: copa_loss: training: 0.000000 validation: 0.649861
09/05 03:36:38 PM: mnli_loss: training: 0.513267 validation: 0.489431
09/05 03:36:38 PM: macro_avg: validation: 0.752500
09/05 03:36:38 PM: micro_avg: validation: 0.802941
09/05 03:36:38 PM: copa_accuracy: validation: 0.700000
09/05 03:36:38 PM: mnli_accuracy: training: 0.792626 validation: 0.805000
09/05 03:36:38 PM: Global learning rate: 1e-05
09/05 03:36:38 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 03:36:41 PM: Update 16008: task mnli, batch 8 (15988): accuracy: 0.7865, mnli_loss: 0.5173
09/05 03:36:51 PM: Update 16047: task mnli, batch 47 (16027): accuracy: 0.8112, mnli_loss: 0.4909
09/05 03:37:01 PM: Update 16088: task mnli, batch 88 (16068): accuracy: 0.7983, mnli_loss: 0.5074
09/05 03:37:12 PM: Update 16127: task mnli, batch 127 (16107): accuracy: 0.7972, mnli_loss: 0.5084
09/05 03:37:22 PM: Update 16166: task mnli, batch 166 (16146): accuracy: 0.8015, mnli_loss: 0.5015
09/05 03:37:32 PM: Update 16204: task mnli, batch 204 (16184): accuracy: 0.8029, mnli_loss: 0.4963
09/05 03:37:42 PM: Update 16242: task mnli, batch 242 (16222): accuracy: 0.8037, mnli_loss: 0.4961
09/05 03:37:52 PM: Update 16278: task mnli, batch 278 (16258): accuracy: 0.8020, mnli_loss: 0.4995
09/05 03:38:02 PM: Update 16314: task mnli, batch 314 (16294): accuracy: 0.7982, mnli_loss: 0.5043
09/05 03:38:12 PM: Update 16354: task mnli, batch 354 (16334): accuracy: 0.7966, mnli_loss: 0.5061
09/05 03:38:23 PM: Update 16392: task mnli, batch 392 (16372): accuracy: 0.7975, mnli_loss: 0.5018
09/05 03:38:33 PM: Update 16423: task mnli, batch 423 (16403): accuracy: 0.7988, mnli_loss: 0.4999
09/05 03:38:43 PM: Update 16461: task mnli, batch 461 (16441): accuracy: 0.7979, mnli_loss: 0.5021
09/05 03:38:53 PM: Update 16500: task mnli, batch 500 (16480): accuracy: 0.7982, mnli_loss: 0.5027
09/05 03:39:03 PM: Update 16538: task mnli, batch 538 (16518): accuracy: 0.7989, mnli_loss: 0.5029
09/05 03:39:13 PM: Update 16575: task mnli, batch 575 (16555): accuracy: 0.7991, mnli_loss: 0.5047
09/05 03:39:23 PM: Update 16614: task mnli, batch 614 (16594): accuracy: 0.8015, mnli_loss: 0.4996
09/05 03:39:33 PM: Update 16651: task mnli, batch 651 (16631): accuracy: 0.8024, mnli_loss: 0.4987
09/05 03:39:43 PM: Update 16690: task mnli, batch 690 (16670): accuracy: 0.8025, mnli_loss: 0.4984
09/05 03:39:53 PM: Update 16729: task mnli, batch 729 (16709): accuracy: 0.8028, mnli_loss: 0.4971
09/05 03:40:04 PM: Update 16767: task mnli, batch 767 (16747): accuracy: 0.8032, mnli_loss: 0.4959
09/05 03:40:14 PM: Update 16805: task mnli, batch 805 (16785): accuracy: 0.8033, mnli_loss: 0.4941
09/05 03:40:24 PM: Update 16837: task mnli, batch 837 (16817): accuracy: 0.8040, mnli_loss: 0.4930
09/05 03:40:34 PM: Update 16875: task mnli, batch 875 (16855): accuracy: 0.8052, mnli_loss: 0.4910
09/05 03:40:44 PM: Update 16916: task mnli, batch 916 (16896): accuracy: 0.8063, mnli_loss: 0.4881
09/05 03:40:55 PM: Update 16952: task mnli, batch 952 (16932): accuracy: 0.8054, mnli_loss: 0.4889
09/05 03:41:05 PM: Update 16989: task mnli, batch 989 (16969): accuracy: 0.8049, mnli_loss: 0.4897
09/05 03:41:08 PM: ***** Step 17000 / Validation 17 *****
09/05 03:41:08 PM: copa: trained on 0 batches, 0.000 epochs
09/05 03:41:08 PM: mnli: trained on 1000 batches, 0.061 epochs
09/05 03:41:08 PM: Validating...
09/05 03:41:08 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8750, copa_loss: 0.5885
09/05 03:41:15 PM: Evaluate: task mnli, batch 90 (209): accuracy: 0.8181, mnli_loss: 0.4637
09/05 03:41:23 PM: Best result seen so far for mnli.
09/05 03:41:23 PM: Best result seen so far for micro.
09/05 03:41:23 PM: Updating LR scheduler:
09/05 03:41:23 PM: 	Best result seen so far for macro_avg: 0.764
09/05 03:41:23 PM: 	# validation passes without improvement: 2
09/05 03:41:23 PM: copa_loss: training: 0.000000 validation: 0.634322
09/05 03:41:23 PM: mnli_loss: training: 0.489334 validation: 0.474637
09/05 03:41:23 PM: macro_avg: validation: 0.756000
09/05 03:41:23 PM: micro_avg: validation: 0.809804
09/05 03:41:23 PM: copa_accuracy: validation: 0.700000
09/05 03:41:23 PM: mnli_accuracy: training: 0.805372 validation: 0.812000
09/05 03:41:23 PM: Global learning rate: 1e-05
09/05 03:41:23 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 03:41:25 PM: Update 17002: task mnli, batch 2 (16982): accuracy: 0.8958, mnli_loss: 0.3723
09/05 03:41:35 PM: Update 17041: task mnli, batch 41 (17021): accuracy: 0.8252, mnli_loss: 0.4611
09/05 03:41:45 PM: Update 17081: task mnli, batch 81 (17061): accuracy: 0.8148, mnli_loss: 0.4826
09/05 03:41:55 PM: Update 17118: task mnli, batch 118 (17098): accuracy: 0.8090, mnli_loss: 0.4867
09/05 03:42:06 PM: Update 17157: task mnli, batch 157 (17137): accuracy: 0.8092, mnli_loss: 0.4928
09/05 03:42:16 PM: Update 17197: task mnli, batch 197 (17177): accuracy: 0.8130, mnli_loss: 0.4836
09/05 03:42:26 PM: Update 17231: task mnli, batch 231 (17211): accuracy: 0.8129, mnli_loss: 0.4854
09/05 03:42:36 PM: Update 17269: task mnli, batch 269 (17249): accuracy: 0.8096, mnli_loss: 0.4862
09/05 03:42:47 PM: Update 17308: task mnli, batch 308 (17288): accuracy: 0.8112, mnli_loss: 0.4826
09/05 03:42:53 PM: Update 17332: task copa, batch 1 (21): accuracy: 0.7500, copa_loss: 0.6326
09/05 03:42:57 PM: Update 17346: task mnli, batch 345 (17325): accuracy: 0.8090, mnli_loss: 0.4882
09/05 03:43:07 PM: Update 17385: task mnli, batch 384 (17364): accuracy: 0.8082, mnli_loss: 0.4906
09/05 03:43:17 PM: Update 17422: task mnli, batch 421 (17401): accuracy: 0.8087, mnli_loss: 0.4896
09/05 03:43:27 PM: Update 17460: task mnli, batch 459 (17439): accuracy: 0.8102, mnli_loss: 0.4865
09/05 03:43:37 PM: Update 17499: task mnli, batch 498 (17478): accuracy: 0.8101, mnli_loss: 0.4871
09/05 03:43:47 PM: Update 17539: task mnli, batch 538 (17518): accuracy: 0.8104, mnli_loss: 0.4865
09/05 03:43:57 PM: Update 17577: task mnli, batch 576 (17556): accuracy: 0.8120, mnli_loss: 0.4839
09/05 03:44:07 PM: Update 17615: task mnli, batch 614 (17594): accuracy: 0.8118, mnli_loss: 0.4840
09/05 03:44:19 PM: Update 17649: task mnli, batch 648 (17628): accuracy: 0.8111, mnli_loss: 0.4849
09/05 03:44:29 PM: Update 17687: task mnli, batch 686 (17666): accuracy: 0.8110, mnli_loss: 0.4851
09/05 03:44:39 PM: Update 17727: task mnli, batch 726 (17706): accuracy: 0.8107, mnli_loss: 0.4856
09/05 03:44:49 PM: Update 17765: task mnli, batch 764 (17744): accuracy: 0.8105, mnli_loss: 0.4845
09/05 03:44:59 PM: Update 17803: task mnli, batch 802 (17782): accuracy: 0.8108, mnli_loss: 0.4838
09/05 03:45:10 PM: Update 17842: task mnli, batch 841 (17821): accuracy: 0.8104, mnli_loss: 0.4844
09/05 03:45:20 PM: Update 17880: task mnli, batch 879 (17859): accuracy: 0.8111, mnli_loss: 0.4832
09/05 03:45:30 PM: Update 17918: task mnli, batch 917 (17897): accuracy: 0.8117, mnli_loss: 0.4820
09/05 03:45:40 PM: Update 17958: task mnli, batch 957 (17937): accuracy: 0.8120, mnli_loss: 0.4826
09/05 03:45:50 PM: Update 17995: task mnli, batch 994 (17974): accuracy: 0.8121, mnli_loss: 0.4821
09/05 03:45:51 PM: ***** Step 18000 / Validation 18 *****
09/05 03:45:51 PM: copa: trained on 1 batches, 0.059 epochs
09/05 03:45:51 PM: mnli: trained on 999 batches, 0.061 epochs
09/05 03:45:51 PM: Validating...
09/05 03:45:52 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8333, copa_loss: 0.5804
09/05 03:46:00 PM: Evaluate: task mnli, batch 116 (209): accuracy: 0.8139, mnli_loss: 0.4826
09/05 03:46:07 PM: Updating LR scheduler:
09/05 03:46:07 PM: 	Best result seen so far for macro_avg: 0.764
09/05 03:46:07 PM: 	# validation passes without improvement: 3
09/05 03:46:07 PM: copa_loss: training: 0.632574 validation: 0.633742
09/05 03:46:07 PM: mnli_loss: training: 0.482090 validation: 0.490960
09/05 03:46:07 PM: macro_avg: validation: 0.749800
09/05 03:46:07 PM: micro_avg: validation: 0.807255
09/05 03:46:07 PM: copa_accuracy: training: 0.750000 validation: 0.690000
09/05 03:46:07 PM: mnli_accuracy: training: 0.811937 validation: 0.809600
09/05 03:46:07 PM: Global learning rate: 1e-05
09/05 03:46:07 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 03:46:10 PM: Update 18009: task mnli, batch 9 (17988): accuracy: 0.8796, mnli_loss: 0.3583
09/05 03:46:20 PM: Update 18047: task mnli, batch 47 (18026): accuracy: 0.8032, mnli_loss: 0.4750
09/05 03:46:31 PM: Update 18079: task mnli, batch 79 (18058): accuracy: 0.8077, mnli_loss: 0.4814
09/05 03:46:41 PM: Update 18116: task mnli, batch 116 (18095): accuracy: 0.8080, mnli_loss: 0.4834
09/05 03:46:51 PM: Update 18152: task mnli, batch 152 (18131): accuracy: 0.8082, mnli_loss: 0.4850
09/05 03:47:01 PM: Update 18192: task mnli, batch 192 (18171): accuracy: 0.8089, mnli_loss: 0.4792
09/05 03:47:11 PM: Update 18229: task mnli, batch 229 (18208): accuracy: 0.8118, mnli_loss: 0.4779
09/05 03:47:21 PM: Update 18266: task mnli, batch 266 (18245): accuracy: 0.8124, mnli_loss: 0.4775
09/05 03:47:31 PM: Update 18305: task mnli, batch 305 (18284): accuracy: 0.8121, mnli_loss: 0.4748
09/05 03:47:33 PM: Update 18314: task copa, batch 1 (22): accuracy: 0.5417, copa_loss: 0.6789
09/05 03:47:41 PM: Update 18343: task mnli, batch 341 (18320): accuracy: 0.8126, mnli_loss: 0.4748
09/05 03:47:52 PM: Update 18381: task mnli, batch 379 (18358): accuracy: 0.8112, mnli_loss: 0.4772
09/05 03:48:02 PM: Update 18422: task mnli, batch 420 (18399): accuracy: 0.8122, mnli_loss: 0.4768
09/05 03:48:12 PM: Update 18462: task mnli, batch 460 (18439): accuracy: 0.8115, mnli_loss: 0.4769
09/05 03:48:22 PM: Update 18493: task mnli, batch 491 (18470): accuracy: 0.8121, mnli_loss: 0.4762
09/05 03:48:32 PM: Update 18532: task mnli, batch 530 (18509): accuracy: 0.8125, mnli_loss: 0.4753
09/05 03:48:42 PM: Update 18572: task mnli, batch 570 (18549): accuracy: 0.8135, mnli_loss: 0.4726
09/05 03:48:52 PM: Update 18611: task mnli, batch 609 (18588): accuracy: 0.8141, mnli_loss: 0.4722
09/05 03:49:03 PM: Update 18647: task mnli, batch 645 (18624): accuracy: 0.8136, mnli_loss: 0.4726
09/05 03:49:13 PM: Update 18684: task mnli, batch 682 (18661): accuracy: 0.8132, mnli_loss: 0.4741
09/05 03:49:18 PM: Update 18705: task copa, batch 3 (24): accuracy: 0.6528, copa_loss: 0.6516
09/05 03:49:23 PM: Update 18723: task mnli, batch 720 (18699): accuracy: 0.8141, mnli_loss: 0.4720
09/05 03:49:33 PM: Update 18762: task mnli, batch 759 (18738): accuracy: 0.8132, mnli_loss: 0.4724
09/05 03:49:38 PM: Update 18779: task copa, batch 4 (25): accuracy: 0.6771, copa_loss: 0.6392
09/05 03:49:43 PM: Update 18799: task mnli, batch 795 (18774): accuracy: 0.8129, mnli_loss: 0.4724
09/05 03:49:53 PM: Update 18839: task mnli, batch 835 (18814): accuracy: 0.8135, mnli_loss: 0.4705
09/05 03:50:04 PM: Update 18879: task mnli, batch 875 (18854): accuracy: 0.8141, mnli_loss: 0.4699
09/05 03:50:14 PM: Update 18909: task mnli, batch 905 (18884): accuracy: 0.8138, mnli_loss: 0.4710
09/05 03:50:24 PM: Update 18948: task mnli, batch 944 (18923): accuracy: 0.8141, mnli_loss: 0.4707
09/05 03:50:34 PM: Update 18987: task mnli, batch 983 (18962): accuracy: 0.8148, mnli_loss: 0.4695
09/05 03:50:38 PM: ***** Step 19000 / Validation 19 *****
09/05 03:50:38 PM: copa: trained on 4 batches, 0.235 epochs
09/05 03:50:38 PM: mnli: trained on 996 batches, 0.061 epochs
09/05 03:50:38 PM: Validating...
09/05 03:50:38 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8333, copa_loss: 0.5665
09/05 03:50:44 PM: Evaluate: task mnli, batch 83 (209): accuracy: 0.8248, mnli_loss: 0.4590
09/05 03:50:53 PM: Best result seen so far for mnli.
09/05 03:50:53 PM: Best result seen so far for micro.
09/05 03:50:53 PM: Updating LR scheduler:
09/05 03:50:53 PM: 	Best result seen so far for macro_avg: 0.764
09/05 03:50:53 PM: 	# validation passes without improvement: 4
09/05 03:50:53 PM: copa_loss: training: 0.639236 validation: 0.618365
09/05 03:50:53 PM: mnli_loss: training: 0.468995 validation: 0.482287
09/05 03:50:53 PM: macro_avg: validation: 0.756700
09/05 03:50:53 PM: micro_avg: validation: 0.811176
09/05 03:50:53 PM: copa_accuracy: training: 0.677083 validation: 0.700000
09/05 03:50:53 PM: mnli_accuracy: training: 0.814908 validation: 0.813400
09/05 03:50:53 PM: Global learning rate: 1e-05
09/05 03:50:53 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 03:50:54 PM: Update 19001: task mnli, batch 1 (18976): accuracy: 0.9583, mnli_loss: 0.1167
09/05 03:51:04 PM: Update 19038: task mnli, batch 38 (19013): accuracy: 0.7982, mnli_loss: 0.5025
09/05 03:51:15 PM: Update 19076: task mnli, batch 76 (19051): accuracy: 0.8026, mnli_loss: 0.4924
09/05 03:51:25 PM: Update 19114: task mnli, batch 114 (19089): accuracy: 0.8110, mnli_loss: 0.4702
09/05 03:51:35 PM: Update 19152: task mnli, batch 152 (19127): accuracy: 0.8092, mnli_loss: 0.4708
09/05 03:51:45 PM: Update 19193: task mnli, batch 193 (19168): accuracy: 0.8143, mnli_loss: 0.4617
09/05 03:51:56 PM: Update 19233: task mnli, batch 233 (19208): accuracy: 0.8146, mnli_loss: 0.4604
09/05 03:52:06 PM: Update 19272: task mnli, batch 272 (19247): accuracy: 0.8165, mnli_loss: 0.4605
09/05 03:52:16 PM: Update 19309: task mnli, batch 309 (19284): accuracy: 0.8150, mnli_loss: 0.4638
09/05 03:52:22 PM: Update 19325: task copa, batch 1 (26): accuracy: 0.8333, copa_loss: 0.5240
09/05 03:52:26 PM: Update 19339: task mnli, batch 338 (19313): accuracy: 0.8154, mnli_loss: 0.4614
09/05 03:52:36 PM: Update 19379: task mnli, batch 378 (19353): accuracy: 0.8160, mnli_loss: 0.4586
09/05 03:52:46 PM: Update 19418: task mnli, batch 417 (19392): accuracy: 0.8180, mnli_loss: 0.4574
09/05 03:52:56 PM: Update 19455: task mnli, batch 454 (19429): accuracy: 0.8173, mnli_loss: 0.4575
09/05 03:53:06 PM: Update 19493: task mnli, batch 492 (19467): accuracy: 0.8169, mnli_loss: 0.4608
09/05 03:53:16 PM: Update 19532: task mnli, batch 531 (19506): accuracy: 0.8176, mnli_loss: 0.4587
09/05 03:53:26 PM: Update 19569: task mnli, batch 568 (19543): accuracy: 0.8183, mnli_loss: 0.4568
09/05 03:53:31 PM: Update 19587: task copa, batch 2 (27): accuracy: 0.7500, copa_loss: 0.5683
09/05 03:53:37 PM: Update 19607: task mnli, batch 605 (19580): accuracy: 0.8168, mnli_loss: 0.4589
09/05 03:53:47 PM: Update 19646: task mnli, batch 644 (19619): accuracy: 0.8169, mnli_loss: 0.4591
09/05 03:53:57 PM: Update 19684: task mnli, batch 682 (19657): accuracy: 0.8184, mnli_loss: 0.4558
09/05 03:54:07 PM: Update 19722: task mnli, batch 720 (19695): accuracy: 0.8191, mnli_loss: 0.4554
09/05 03:54:17 PM: Update 19749: task mnli, batch 747 (19722): accuracy: 0.8189, mnli_loss: 0.4567
09/05 03:54:27 PM: Update 19790: task mnli, batch 788 (19763): accuracy: 0.8186, mnli_loss: 0.4573
09/05 03:54:37 PM: Update 19829: task mnli, batch 827 (19802): accuracy: 0.8190, mnli_loss: 0.4561
09/05 03:54:48 PM: Update 19867: task mnli, batch 865 (19840): accuracy: 0.8199, mnli_loss: 0.4547
09/05 03:54:58 PM: Update 19903: task mnli, batch 901 (19876): accuracy: 0.8194, mnli_loss: 0.4556
09/05 03:55:08 PM: Update 19944: task mnli, batch 942 (19917): accuracy: 0.8196, mnli_loss: 0.4556
09/05 03:55:18 PM: Update 19981: task mnli, batch 979 (19954): accuracy: 0.8183, mnli_loss: 0.4579
09/05 03:55:23 PM: ***** Step 20000 / Validation 20 *****
09/05 03:55:23 PM: copa: trained on 2 batches, 0.118 epochs
09/05 03:55:23 PM: mnli: trained on 998 batches, 0.061 epochs
09/05 03:55:23 PM: Validating...
09/05 03:55:23 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8333, copa_loss: 0.5738
09/05 03:55:28 PM: Evaluate: task mnli, batch 60 (209): accuracy: 0.8229, mnli_loss: 0.4603
09/05 03:55:38 PM: Evaluate: task mnli, batch 196 (209): accuracy: 0.8185, mnli_loss: 0.4657
09/05 03:55:39 PM: Best result seen so far for mnli.
09/05 03:55:39 PM: Best result seen so far for micro.
09/05 03:55:39 PM: Best result seen so far for macro.
09/05 03:55:39 PM: Updating LR scheduler:
09/05 03:55:39 PM: 	Best result seen so far for macro_avg: 0.772
09/05 03:55:39 PM: 	# validation passes without improvement: 0
09/05 03:55:39 PM: copa_loss: training: 0.568266 validation: 0.622441
09/05 03:55:39 PM: mnli_loss: training: 0.458077 validation: 0.472788
09/05 03:55:39 PM: macro_avg: validation: 0.772400
09/05 03:55:39 PM: micro_avg: validation: 0.813137
09/05 03:55:39 PM: copa_accuracy: training: 0.750000 validation: 0.730000
09/05 03:55:39 PM: mnli_accuracy: training: 0.818056 validation: 0.814800
09/05 03:55:39 PM: Global learning rate: 1e-05
09/05 03:55:39 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 03:55:48 PM: Update 20031: task mnli, batch 31 (20004): accuracy: 0.7890, mnli_loss: 0.5126
09/05 03:55:58 PM: Update 20069: task mnli, batch 69 (20042): accuracy: 0.8128, mnli_loss: 0.4670
09/05 03:56:08 PM: Update 20108: task mnli, batch 108 (20081): accuracy: 0.8152, mnli_loss: 0.4639
09/05 03:56:18 PM: Update 20146: task mnli, batch 146 (20119): accuracy: 0.8185, mnli_loss: 0.4573
09/05 03:56:28 PM: Update 20178: task mnli, batch 178 (20151): accuracy: 0.8194, mnli_loss: 0.4582
09/05 03:56:38 PM: Update 20216: task mnli, batch 216 (20189): accuracy: 0.8194, mnli_loss: 0.4578
09/05 03:56:48 PM: Update 20255: task mnli, batch 255 (20228): accuracy: 0.8200, mnli_loss: 0.4575
09/05 03:56:58 PM: Update 20292: task mnli, batch 292 (20265): accuracy: 0.8243, mnli_loss: 0.4485
09/05 03:57:09 PM: Update 20330: task mnli, batch 330 (20303): accuracy: 0.8271, mnli_loss: 0.4456
09/05 03:57:19 PM: Update 20369: task mnli, batch 369 (20342): accuracy: 0.8280, mnli_loss: 0.4461
09/05 03:57:29 PM: Update 20408: task mnli, batch 408 (20381): accuracy: 0.8288, mnli_loss: 0.4457
09/05 03:57:39 PM: Update 20446: task mnli, batch 446 (20419): accuracy: 0.8282, mnli_loss: 0.4459
09/05 03:57:49 PM: Update 20485: task mnli, batch 485 (20458): accuracy: 0.8292, mnli_loss: 0.4419
09/05 03:57:59 PM: Update 20524: task mnli, batch 524 (20497): accuracy: 0.8285, mnli_loss: 0.4423
09/05 03:58:09 PM: Update 20561: task mnli, batch 561 (20534): accuracy: 0.8282, mnli_loss: 0.4416
09/05 03:58:20 PM: Update 20594: task mnli, batch 594 (20567): accuracy: 0.8307, mnli_loss: 0.4363
09/05 03:58:30 PM: Update 20633: task mnli, batch 633 (20606): accuracy: 0.8314, mnli_loss: 0.4359
09/05 03:58:40 PM: Update 20672: task mnli, batch 672 (20645): accuracy: 0.8322, mnli_loss: 0.4356
09/05 03:58:50 PM: Update 20710: task mnli, batch 710 (20683): accuracy: 0.8327, mnli_loss: 0.4353
09/05 03:59:00 PM: Update 20749: task mnli, batch 749 (20722): accuracy: 0.8336, mnli_loss: 0.4335
09/05 03:59:10 PM: Update 20785: task mnli, batch 785 (20758): accuracy: 0.8336, mnli_loss: 0.4326
09/05 03:59:20 PM: Update 20823: task mnli, batch 823 (20796): accuracy: 0.8337, mnli_loss: 0.4328
09/05 03:59:31 PM: Update 20861: task mnli, batch 861 (20834): accuracy: 0.8334, mnli_loss: 0.4332
09/05 03:59:41 PM: Update 20900: task mnli, batch 900 (20873): accuracy: 0.8330, mnli_loss: 0.4344
09/05 03:59:51 PM: Update 20938: task mnli, batch 938 (20911): accuracy: 0.8330, mnli_loss: 0.4355
09/05 04:00:01 PM: Update 20975: task mnli, batch 975 (20948): accuracy: 0.8328, mnli_loss: 0.4355
09/05 04:00:09 PM: ***** Step 21000 / Validation 21 *****
09/05 04:00:09 PM: copa: trained on 0 batches, 0.000 epochs
09/05 04:00:09 PM: mnli: trained on 1000 batches, 0.061 epochs
09/05 04:00:09 PM: Validating...
09/05 04:00:10 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.7917, copa_loss: 0.5634
09/05 04:00:11 PM: Evaluate: task mnli, batch 15 (209): accuracy: 0.8194, mnli_loss: 0.4876
09/05 04:00:21 PM: Evaluate: task mnli, batch 152 (209): accuracy: 0.8163, mnli_loss: 0.4859
09/05 04:00:25 PM: Best result seen so far for copa.
09/05 04:00:25 PM: Best result seen so far for macro.
09/05 04:00:25 PM: Updating LR scheduler:
09/05 04:00:25 PM: 	Best result seen so far for macro_avg: 0.792
09/05 04:00:25 PM: 	# validation passes without improvement: 0
09/05 04:00:25 PM: copa_loss: training: 0.000000 validation: 0.629160
09/05 04:00:25 PM: mnli_loss: training: 0.435576 validation: 0.491770
09/05 04:00:25 PM: macro_avg: validation: 0.791900
09/05 04:00:25 PM: micro_avg: validation: 0.812941
09/05 04:00:25 PM: copa_accuracy: validation: 0.770000
09/05 04:00:25 PM: mnli_accuracy: training: 0.832791 validation: 0.813800
09/05 04:00:25 PM: Global learning rate: 1e-05
09/05 04:00:25 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 04:00:31 PM: Update 21018: task mnli, batch 18 (20991): accuracy: 0.8287, mnli_loss: 0.4333
09/05 04:00:41 PM: Update 21055: task mnli, batch 55 (21028): accuracy: 0.8189, mnli_loss: 0.4555
09/05 04:00:52 PM: Update 21095: task mnli, batch 95 (21068): accuracy: 0.8189, mnli_loss: 0.4591
09/05 04:01:02 PM: Update 21133: task mnli, batch 133 (21106): accuracy: 0.8202, mnli_loss: 0.4537
09/05 04:01:12 PM: Update 21173: task mnli, batch 173 (21146): accuracy: 0.8242, mnli_loss: 0.4466
09/05 04:01:22 PM: Update 21212: task mnli, batch 212 (21185): accuracy: 0.8298, mnli_loss: 0.4375
09/05 04:01:32 PM: Update 21250: task mnli, batch 250 (21223): accuracy: 0.8305, mnli_loss: 0.4377
09/05 04:01:42 PM: Update 21289: task mnli, batch 289 (21262): accuracy: 0.8312, mnli_loss: 0.4377
09/05 04:01:52 PM: Update 21327: task mnli, batch 327 (21300): accuracy: 0.8318, mnli_loss: 0.4370
09/05 04:01:58 PM: Update 21349: task copa, batch 1 (28): accuracy: 0.5833, copa_loss: 0.6390
09/05 04:02:02 PM: Update 21365: task mnli, batch 364 (21337): accuracy: 0.8328, mnli_loss: 0.4322
09/05 04:02:12 PM: Update 21403: task mnli, batch 402 (21375): accuracy: 0.8343, mnli_loss: 0.4321
09/05 04:02:23 PM: Update 21431: task mnli, batch 430 (21403): accuracy: 0.8345, mnli_loss: 0.4309
09/05 04:02:33 PM: Update 21469: task mnli, batch 468 (21441): accuracy: 0.8356, mnli_loss: 0.4293
09/05 04:02:43 PM: Update 21506: task mnli, batch 505 (21478): accuracy: 0.8346, mnli_loss: 0.4302
09/05 04:02:53 PM: Update 21543: task mnli, batch 542 (21515): accuracy: 0.8352, mnli_loss: 0.4288
09/05 04:03:03 PM: Update 21581: task mnli, batch 580 (21553): accuracy: 0.8350, mnli_loss: 0.4282
09/05 04:03:13 PM: Update 21620: task mnli, batch 619 (21592): accuracy: 0.8357, mnli_loss: 0.4267
09/05 04:03:23 PM: Update 21659: task mnli, batch 658 (21631): accuracy: 0.8371, mnli_loss: 0.4234
09/05 04:03:33 PM: Update 21698: task mnli, batch 697 (21670): accuracy: 0.8380, mnli_loss: 0.4216
09/05 04:03:43 PM: Update 21737: task mnli, batch 736 (21709): accuracy: 0.8379, mnli_loss: 0.4202
09/05 04:03:53 PM: Update 21776: task mnli, batch 775 (21748): accuracy: 0.8388, mnli_loss: 0.4187
09/05 04:04:04 PM: Update 21814: task mnli, batch 813 (21786): accuracy: 0.8387, mnli_loss: 0.4200
09/05 04:04:14 PM: Update 21844: task mnli, batch 843 (21816): accuracy: 0.8375, mnli_loss: 0.4225
09/05 04:04:24 PM: Update 21883: task mnli, batch 882 (21855): accuracy: 0.8381, mnli_loss: 0.4205
09/05 04:04:34 PM: Update 21922: task mnli, batch 921 (21894): accuracy: 0.8379, mnli_loss: 0.4213
09/05 04:04:44 PM: Update 21960: task mnli, batch 959 (21932): accuracy: 0.8381, mnli_loss: 0.4213
09/05 04:04:54 PM: Update 21999: task mnli, batch 998 (21971): accuracy: 0.8381, mnli_loss: 0.4211
09/05 04:04:55 PM: ***** Step 22000 / Validation 22 *****
09/05 04:04:55 PM: copa: trained on 1 batches, 0.059 epochs
09/05 04:04:55 PM: mnli: trained on 999 batches, 0.061 epochs
09/05 04:04:55 PM: Validating...
09/05 04:04:55 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8333, copa_loss: 0.5505
09/05 04:05:04 PM: Evaluate: task mnli, batch 128 (209): accuracy: 0.8219, mnli_loss: 0.4540
09/05 04:05:10 PM: Best result seen so far for mnli.
09/05 04:05:10 PM: Best result seen so far for micro.
09/05 04:05:10 PM: Updating LR scheduler:
09/05 04:05:10 PM: 	Best result seen so far for macro_avg: 0.792
09/05 04:05:10 PM: 	# validation passes without improvement: 1
09/05 04:05:10 PM: copa_loss: training: 0.638995 validation: 0.622677
09/05 04:05:10 PM: mnli_loss: training: 0.421058 validation: 0.461852
09/05 04:05:10 PM: macro_avg: validation: 0.769700
09/05 04:05:10 PM: micro_avg: validation: 0.817451
09/05 04:05:10 PM: copa_accuracy: training: 0.583333 validation: 0.720000
09/05 04:05:10 PM: mnli_accuracy: training: 0.838063 validation: 0.819400
09/05 04:05:10 PM: Global learning rate: 1e-05
09/05 04:05:10 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 04:05:15 PM: Update 22013: task mnli, batch 13 (21985): accuracy: 0.8077, mnli_loss: 0.4684
09/05 04:05:25 PM: Update 22049: task mnli, batch 49 (22021): accuracy: 0.8325, mnli_loss: 0.4403
09/05 04:05:35 PM: Update 22086: task mnli, batch 86 (22058): accuracy: 0.8479, mnli_loss: 0.4027
09/05 04:05:45 PM: Update 22126: task mnli, batch 126 (22098): accuracy: 0.8495, mnli_loss: 0.3889
09/05 04:05:55 PM: Update 22164: task mnli, batch 164 (22136): accuracy: 0.8519, mnli_loss: 0.3896
09/05 04:06:05 PM: Update 22202: task mnli, batch 202 (22174): accuracy: 0.8467, mnli_loss: 0.3991
09/05 04:06:15 PM: Update 22242: task mnli, batch 242 (22214): accuracy: 0.8483, mnli_loss: 0.3974
09/05 04:06:21 PM: Update 22256: task copa, batch 1 (29): accuracy: 0.7083, copa_loss: 0.6008
09/05 04:06:25 PM: Update 22273: task mnli, batch 272 (22244): accuracy: 0.8506, mnli_loss: 0.3943
09/05 04:06:35 PM: Update 22312: task mnli, batch 311 (22283): accuracy: 0.8507, mnli_loss: 0.3943
09/05 04:06:45 PM: Update 22352: task mnli, batch 351 (22323): accuracy: 0.8499, mnli_loss: 0.3949
09/05 04:06:56 PM: Update 22389: task mnli, batch 388 (22360): accuracy: 0.8492, mnli_loss: 0.3939
09/05 04:07:06 PM: Update 22429: task mnli, batch 428 (22400): accuracy: 0.8480, mnli_loss: 0.3984
09/05 04:07:16 PM: Update 22468: task mnli, batch 467 (22439): accuracy: 0.8484, mnli_loss: 0.3984
09/05 04:07:26 PM: Update 22507: task mnli, batch 506 (22478): accuracy: 0.8483, mnli_loss: 0.3986
09/05 04:07:36 PM: Update 22546: task mnli, batch 545 (22517): accuracy: 0.8488, mnli_loss: 0.3989
09/05 04:07:47 PM: Update 22583: task mnli, batch 582 (22554): accuracy: 0.8490, mnli_loss: 0.3997
09/05 04:07:57 PM: Update 22621: task mnli, batch 620 (22592): accuracy: 0.8480, mnli_loss: 0.4011
09/05 04:08:07 PM: Update 22658: task mnli, batch 657 (22629): accuracy: 0.8482, mnli_loss: 0.4002
09/05 04:08:17 PM: Update 22690: task mnli, batch 689 (22661): accuracy: 0.8473, mnli_loss: 0.4009
09/05 04:08:18 PM: Update 22693: task copa, batch 2 (30): accuracy: 0.7083, copa_loss: 0.5985
09/05 04:08:27 PM: Update 22727: task mnli, batch 725 (22697): accuracy: 0.8476, mnli_loss: 0.4007
09/05 04:08:37 PM: Update 22766: task mnli, batch 764 (22736): accuracy: 0.8477, mnli_loss: 0.4005
09/05 04:08:41 PM: Update 22782: task copa, batch 3 (31): accuracy: 0.6944, copa_loss: 0.6143
09/05 04:08:47 PM: Update 22803: task mnli, batch 800 (22772): accuracy: 0.8470, mnli_loss: 0.4021
09/05 04:08:58 PM: Update 22842: task mnli, batch 839 (22811): accuracy: 0.8468, mnli_loss: 0.4019
09/05 04:09:08 PM: Update 22879: task mnli, batch 876 (22848): accuracy: 0.8465, mnli_loss: 0.4040
09/05 04:09:12 PM: Update 22895: task copa, batch 4 (32): accuracy: 0.7292, copa_loss: 0.5933
09/05 04:09:18 PM: Update 22916: task mnli, batch 912 (22884): accuracy: 0.8461, mnli_loss: 0.4065
09/05 04:09:28 PM: Update 22954: task mnli, batch 950 (22922): accuracy: 0.8457, mnli_loss: 0.4067
09/05 04:09:38 PM: Update 22994: task mnli, batch 990 (22962): accuracy: 0.8459, mnli_loss: 0.4062
09/05 04:09:40 PM: ***** Step 23000 / Validation 23 *****
09/05 04:09:40 PM: copa: trained on 4 batches, 0.235 epochs
09/05 04:09:40 PM: mnli: trained on 996 batches, 0.061 epochs
09/05 04:09:40 PM: Validating...
09/05 04:09:40 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.7917, copa_loss: 0.5211
09/05 04:09:48 PM: Evaluate: task mnli, batch 111 (209): accuracy: 0.8142, mnli_loss: 0.4957
09/05 04:09:55 PM: Updating LR scheduler:
09/05 04:09:55 PM: 	Best result seen so far for macro_avg: 0.792
09/05 04:09:55 PM: 	# validation passes without improvement: 2
09/05 04:09:55 PM: copa_loss: training: 0.593313 validation: 0.620180
09/05 04:09:55 PM: mnli_loss: training: 0.405568 validation: 0.488721
09/05 04:09:55 PM: macro_avg: validation: 0.773100
09/05 04:09:55 PM: micro_avg: validation: 0.814510
09/05 04:09:55 PM: copa_accuracy: training: 0.729167 validation: 0.730000
09/05 04:09:55 PM: mnli_accuracy: training: 0.845990 validation: 0.816200
09/05 04:09:55 PM: Global learning rate: 1e-05
09/05 04:09:55 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 04:09:58 PM: Update 23007: task mnli, batch 7 (22975): accuracy: 0.8452, mnli_loss: 0.4445
09/05 04:10:08 PM: Update 23046: task mnli, batch 46 (23014): accuracy: 0.8460, mnli_loss: 0.4074
09/05 04:10:20 PM: Update 23081: task mnli, batch 81 (23049): accuracy: 0.8471, mnli_loss: 0.4132
09/05 04:10:30 PM: Update 23118: task mnli, batch 118 (23086): accuracy: 0.8470, mnli_loss: 0.4157
09/05 04:10:40 PM: Update 23155: task mnli, batch 155 (23123): accuracy: 0.8432, mnli_loss: 0.4215
09/05 04:10:51 PM: Update 23193: task mnli, batch 193 (23161): accuracy: 0.8428, mnli_loss: 0.4188
09/05 04:11:01 PM: Update 23232: task mnli, batch 232 (23200): accuracy: 0.8453, mnli_loss: 0.4148
09/05 04:11:07 PM: Update 23254: task copa, batch 1 (33): accuracy: 0.8333, copa_loss: 0.5435
09/05 04:11:11 PM: Update 23270: task mnli, batch 269 (23237): accuracy: 0.8483, mnli_loss: 0.4064
09/05 04:11:21 PM: Update 23309: task mnli, batch 308 (23276): accuracy: 0.8494, mnli_loss: 0.4035
09/05 04:11:31 PM: Update 23346: task mnli, batch 345 (23313): accuracy: 0.8484, mnli_loss: 0.4023
09/05 04:11:41 PM: Update 23385: task mnli, batch 384 (23352): accuracy: 0.8480, mnli_loss: 0.4021
09/05 04:11:52 PM: Update 23425: task mnli, batch 424 (23392): accuracy: 0.8493, mnli_loss: 0.4012
09/05 04:12:02 PM: Update 23463: task mnli, batch 462 (23430): accuracy: 0.8484, mnli_loss: 0.4007
09/05 04:12:13 PM: Update 23499: task mnli, batch 498 (23466): accuracy: 0.8481, mnli_loss: 0.4012
09/05 04:12:23 PM: Update 23539: task mnli, batch 538 (23506): accuracy: 0.8481, mnli_loss: 0.4019
09/05 04:12:33 PM: Update 23578: task mnli, batch 577 (23545): accuracy: 0.8489, mnli_loss: 0.4004
09/05 04:12:43 PM: Update 23616: task mnli, batch 615 (23583): accuracy: 0.8503, mnli_loss: 0.3981
09/05 04:12:53 PM: Update 23653: task mnli, batch 652 (23620): accuracy: 0.8496, mnli_loss: 0.3993
09/05 04:13:03 PM: Update 23691: task mnli, batch 690 (23658): accuracy: 0.8488, mnli_loss: 0.3999
09/05 04:13:13 PM: Update 23729: task mnli, batch 728 (23696): accuracy: 0.8484, mnli_loss: 0.4000
09/05 04:13:23 PM: Update 23766: task mnli, batch 765 (23733): accuracy: 0.8483, mnli_loss: 0.4000
09/05 04:13:33 PM: Update 23804: task mnli, batch 803 (23771): accuracy: 0.8486, mnli_loss: 0.3986
09/05 04:13:44 PM: Update 23844: task mnli, batch 843 (23811): accuracy: 0.8478, mnli_loss: 0.3995
09/05 04:13:54 PM: Update 23882: task mnli, batch 881 (23849): accuracy: 0.8483, mnli_loss: 0.3989
09/05 04:14:05 PM: Update 23916: task mnli, batch 915 (23883): accuracy: 0.8480, mnli_loss: 0.3991
09/05 04:14:15 PM: Update 23953: task mnli, batch 952 (23920): accuracy: 0.8480, mnli_loss: 0.3990
09/05 04:14:25 PM: Update 23993: task mnli, batch 992 (23960): accuracy: 0.8486, mnli_loss: 0.3971
09/05 04:14:27 PM: ***** Step 24000 / Validation 24 *****
09/05 04:14:27 PM: copa: trained on 1 batches, 0.059 epochs
09/05 04:14:27 PM: mnli: trained on 999 batches, 0.061 epochs
09/05 04:14:27 PM: Validating...
09/05 04:14:27 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8750, copa_loss: 0.4942
09/05 04:14:35 PM: Evaluate: task mnli, batch 109 (209): accuracy: 0.8261, mnli_loss: 0.4689
09/05 04:14:42 PM: Best result seen so far for mnli.
09/05 04:14:42 PM: Best result seen so far for micro.
09/05 04:14:42 PM: Updating LR scheduler:
09/05 04:14:42 PM: 	Best result seen so far for macro_avg: 0.792
09/05 04:14:42 PM: 	# validation passes without improvement: 3
09/05 04:14:42 PM: copa_loss: training: 0.543515 validation: 0.614713
09/05 04:14:42 PM: mnli_loss: training: 0.396953 validation: 0.482879
09/05 04:14:42 PM: macro_avg: validation: 0.780200
09/05 04:14:42 PM: micro_avg: validation: 0.818824
09/05 04:14:42 PM: copa_accuracy: training: 0.833333 validation: 0.740000
09/05 04:14:42 PM: mnli_accuracy: training: 0.848572 validation: 0.820400
09/05 04:14:42 PM: Global learning rate: 1e-05
09/05 04:14:42 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 04:14:45 PM: Update 24008: task mnli, batch 8 (23975): accuracy: 0.8385, mnli_loss: 0.4330
09/05 04:14:55 PM: Update 24045: task mnli, batch 45 (24012): accuracy: 0.8324, mnli_loss: 0.4353
09/05 04:15:05 PM: Update 24084: task mnli, batch 84 (24051): accuracy: 0.8373, mnli_loss: 0.4175
09/05 04:15:16 PM: Update 24123: task mnli, batch 123 (24090): accuracy: 0.8428, mnli_loss: 0.4028
09/05 04:15:26 PM: Update 24162: task mnli, batch 162 (24129): accuracy: 0.8452, mnli_loss: 0.3983
09/05 04:15:36 PM: Update 24200: task mnli, batch 200 (24167): accuracy: 0.8473, mnli_loss: 0.3955
09/05 04:15:46 PM: Update 24239: task mnli, batch 239 (24206): accuracy: 0.8468, mnli_loss: 0.3962
09/05 04:15:56 PM: Update 24277: task mnli, batch 277 (24244): accuracy: 0.8488, mnli_loss: 0.3936
09/05 04:16:04 PM: Update 24307: task copa, batch 1 (34): accuracy: 0.7083, copa_loss: 0.6031
09/05 04:16:06 PM: Update 24315: task mnli, batch 314 (24281): accuracy: 0.8485, mnli_loss: 0.3952
09/05 04:16:16 PM: Update 24344: task copa, batch 2 (35): accuracy: 0.7500, copa_loss: 0.5890
09/05 04:16:17 PM: Update 24345: task mnli, batch 343 (24310): accuracy: 0.8482, mnli_loss: 0.3952
09/05 04:16:27 PM: Update 24384: task mnli, batch 382 (24349): accuracy: 0.8489, mnli_loss: 0.3955
09/05 04:16:37 PM: Update 24421: task mnli, batch 419 (24386): accuracy: 0.8475, mnli_loss: 0.3979
09/05 04:16:47 PM: Update 24460: task mnli, batch 458 (24425): accuracy: 0.8479, mnli_loss: 0.3986
09/05 04:16:57 PM: Update 24500: task mnli, batch 498 (24465): accuracy: 0.8490, mnli_loss: 0.3961
09/05 04:17:07 PM: Update 24540: task mnli, batch 538 (24505): accuracy: 0.8478, mnli_loss: 0.3974
09/05 04:17:17 PM: Update 24579: task mnli, batch 577 (24544): accuracy: 0.8475, mnli_loss: 0.3983
09/05 04:17:28 PM: Update 24616: task mnli, batch 614 (24581): accuracy: 0.8450, mnli_loss: 0.4031
09/05 04:17:38 PM: Update 24653: task mnli, batch 651 (24618): accuracy: 0.8446, mnli_loss: 0.4032
09/05 04:17:45 PM: Update 24680: task copa, batch 3 (36): accuracy: 0.7188, copa_loss: 0.5912
09/05 04:17:48 PM: Update 24691: task mnli, batch 688 (24655): accuracy: 0.8449, mnli_loss: 0.4033
09/05 04:17:58 PM: Update 24728: task mnli, batch 725 (24692): accuracy: 0.8450, mnli_loss: 0.4034
09/05 04:18:08 PM: Update 24758: task mnli, batch 755 (24722): accuracy: 0.8447, mnli_loss: 0.4031
09/05 04:18:18 PM: Update 24797: task mnli, batch 794 (24761): accuracy: 0.8446, mnli_loss: 0.4023
09/05 04:18:28 PM: Update 24835: task mnli, batch 832 (24799): accuracy: 0.8456, mnli_loss: 0.4006
09/05 04:18:39 PM: Update 24873: task mnli, batch 870 (24837): accuracy: 0.8445, mnli_loss: 0.4041
09/05 04:18:49 PM: Update 24912: task mnli, batch 909 (24876): accuracy: 0.8454, mnli_loss: 0.4012
09/05 04:18:59 PM: Update 24951: task mnli, batch 948 (24915): accuracy: 0.8450, mnli_loss: 0.4015
09/05 04:19:09 PM: Update 24989: task mnli, batch 986 (24953): accuracy: 0.8448, mnli_loss: 0.4014
09/05 04:19:12 PM: ***** Step 25000 / Validation 25 *****
09/05 04:19:12 PM: copa: trained on 3 batches, 0.176 epochs
09/05 04:19:12 PM: mnli: trained on 997 batches, 0.061 epochs
09/05 04:19:12 PM: Validating...
09/05 04:19:12 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.9167, copa_loss: 0.4877
09/05 04:19:19 PM: Evaluate: task mnli, batch 89 (209): accuracy: 0.8244, mnli_loss: 0.4692
09/05 04:19:28 PM: Best result seen so far for copa.
09/05 04:19:28 PM: Best result seen so far for macro.
09/05 04:19:28 PM: Updating LR scheduler:
09/05 04:19:28 PM: 	Best result seen so far for macro_avg: 0.800
09/05 04:19:28 PM: 	# validation passes without improvement: 0
09/05 04:19:28 PM: copa_loss: training: 0.591204 validation: 0.604324
09/05 04:19:28 PM: mnli_loss: training: 0.401135 validation: 0.480280
09/05 04:19:28 PM: macro_avg: validation: 0.799700
09/05 04:19:28 PM: micro_avg: validation: 0.818627
09/05 04:19:28 PM: copa_accuracy: training: 0.718750 validation: 0.780000
09/05 04:19:28 PM: mnli_accuracy: training: 0.844890 validation: 0.819400
09/05 04:19:28 PM: Global learning rate: 1e-05
09/05 04:19:28 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 04:19:29 PM: Update 25001: task mnli, batch 1 (24965): accuracy: 0.9167, mnli_loss: 0.2836
09/05 04:19:39 PM: Update 25039: task mnli, batch 39 (25003): accuracy: 0.8312, mnli_loss: 0.4023
09/05 04:19:49 PM: Update 25078: task mnli, batch 78 (25042): accuracy: 0.8413, mnli_loss: 0.3980
09/05 04:20:00 PM: Update 25116: task mnli, batch 116 (25080): accuracy: 0.8420, mnli_loss: 0.3962
09/05 04:20:10 PM: Update 25155: task mnli, batch 155 (25119): accuracy: 0.8406, mnli_loss: 0.4009
09/05 04:20:20 PM: Update 25183: task mnli, batch 183 (25147): accuracy: 0.8403, mnli_loss: 0.4041
09/05 04:20:22 PM: Update 25194: task copa, batch 1 (37): accuracy: 0.8333, copa_loss: 0.4945
09/05 04:20:30 PM: Update 25223: task mnli, batch 222 (25186): accuracy: 0.8421, mnli_loss: 0.4048
09/05 04:20:40 PM: Update 25262: task mnli, batch 261 (25225): accuracy: 0.8427, mnli_loss: 0.4045
09/05 04:20:50 PM: Update 25301: task mnli, batch 300 (25264): accuracy: 0.8433, mnli_loss: 0.4049
09/05 04:21:00 PM: Update 25339: task mnli, batch 338 (25302): accuracy: 0.8440, mnli_loss: 0.4048
09/05 04:21:10 PM: Update 25378: task mnli, batch 377 (25341): accuracy: 0.8451, mnli_loss: 0.4029
09/05 04:21:20 PM: Update 25414: task mnli, batch 413 (25377): accuracy: 0.8431, mnli_loss: 0.4072
09/05 04:21:26 PM: Update 25435: task copa, batch 2 (38): accuracy: 0.7708, copa_loss: 0.5239
09/05 04:21:31 PM: Update 25452: task mnli, batch 450 (25414): accuracy: 0.8419, mnli_loss: 0.4076
09/05 04:21:41 PM: Update 25490: task mnli, batch 488 (25452): accuracy: 0.8430, mnli_loss: 0.4068
09/05 04:21:51 PM: Update 25528: task mnli, batch 526 (25490): accuracy: 0.8434, mnli_loss: 0.4071
09/05 04:22:01 PM: Update 25567: task mnli, batch 565 (25529): accuracy: 0.8449, mnli_loss: 0.4045
09/05 04:22:11 PM: Update 25598: task mnli, batch 596 (25560): accuracy: 0.8436, mnli_loss: 0.4075
09/05 04:22:21 PM: Update 25636: task mnli, batch 634 (25598): accuracy: 0.8439, mnli_loss: 0.4069
09/05 04:22:32 PM: Update 25674: task mnli, batch 672 (25636): accuracy: 0.8444, mnli_loss: 0.4062
09/05 04:22:42 PM: Update 25713: task mnli, batch 711 (25675): accuracy: 0.8448, mnli_loss: 0.4052
09/05 04:22:52 PM: Update 25752: task mnli, batch 750 (25714): accuracy: 0.8451, mnli_loss: 0.4038
09/05 04:23:02 PM: Update 25790: task mnli, batch 788 (25752): accuracy: 0.8454, mnli_loss: 0.4031
09/05 04:23:12 PM: Update 25830: task mnli, batch 828 (25792): accuracy: 0.8457, mnli_loss: 0.4024
09/05 04:23:22 PM: Update 25868: task mnli, batch 866 (25830): accuracy: 0.8461, mnli_loss: 0.4026
09/05 04:23:33 PM: Update 25907: task mnli, batch 905 (25869): accuracy: 0.8468, mnli_loss: 0.4011
09/05 04:23:43 PM: Update 25943: task mnli, batch 941 (25905): accuracy: 0.8460, mnli_loss: 0.4026
09/05 04:23:53 PM: Update 25982: task mnli, batch 980 (25944): accuracy: 0.8464, mnli_loss: 0.4012
09/05 04:23:58 PM: ***** Step 26000 / Validation 26 *****
09/05 04:23:58 PM: copa: trained on 2 batches, 0.118 epochs
09/05 04:23:58 PM: mnli: trained on 998 batches, 0.061 epochs
09/05 04:23:58 PM: Validating...
09/05 04:23:58 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8750, copa_loss: 0.4811
09/05 04:24:03 PM: Evaluate: task mnli, batch 67 (209): accuracy: 0.8246, mnli_loss: 0.4658
09/05 04:24:13 PM: Evaluate: task mnli, batch 203 (209): accuracy: 0.8216, mnli_loss: 0.4817
09/05 04:24:13 PM: Updating LR scheduler:
09/05 04:24:13 PM: 	Best result seen so far for macro_avg: 0.800
09/05 04:24:13 PM: 	# validation passes without improvement: 1
09/05 04:24:13 PM: copa_loss: training: 0.523923 validation: 0.608643
09/05 04:24:13 PM: mnli_loss: training: 0.402596 validation: 0.487752
09/05 04:24:13 PM: macro_avg: validation: 0.784600
09/05 04:24:13 PM: micro_avg: validation: 0.817843
09/05 04:24:13 PM: copa_accuracy: training: 0.770833 validation: 0.750000
09/05 04:24:13 PM: mnli_accuracy: training: 0.845505 validation: 0.819200
09/05 04:24:13 PM: Global learning rate: 1e-05
09/05 04:24:13 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 04:24:23 PM: Update 26026: task mnli, batch 26 (25988): accuracy: 0.8555, mnli_loss: 0.4069
09/05 04:24:33 PM: Update 26064: task mnli, batch 64 (26026): accuracy: 0.8429, mnli_loss: 0.4085
09/05 04:24:43 PM: Update 26102: task mnli, batch 102 (26064): accuracy: 0.8488, mnli_loss: 0.3950
09/05 04:24:53 PM: Update 26141: task mnli, batch 141 (26103): accuracy: 0.8442, mnli_loss: 0.4060
09/05 04:25:03 PM: Update 26179: task mnli, batch 179 (26141): accuracy: 0.8479, mnli_loss: 0.4022
09/05 04:25:14 PM: Update 26217: task mnli, batch 217 (26179): accuracy: 0.8506, mnli_loss: 0.3972
09/05 04:25:19 PM: Update 26238: task copa, batch 1 (39): accuracy: 0.7917, copa_loss: 0.5434
09/05 04:25:24 PM: Update 26255: task mnli, batch 254 (26216): accuracy: 0.8543, mnli_loss: 0.3918
09/05 04:25:34 PM: Update 26295: task mnli, batch 294 (26256): accuracy: 0.8522, mnli_loss: 0.3954
09/05 04:25:44 PM: Update 26335: task mnli, batch 334 (26296): accuracy: 0.8518, mnli_loss: 0.3949
09/05 04:25:54 PM: Update 26373: task mnli, batch 372 (26334): accuracy: 0.8513, mnli_loss: 0.3961
09/05 04:26:04 PM: Update 26411: task mnli, batch 410 (26372): accuracy: 0.8501, mnli_loss: 0.3968
09/05 04:26:15 PM: Update 26442: task mnli, batch 441 (26403): accuracy: 0.8501, mnli_loss: 0.3978
09/05 04:26:25 PM: Update 26479: task mnli, batch 478 (26440): accuracy: 0.8507, mnli_loss: 0.3944
09/05 04:26:35 PM: Update 26517: task mnli, batch 516 (26478): accuracy: 0.8524, mnli_loss: 0.3901
09/05 04:26:45 PM: Update 26556: task mnli, batch 555 (26517): accuracy: 0.8540, mnli_loss: 0.3869
09/05 04:26:55 PM: Update 26596: task mnli, batch 595 (26557): accuracy: 0.8542, mnli_loss: 0.3859
09/05 04:27:05 PM: Update 26635: task mnli, batch 634 (26596): accuracy: 0.8537, mnli_loss: 0.3864
09/05 04:27:15 PM: Update 26672: task mnli, batch 671 (26633): accuracy: 0.8541, mnli_loss: 0.3869
09/05 04:27:25 PM: Update 26711: task mnli, batch 710 (26672): accuracy: 0.8546, mnli_loss: 0.3863
09/05 04:27:35 PM: Update 26747: task mnli, batch 746 (26708): accuracy: 0.8548, mnli_loss: 0.3854
09/05 04:27:45 PM: Update 26787: task mnli, batch 786 (26748): accuracy: 0.8546, mnli_loss: 0.3859
09/05 04:27:56 PM: Update 26826: task mnli, batch 825 (26787): accuracy: 0.8549, mnli_loss: 0.3858
09/05 04:28:06 PM: Update 26857: task mnli, batch 856 (26818): accuracy: 0.8545, mnli_loss: 0.3863
09/05 04:28:16 PM: Update 26896: task mnli, batch 895 (26857): accuracy: 0.8546, mnli_loss: 0.3860
09/05 04:28:26 PM: Update 26934: task mnli, batch 933 (26895): accuracy: 0.8538, mnli_loss: 0.3866
09/05 04:28:34 PM: Update 26961: task copa, batch 2 (40): accuracy: 0.7292, copa_loss: 0.5740
09/05 04:28:36 PM: Update 26971: task mnli, batch 969 (26931): accuracy: 0.8527, mnli_loss: 0.3889
09/05 04:28:43 PM: ***** Step 27000 / Validation 27 *****
09/05 04:28:43 PM: copa: trained on 2 batches, 0.118 epochs
09/05 04:28:43 PM: mnli: trained on 998 batches, 0.061 epochs
09/05 04:28:43 PM: Validating...
09/05 04:28:44 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8333, copa_loss: 0.4730
09/05 04:28:46 PM: Evaluate: task mnli, batch 32 (209): accuracy: 0.8125, mnli_loss: 0.4729
09/05 04:28:56 PM: Evaluate: task mnli, batch 159 (209): accuracy: 0.8218, mnli_loss: 0.4733
09/05 04:29:00 PM: Updating LR scheduler:
09/05 04:29:00 PM: 	Best result seen so far for macro_avg: 0.800
09/05 04:29:00 PM: 	# validation passes without improvement: 2
09/05 04:29:00 PM: copa_loss: training: 0.574006 validation: 0.597576
09/05 04:29:00 PM: mnli_loss: training: 0.388065 validation: 0.482648
09/05 04:29:00 PM: macro_avg: validation: 0.779900
09/05 04:29:00 PM: micro_avg: validation: 0.818235
09/05 04:29:00 PM: copa_accuracy: training: 0.729167 validation: 0.740000
09/05 04:29:00 PM: mnli_accuracy: training: 0.853477 validation: 0.819800
09/05 04:29:00 PM: Global learning rate: 1e-05
09/05 04:29:00 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 04:29:06 PM: Update 27021: task mnli, batch 21 (26981): accuracy: 0.8730, mnli_loss: 0.3713
09/05 04:29:16 PM: Update 27058: task mnli, batch 58 (27018): accuracy: 0.8664, mnli_loss: 0.3671
09/05 04:29:27 PM: Update 27097: task mnli, batch 97 (27057): accuracy: 0.8664, mnli_loss: 0.3581
09/05 04:29:37 PM: Update 27137: task mnli, batch 137 (27097): accuracy: 0.8677, mnli_loss: 0.3487
09/05 04:29:47 PM: Update 27174: task mnli, batch 174 (27134): accuracy: 0.8657, mnli_loss: 0.3542
09/05 04:29:57 PM: Update 27214: task mnli, batch 214 (27174): accuracy: 0.8586, mnli_loss: 0.3646
09/05 04:30:07 PM: Update 27252: task mnli, batch 252 (27212): accuracy: 0.8600, mnli_loss: 0.3615
09/05 04:30:18 PM: Update 27282: task mnli, batch 282 (27242): accuracy: 0.8562, mnli_loss: 0.3717
09/05 04:30:28 PM: Update 27319: task mnli, batch 319 (27279): accuracy: 0.8568, mnli_loss: 0.3693
09/05 04:30:37 PM: Update 27355: task copa, batch 1 (41): accuracy: 0.8333, copa_loss: 0.5048
09/05 04:30:38 PM: Update 27356: task mnli, batch 355 (27315): accuracy: 0.8576, mnli_loss: 0.3680
09/05 04:30:48 PM: Update 27394: task mnli, batch 393 (27353): accuracy: 0.8558, mnli_loss: 0.3724
09/05 04:30:58 PM: Update 27432: task mnli, batch 431 (27391): accuracy: 0.8541, mnli_loss: 0.3765
09/05 04:31:08 PM: Update 27471: task mnli, batch 470 (27430): accuracy: 0.8534, mnli_loss: 0.3782
09/05 04:31:18 PM: Update 27510: task mnli, batch 509 (27469): accuracy: 0.8544, mnli_loss: 0.3766
09/05 04:31:28 PM: Update 27548: task mnli, batch 547 (27507): accuracy: 0.8550, mnli_loss: 0.3745
09/05 04:31:38 PM: Update 27586: task mnli, batch 585 (27545): accuracy: 0.8536, mnli_loss: 0.3770
09/05 04:31:49 PM: Update 27626: task mnli, batch 625 (27585): accuracy: 0.8532, mnli_loss: 0.3784
09/05 04:31:59 PM: Update 27666: task mnli, batch 665 (27625): accuracy: 0.8539, mnli_loss: 0.3769
09/05 04:32:09 PM: Update 27697: task mnli, batch 696 (27656): accuracy: 0.8534, mnli_loss: 0.3776
09/05 04:32:19 PM: Update 27735: task mnli, batch 734 (27694): accuracy: 0.8527, mnli_loss: 0.3800
09/05 04:32:29 PM: Update 27774: task mnli, batch 773 (27733): accuracy: 0.8515, mnli_loss: 0.3828
09/05 04:32:39 PM: Update 27814: task mnli, batch 813 (27773): accuracy: 0.8513, mnli_loss: 0.3847
09/05 04:32:49 PM: Update 27853: task mnli, batch 852 (27812): accuracy: 0.8519, mnli_loss: 0.3833
09/05 04:32:59 PM: Update 27890: task mnli, batch 889 (27849): accuracy: 0.8514, mnli_loss: 0.3839
09/05 04:33:10 PM: Update 27928: task mnli, batch 927 (27887): accuracy: 0.8510, mnli_loss: 0.3851
09/05 04:33:20 PM: Update 27965: task mnli, batch 964 (27924): accuracy: 0.8511, mnli_loss: 0.3844
09/05 04:33:29 PM: ***** Step 28000 / Validation 28 *****
09/05 04:33:29 PM: copa: trained on 1 batches, 0.059 epochs
09/05 04:33:29 PM: mnli: trained on 999 batches, 0.061 epochs
09/05 04:33:29 PM: Validating...
09/05 04:33:29 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8750, copa_loss: 0.4683
09/05 04:33:30 PM: Evaluate: task mnli, batch 6 (209): accuracy: 0.8056, mnli_loss: 0.4541
09/05 04:33:40 PM: Evaluate: task mnli, batch 142 (209): accuracy: 0.8175, mnli_loss: 0.4809
09/05 04:33:45 PM: Best result seen so far for mnli.
09/05 04:33:45 PM: Best result seen so far for micro.
09/05 04:33:45 PM: Updating LR scheduler:
09/05 04:33:45 PM: 	Best result seen so far for macro_avg: 0.800
09/05 04:33:45 PM: 	# validation passes without improvement: 3
09/05 04:33:45 PM: copa_loss: training: 0.504810 validation: 0.587826
09/05 04:33:45 PM: mnli_loss: training: 0.384413 validation: 0.479926
09/05 04:33:45 PM: macro_avg: validation: 0.795300
09/05 04:33:45 PM: micro_avg: validation: 0.819608
09/05 04:33:45 PM: copa_accuracy: training: 0.833333 validation: 0.770000
09/05 04:33:45 PM: mnli_accuracy: training: 0.850710 validation: 0.820600
09/05 04:33:45 PM: Global learning rate: 1e-05
09/05 04:33:45 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 04:33:50 PM: Update 28016: task mnli, batch 16 (27975): accuracy: 0.8516, mnli_loss: 0.3583
09/05 04:34:00 PM: Update 28054: task mnli, batch 54 (28013): accuracy: 0.8665, mnli_loss: 0.3588
09/05 04:34:12 PM: Update 28094: task mnli, batch 94 (28053): accuracy: 0.8732, mnli_loss: 0.3498
09/05 04:34:22 PM: Update 28131: task mnli, batch 131 (28090): accuracy: 0.8613, mnli_loss: 0.3764
09/05 04:34:32 PM: Update 28170: task mnli, batch 170 (28129): accuracy: 0.8551, mnli_loss: 0.3866
09/05 04:34:43 PM: Update 28209: task mnli, batch 209 (28168): accuracy: 0.8578, mnli_loss: 0.3773
09/05 04:34:53 PM: Update 28246: task mnli, batch 246 (28205): accuracy: 0.8584, mnli_loss: 0.3778
09/05 04:35:03 PM: Update 28284: task mnli, batch 284 (28243): accuracy: 0.8572, mnli_loss: 0.3797
09/05 04:35:13 PM: Update 28323: task mnli, batch 323 (28282): accuracy: 0.8569, mnli_loss: 0.3796
09/05 04:35:23 PM: Update 28362: task mnli, batch 362 (28321): accuracy: 0.8559, mnli_loss: 0.3796
09/05 04:35:33 PM: Update 28399: task mnli, batch 399 (28358): accuracy: 0.8561, mnli_loss: 0.3782
09/05 04:35:43 PM: Update 28438: task mnli, batch 438 (28397): accuracy: 0.8555, mnli_loss: 0.3781
09/05 04:35:54 PM: Update 28477: task mnli, batch 477 (28436): accuracy: 0.8564, mnli_loss: 0.3781
09/05 04:36:05 PM: Update 28511: task mnli, batch 511 (28470): accuracy: 0.8570, mnli_loss: 0.3767
09/05 04:36:15 PM: Update 28549: task mnli, batch 549 (28508): accuracy: 0.8554, mnli_loss: 0.3794
09/05 04:36:25 PM: Update 28585: task mnli, batch 585 (28544): accuracy: 0.8546, mnli_loss: 0.3823
09/05 04:36:35 PM: Update 28624: task mnli, batch 624 (28583): accuracy: 0.8542, mnli_loss: 0.3833
09/05 04:36:45 PM: Update 28663: task mnli, batch 663 (28622): accuracy: 0.8547, mnli_loss: 0.3820
09/05 04:36:55 PM: Update 28703: task mnli, batch 703 (28662): accuracy: 0.8562, mnli_loss: 0.3793
09/05 04:37:06 PM: Update 28743: task mnli, batch 743 (28702): accuracy: 0.8566, mnli_loss: 0.3783
09/05 04:37:16 PM: Update 28782: task mnli, batch 782 (28741): accuracy: 0.8568, mnli_loss: 0.3776
09/05 04:37:26 PM: Update 28820: task mnli, batch 820 (28779): accuracy: 0.8564, mnli_loss: 0.3790
09/05 04:37:36 PM: Update 28858: task mnli, batch 858 (28817): accuracy: 0.8563, mnli_loss: 0.3795
09/05 04:37:41 PM: Update 28874: task copa, batch 1 (42): accuracy: 0.7083, copa_loss: 0.5017
09/05 04:37:46 PM: Update 28894: task mnli, batch 892 (28851): accuracy: 0.8568, mnli_loss: 0.3781
09/05 04:37:57 PM: Update 28930: task mnli, batch 928 (28887): accuracy: 0.8573, mnli_loss: 0.3776
09/05 04:38:07 PM: Update 28969: task mnli, batch 967 (28926): accuracy: 0.8580, mnli_loss: 0.3770
09/05 04:38:15 PM: ***** Step 29000 / Validation 29 *****
09/05 04:38:15 PM: copa: trained on 2 batches, 0.118 epochs
09/05 04:38:15 PM: mnli: trained on 998 batches, 0.061 epochs
09/05 04:38:15 PM: Validating...
09/05 04:38:15 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8333, copa_loss: 0.4511
09/05 04:38:17 PM: Evaluate: task mnli, batch 22 (209): accuracy: 0.8163, mnli_loss: 0.4676
09/05 04:38:27 PM: Evaluate: task mnli, batch 158 (209): accuracy: 0.8175, mnli_loss: 0.4998
09/05 04:38:31 PM: Updating LR scheduler:
09/05 04:38:31 PM: 	Best result seen so far for macro_avg: 0.800
09/05 04:38:31 PM: 	# validation passes without improvement: 4
09/05 04:38:31 PM: copa_loss: training: 0.505583 validation: 0.595271
09/05 04:38:31 PM: mnli_loss: training: 0.376587 validation: 0.509644
09/05 04:38:31 PM: macro_avg: validation: 0.791700
09/05 04:38:31 PM: micro_avg: validation: 0.812549
09/05 04:38:31 PM: copa_accuracy: training: 0.770833 validation: 0.770000
09/05 04:38:31 PM: mnli_accuracy: training: 0.858074 validation: 0.813400
09/05 04:38:31 PM: Global learning rate: 1e-05
09/05 04:38:31 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 04:38:37 PM: Update 29019: task mnli, batch 19 (28976): accuracy: 0.8750, mnli_loss: 0.3523
09/05 04:38:48 PM: Update 29058: task mnli, batch 58 (29015): accuracy: 0.8700, mnli_loss: 0.3537
09/05 04:38:58 PM: Update 29095: task mnli, batch 95 (29052): accuracy: 0.8610, mnli_loss: 0.3659
09/05 04:39:08 PM: Update 29133: task mnli, batch 133 (29090): accuracy: 0.8647, mnli_loss: 0.3614
09/05 04:39:18 PM: Update 29170: task mnli, batch 170 (29127): accuracy: 0.8625, mnli_loss: 0.3631
09/05 04:39:28 PM: Update 29207: task copa, batch 1 (44): accuracy: 0.8333, copa_loss: 0.4943
09/05 04:39:28 PM: Update 29208: task mnli, batch 207 (29164): accuracy: 0.8625, mnli_loss: 0.3621
09/05 04:39:38 PM: Update 29246: task mnli, batch 245 (29202): accuracy: 0.8631, mnli_loss: 0.3581
09/05 04:39:48 PM: Update 29285: task mnli, batch 284 (29241): accuracy: 0.8619, mnli_loss: 0.3618
09/05 04:39:58 PM: Update 29324: task mnli, batch 323 (29280): accuracy: 0.8642, mnli_loss: 0.3620
09/05 04:40:09 PM: Update 29355: task mnli, batch 354 (29311): accuracy: 0.8625, mnli_loss: 0.3642
09/05 04:40:19 PM: Update 29393: task mnli, batch 392 (29349): accuracy: 0.8609, mnli_loss: 0.3659
09/05 04:40:29 PM: Update 29431: task mnli, batch 430 (29387): accuracy: 0.8594, mnli_loss: 0.3704
09/05 04:40:39 PM: Update 29469: task mnli, batch 468 (29425): accuracy: 0.8560, mnli_loss: 0.3783
09/05 04:40:49 PM: Update 29508: task mnli, batch 507 (29464): accuracy: 0.8558, mnli_loss: 0.3790
09/05 04:40:59 PM: Update 29548: task mnli, batch 547 (29504): accuracy: 0.8569, mnli_loss: 0.3784
09/05 04:41:10 PM: Update 29587: task mnli, batch 586 (29543): accuracy: 0.8556, mnli_loss: 0.3814
09/05 04:41:20 PM: Update 29624: task mnli, batch 623 (29580): accuracy: 0.8557, mnli_loss: 0.3817
09/05 04:41:25 PM: Update 29644: task copa, batch 2 (45): accuracy: 0.8333, copa_loss: 0.4752
09/05 04:41:30 PM: Update 29662: task mnli, batch 660 (29617): accuracy: 0.8547, mnli_loss: 0.3836
09/05 04:41:40 PM: Update 29699: task mnli, batch 697 (29654): accuracy: 0.8557, mnli_loss: 0.3820
09/05 04:41:45 PM: Update 29718: task copa, batch 3 (46): accuracy: 0.7778, copa_loss: 0.5027
09/05 04:41:50 PM: Update 29737: task mnli, batch 734 (29691): accuracy: 0.8545, mnli_loss: 0.3840
09/05 04:42:00 PM: Update 29769: task mnli, batch 766 (29723): accuracy: 0.8539, mnli_loss: 0.3847
09/05 04:42:10 PM: Update 29808: task copa, batch 4 (47): accuracy: 0.7812, copa_loss: 0.4839
09/05 04:42:11 PM: Update 29809: task mnli, batch 805 (29762): accuracy: 0.8542, mnli_loss: 0.3841
09/05 04:42:21 PM: Update 29849: task mnli, batch 845 (29802): accuracy: 0.8544, mnli_loss: 0.3833
09/05 04:42:31 PM: Update 29889: task mnli, batch 885 (29842): accuracy: 0.8546, mnli_loss: 0.3820
09/05 04:42:41 PM: Update 29926: task mnli, batch 922 (29879): accuracy: 0.8547, mnli_loss: 0.3810
09/05 04:42:51 PM: Update 29965: task mnli, batch 961 (29918): accuracy: 0.8544, mnli_loss: 0.3821
09/05 04:43:01 PM: ***** Step 30000 / Validation 30 *****
09/05 04:43:01 PM: copa: trained on 4 batches, 0.235 epochs
09/05 04:43:01 PM: mnli: trained on 996 batches, 0.061 epochs
09/05 04:43:01 PM: Validating...
09/05 04:43:01 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8750, copa_loss: 0.4280
09/05 04:43:01 PM: Evaluate: task mnli, batch 3 (209): accuracy: 0.7222, mnli_loss: 0.5791
09/05 04:43:11 PM: Evaluate: task mnli, batch 140 (209): accuracy: 0.8167, mnli_loss: 0.4939
09/05 04:43:16 PM: Updating LR scheduler:
09/05 04:43:16 PM: 	Best result seen so far for macro_avg: 0.800
09/05 04:43:16 PM: 	# validation passes without improvement: 0
09/05 04:43:16 PM: copa_loss: training: 0.483897 validation: 0.601372
09/05 04:43:16 PM: mnli_loss: training: 0.381337 validation: 0.501863
09/05 04:43:16 PM: macro_avg: validation: 0.793100
09/05 04:43:16 PM: micro_avg: validation: 0.815294
09/05 04:43:16 PM: copa_accuracy: training: 0.781250 validation: 0.770000
09/05 04:43:16 PM: mnli_accuracy: training: 0.854278 validation: 0.816200
09/05 04:43:16 PM: Global learning rate: 5e-06
09/05 04:43:16 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 04:43:22 PM: Update 30016: task mnli, batch 16 (29969): accuracy: 0.8307, mnli_loss: 0.3919
09/05 04:43:32 PM: Update 30054: task mnli, batch 54 (30007): accuracy: 0.8387, mnli_loss: 0.3875
09/05 04:43:42 PM: Update 30091: task mnli, batch 91 (30044): accuracy: 0.8407, mnli_loss: 0.3959
09/05 04:43:52 PM: Update 30128: task mnli, batch 128 (30081): accuracy: 0.8506, mnli_loss: 0.3750
09/05 04:44:02 PM: Update 30166: task mnli, batch 166 (30119): accuracy: 0.8524, mnli_loss: 0.3747
09/05 04:44:12 PM: Update 30194: task mnli, batch 194 (30147): accuracy: 0.8528, mnli_loss: 0.3753
09/05 04:44:22 PM: Update 30235: task mnli, batch 235 (30188): accuracy: 0.8555, mnli_loss: 0.3728
09/05 04:44:32 PM: Update 30273: task mnli, batch 273 (30226): accuracy: 0.8535, mnli_loss: 0.3779
09/05 04:44:43 PM: Update 30312: task mnli, batch 312 (30265): accuracy: 0.8547, mnli_loss: 0.3755
09/05 04:44:53 PM: Update 30350: task mnli, batch 350 (30303): accuracy: 0.8550, mnli_loss: 0.3764
09/05 04:45:03 PM: Update 30388: task mnli, batch 388 (30341): accuracy: 0.8568, mnli_loss: 0.3761
09/05 04:45:13 PM: Update 30426: task mnli, batch 426 (30379): accuracy: 0.8559, mnli_loss: 0.3760
09/05 04:45:23 PM: Update 30465: task mnli, batch 465 (30418): accuracy: 0.8550, mnli_loss: 0.3803
09/05 04:45:33 PM: Update 30503: task mnli, batch 503 (30456): accuracy: 0.8552, mnli_loss: 0.3809
09/05 04:45:43 PM: Update 30541: task mnli, batch 541 (30494): accuracy: 0.8540, mnli_loss: 0.3825
09/05 04:45:54 PM: Update 30578: task mnli, batch 578 (30531): accuracy: 0.8534, mnli_loss: 0.3860
09/05 04:46:04 PM: Update 30609: task mnli, batch 609 (30562): accuracy: 0.8535, mnli_loss: 0.3862
09/05 04:46:14 PM: Update 30649: task mnli, batch 649 (30602): accuracy: 0.8538, mnli_loss: 0.3860
09/05 04:46:24 PM: Update 30689: task mnli, batch 689 (30642): accuracy: 0.8559, mnli_loss: 0.3818
09/05 04:46:35 PM: Update 30725: task mnli, batch 725 (30678): accuracy: 0.8554, mnli_loss: 0.3833
09/05 04:46:45 PM: Update 30762: task mnli, batch 762 (30715): accuracy: 0.8543, mnli_loss: 0.3850
09/05 04:46:55 PM: Update 30803: task mnli, batch 803 (30756): accuracy: 0.8545, mnli_loss: 0.3836
09/05 04:47:05 PM: Update 30841: task mnli, batch 841 (30794): accuracy: 0.8543, mnli_loss: 0.3839
09/05 04:47:15 PM: Update 30880: task mnli, batch 880 (30833): accuracy: 0.8535, mnli_loss: 0.3846
09/05 04:47:25 PM: Update 30918: task mnli, batch 918 (30871): accuracy: 0.8542, mnli_loss: 0.3828
09/05 04:47:35 PM: Update 30956: task mnli, batch 956 (30909): accuracy: 0.8540, mnli_loss: 0.3830
09/05 04:47:46 PM: Update 30994: task mnli, batch 994 (30947): accuracy: 0.8537, mnli_loss: 0.3830
09/05 04:47:47 PM: ***** Step 31000 / Validation 31 *****
09/05 04:47:47 PM: copa: trained on 0 batches, 0.000 epochs
09/05 04:47:47 PM: mnli: trained on 1000 batches, 0.061 epochs
09/05 04:47:47 PM: Validating...
09/05 04:47:47 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8750, copa_loss: 0.4264
09/05 04:47:56 PM: Evaluate: task mnli, batch 109 (209): accuracy: 0.8173, mnli_loss: 0.4895
09/05 04:48:03 PM: Updating LR scheduler:
09/05 04:48:03 PM: 	Best result seen so far for macro_avg: 0.800
09/05 04:48:03 PM: 	# validation passes without improvement: 1
09/05 04:48:03 PM: copa_loss: training: 0.000000 validation: 0.599036
09/05 04:48:03 PM: mnli_loss: training: 0.383197 validation: 0.507265
09/05 04:48:03 PM: macro_avg: validation: 0.796100
09/05 04:48:03 PM: micro_avg: validation: 0.811569
09/05 04:48:03 PM: copa_accuracy: validation: 0.780000
09/05 04:48:03 PM: mnli_accuracy: training: 0.853611 validation: 0.812200
09/05 04:48:03 PM: Global learning rate: 5e-06
09/05 04:48:03 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 04:48:06 PM: Update 31007: task mnli, batch 7 (30960): accuracy: 0.8810, mnli_loss: 0.3020
09/05 04:48:16 PM: Update 31036: task mnli, batch 36 (30989): accuracy: 0.8680, mnli_loss: 0.3422
09/05 04:48:26 PM: Update 31074: task mnli, batch 74 (31027): accuracy: 0.8586, mnli_loss: 0.3693
09/05 04:48:36 PM: Update 31113: task mnli, batch 113 (31066): accuracy: 0.8513, mnli_loss: 0.3908
09/05 04:48:47 PM: Update 31151: task mnli, batch 151 (31104): accuracy: 0.8501, mnli_loss: 0.3937
09/05 04:48:57 PM: Update 31189: task mnli, batch 189 (31142): accuracy: 0.8487, mnli_loss: 0.3934
09/05 04:49:04 PM: Update 31216: task copa, batch 1 (48): accuracy: 0.7917, copa_loss: 0.4723
09/05 04:49:07 PM: Update 31227: task mnli, batch 226 (31179): accuracy: 0.8519, mnli_loss: 0.3881
09/05 04:49:17 PM: Update 31266: task mnli, batch 265 (31218): accuracy: 0.8561, mnli_loss: 0.3767
09/05 04:49:27 PM: Update 31304: task mnli, batch 303 (31256): accuracy: 0.8559, mnli_loss: 0.3787
09/05 04:49:37 PM: Update 31342: task mnli, batch 341 (31294): accuracy: 0.8560, mnli_loss: 0.3801
09/05 04:49:47 PM: Update 31381: task mnli, batch 380 (31333): accuracy: 0.8573, mnli_loss: 0.3775
09/05 04:49:58 PM: Update 31421: task mnli, batch 420 (31373): accuracy: 0.8577, mnli_loss: 0.3754
09/05 04:50:08 PM: Update 31452: task mnli, batch 451 (31404): accuracy: 0.8577, mnli_loss: 0.3741
09/05 04:50:18 PM: Update 31489: task mnli, batch 488 (31441): accuracy: 0.8558, mnli_loss: 0.3757
09/05 04:50:28 PM: Update 31527: task mnli, batch 526 (31479): accuracy: 0.8549, mnli_loss: 0.3767
09/05 04:50:38 PM: Update 31564: task mnli, batch 563 (31516): accuracy: 0.8537, mnli_loss: 0.3805
09/05 04:50:48 PM: Update 31603: task mnli, batch 602 (31555): accuracy: 0.8547, mnli_loss: 0.3791
09/05 04:50:50 PM: Update 31610: task copa, batch 2 (49): accuracy: 0.7708, copa_loss: 0.5126
09/05 04:50:58 PM: Update 31642: task mnli, batch 640 (31593): accuracy: 0.8538, mnli_loss: 0.3794
09/05 04:51:08 PM: Update 31681: task mnli, batch 679 (31632): accuracy: 0.8539, mnli_loss: 0.3802
09/05 04:51:19 PM: Update 31720: task mnli, batch 718 (31671): accuracy: 0.8533, mnli_loss: 0.3820
09/05 04:51:29 PM: Update 31759: task mnli, batch 757 (31710): accuracy: 0.8532, mnli_loss: 0.3825
09/05 04:51:39 PM: Update 31796: task mnli, batch 794 (31747): accuracy: 0.8534, mnli_loss: 0.3820
09/05 04:51:49 PM: Update 31835: task mnli, batch 833 (31786): accuracy: 0.8534, mnli_loss: 0.3818
09/05 04:51:59 PM: Update 31866: task mnli, batch 864 (31817): accuracy: 0.8528, mnli_loss: 0.3835
09/05 04:52:10 PM: Update 31905: task mnli, batch 903 (31856): accuracy: 0.8529, mnli_loss: 0.3834
09/05 04:52:20 PM: Update 31945: task mnli, batch 943 (31896): accuracy: 0.8534, mnli_loss: 0.3825
09/05 04:52:30 PM: Update 31983: task mnli, batch 981 (31934): accuracy: 0.8537, mnli_loss: 0.3821
09/05 04:52:35 PM: ***** Step 32000 / Validation 32 *****
09/05 04:52:35 PM: copa: trained on 2 batches, 0.118 epochs
09/05 04:52:35 PM: mnli: trained on 998 batches, 0.061 epochs
09/05 04:52:35 PM: Validating...
09/05 04:52:35 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8750, copa_loss: 0.4239
09/05 04:52:40 PM: Evaluate: task mnli, batch 69 (209): accuracy: 0.8225, mnli_loss: 0.4778
09/05 04:52:50 PM: Evaluate: task mnli, batch 205 (209): accuracy: 0.8148, mnli_loss: 0.4940
09/05 04:52:50 PM: Updating LR scheduler:
09/05 04:52:50 PM: 	Best result seen so far for macro_avg: 0.800
09/05 04:52:50 PM: 	# validation passes without improvement: 2
09/05 04:52:50 PM: copa_loss: training: 0.512567 validation: 0.605151
09/05 04:52:50 PM: mnli_loss: training: 0.382533 validation: 0.497604
09/05 04:52:50 PM: macro_avg: validation: 0.791600
09/05 04:52:50 PM: micro_avg: validation: 0.812353
09/05 04:52:50 PM: copa_accuracy: training: 0.770833 validation: 0.770000
09/05 04:52:50 PM: mnli_accuracy: training: 0.853644 validation: 0.813200
09/05 04:52:50 PM: Global learning rate: 5e-06
09/05 04:52:50 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 04:53:00 PM: Update 32035: task mnli, batch 35 (31986): accuracy: 0.8679, mnli_loss: 0.3558
09/05 04:53:10 PM: Update 32073: task mnli, batch 73 (32024): accuracy: 0.8653, mnli_loss: 0.3553
09/05 04:53:21 PM: Update 32112: task mnli, batch 112 (32063): accuracy: 0.8653, mnli_loss: 0.3600
09/05 04:53:31 PM: Update 32150: task mnli, batch 150 (32101): accuracy: 0.8669, mnli_loss: 0.3533
09/05 04:53:41 PM: Update 32188: task mnli, batch 188 (32139): accuracy: 0.8659, mnli_loss: 0.3563
09/05 04:53:51 PM: Update 32227: task mnli, batch 227 (32178): accuracy: 0.8596, mnli_loss: 0.3661
09/05 04:54:01 PM: Update 32263: task mnli, batch 263 (32214): accuracy: 0.8552, mnli_loss: 0.3735
09/05 04:54:11 PM: Update 32292: task mnli, batch 292 (32243): accuracy: 0.8571, mnli_loss: 0.3688
09/05 04:54:22 PM: Update 32330: task mnli, batch 330 (32281): accuracy: 0.8541, mnli_loss: 0.3765
09/05 04:54:32 PM: Update 32370: task mnli, batch 370 (32321): accuracy: 0.8527, mnli_loss: 0.3804
09/05 04:54:42 PM: Update 32409: task mnli, batch 409 (32360): accuracy: 0.8540, mnli_loss: 0.3792
09/05 04:54:52 PM: Update 32446: task mnli, batch 446 (32397): accuracy: 0.8541, mnli_loss: 0.3777
09/05 04:55:02 PM: Update 32484: task mnli, batch 484 (32435): accuracy: 0.8555, mnli_loss: 0.3750
09/05 04:55:12 PM: Update 32522: task mnli, batch 522 (32473): accuracy: 0.8561, mnli_loss: 0.3737
09/05 04:55:22 PM: Update 32559: task mnli, batch 559 (32510): accuracy: 0.8550, mnli_loss: 0.3768
09/05 04:55:32 PM: Update 32596: task copa, batch 1 (50): accuracy: 0.7917, copa_loss: 0.4713
09/05 04:55:33 PM: Update 32598: task mnli, batch 597 (32548): accuracy: 0.8541, mnli_loss: 0.3785
09/05 04:55:43 PM: Update 32636: task mnli, batch 635 (32586): accuracy: 0.8550, mnli_loss: 0.3772
09/05 04:55:53 PM: Update 32674: task mnli, batch 673 (32624): accuracy: 0.8549, mnli_loss: 0.3767
09/05 04:56:03 PM: Update 32710: task mnli, batch 709 (32660): accuracy: 0.8537, mnli_loss: 0.3798
09/05 04:56:13 PM: Update 32748: task mnli, batch 747 (32698): accuracy: 0.8544, mnli_loss: 0.3784
09/05 04:56:23 PM: Update 32788: task mnli, batch 787 (32738): accuracy: 0.8547, mnli_loss: 0.3774
09/05 04:56:33 PM: Update 32820: task mnli, batch 819 (32770): accuracy: 0.8543, mnli_loss: 0.3776
09/05 04:56:44 PM: Update 32859: task mnli, batch 858 (32809): accuracy: 0.8551, mnli_loss: 0.3763
09/05 04:56:54 PM: Update 32897: task mnli, batch 896 (32847): accuracy: 0.8554, mnli_loss: 0.3752
09/05 04:57:04 PM: Update 32935: task mnli, batch 934 (32885): accuracy: 0.8556, mnli_loss: 0.3752
09/05 04:57:14 PM: Update 32974: task mnli, batch 973 (32924): accuracy: 0.8558, mnli_loss: 0.3755
09/05 04:57:21 PM: ***** Step 33000 / Validation 33 *****
09/05 04:57:21 PM: copa: trained on 1 batches, 0.059 epochs
09/05 04:57:21 PM: mnli: trained on 999 batches, 0.061 epochs
09/05 04:57:21 PM: Validating...
09/05 04:57:21 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8333, copa_loss: 0.4230
09/05 04:57:24 PM: Evaluate: task mnli, batch 41 (209): accuracy: 0.8252, mnli_loss: 0.4622
09/05 04:57:34 PM: Evaluate: task mnli, batch 177 (209): accuracy: 0.8246, mnli_loss: 0.4731
09/05 04:57:37 PM: Best result seen so far for mnli.
09/05 04:57:37 PM: Best result seen so far for micro.
09/05 04:57:37 PM: Updating LR scheduler:
09/05 04:57:37 PM: 	Best result seen so far for macro_avg: 0.800
09/05 04:57:37 PM: 	# validation passes without improvement: 3
09/05 04:57:37 PM: copa_loss: training: 0.471344 validation: 0.614895
09/05 04:57:37 PM: mnli_loss: training: 0.375789 validation: 0.481182
09/05 04:57:37 PM: macro_avg: validation: 0.786300
09/05 04:57:37 PM: micro_avg: validation: 0.821176
09/05 04:57:37 PM: copa_accuracy: training: 0.791667 validation: 0.750000
09/05 04:57:37 PM: mnli_accuracy: training: 0.855866 validation: 0.822600
09/05 04:57:37 PM: Global learning rate: 5e-06
09/05 04:57:37 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 04:57:44 PM: Update 33023: task mnli, batch 23 (32973): accuracy: 0.8478, mnli_loss: 0.3608
09/05 04:57:55 PM: Update 33063: task mnli, batch 63 (33013): accuracy: 0.8591, mnli_loss: 0.3502
09/05 04:58:05 PM: Update 33102: task mnli, batch 102 (33052): accuracy: 0.8652, mnli_loss: 0.3423
09/05 04:58:15 PM: Update 33140: task mnli, batch 140 (33090): accuracy: 0.8568, mnli_loss: 0.3678
09/05 04:58:25 PM: Update 33179: task mnli, batch 179 (33129): accuracy: 0.8603, mnli_loss: 0.3589
09/05 04:58:35 PM: Update 33217: task mnli, batch 217 (33167): accuracy: 0.8614, mnli_loss: 0.3590
09/05 04:58:45 PM: Update 33246: task mnli, batch 246 (33196): accuracy: 0.8647, mnli_loss: 0.3517
09/05 04:58:56 PM: Update 33284: task mnli, batch 284 (33234): accuracy: 0.8597, mnli_loss: 0.3592
09/05 04:59:06 PM: Update 33321: task mnli, batch 321 (33271): accuracy: 0.8582, mnli_loss: 0.3626
09/05 04:59:16 PM: Update 33360: task mnli, batch 360 (33310): accuracy: 0.8585, mnli_loss: 0.3635
09/05 04:59:26 PM: Update 33398: task mnli, batch 398 (33348): accuracy: 0.8596, mnli_loss: 0.3626
09/05 04:59:36 PM: Update 33437: task mnli, batch 437 (33387): accuracy: 0.8603, mnli_loss: 0.3612
09/05 04:59:46 PM: Update 33476: task mnli, batch 476 (33426): accuracy: 0.8597, mnli_loss: 0.3631
09/05 04:59:57 PM: Update 33516: task mnli, batch 516 (33466): accuracy: 0.8604, mnli_loss: 0.3617
09/05 05:00:07 PM: Update 33555: task mnli, batch 555 (33505): accuracy: 0.8607, mnli_loss: 0.3608
09/05 05:00:17 PM: Update 33594: task mnli, batch 594 (33544): accuracy: 0.8616, mnli_loss: 0.3596
09/05 05:00:27 PM: Update 33632: task mnli, batch 632 (33582): accuracy: 0.8625, mnli_loss: 0.3580
09/05 05:00:37 PM: Update 33663: task mnli, batch 663 (33613): accuracy: 0.8615, mnli_loss: 0.3606
09/05 05:00:47 PM: Update 33703: task mnli, batch 703 (33653): accuracy: 0.8608, mnli_loss: 0.3616
09/05 05:00:58 PM: Update 33742: task mnli, batch 742 (33692): accuracy: 0.8611, mnli_loss: 0.3613
09/05 05:01:08 PM: Update 33780: task mnli, batch 780 (33730): accuracy: 0.8615, mnli_loss: 0.3605
09/05 05:01:18 PM: Update 33819: task mnli, batch 819 (33769): accuracy: 0.8609, mnli_loss: 0.3624
09/05 05:01:28 PM: Update 33857: task mnli, batch 857 (33807): accuracy: 0.8607, mnli_loss: 0.3632
09/05 05:01:38 PM: Update 33895: task mnli, batch 895 (33845): accuracy: 0.8602, mnli_loss: 0.3639
09/05 05:01:48 PM: Update 33933: task mnli, batch 933 (33883): accuracy: 0.8610, mnli_loss: 0.3632
09/05 05:01:58 PM: Update 33971: task mnli, batch 971 (33921): accuracy: 0.8612, mnli_loss: 0.3630
09/05 05:02:06 PM: ***** Step 34000 / Validation 34 *****
09/05 05:02:06 PM: copa: trained on 0 batches, 0.000 epochs
09/05 05:02:06 PM: mnli: trained on 1000 batches, 0.061 epochs
09/05 05:02:06 PM: Validating...
09/05 05:02:06 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8333, copa_loss: 0.4176
09/05 05:02:08 PM: Evaluate: task mnli, batch 26 (209): accuracy: 0.8205, mnli_loss: 0.4662
09/05 05:02:18 PM: Evaluate: task mnli, batch 162 (209): accuracy: 0.8223, mnli_loss: 0.4858
09/05 05:02:22 PM: Best result seen so far for macro.
09/05 05:02:22 PM: Updating LR scheduler:
09/05 05:02:22 PM: 	Best result seen so far for macro_avg: 0.800
09/05 05:02:22 PM: 	# validation passes without improvement: 0
09/05 05:02:22 PM: copa_loss: training: 0.000000 validation: 0.609750
09/05 05:02:22 PM: mnli_loss: training: 0.363139 validation: 0.489273
09/05 05:02:22 PM: macro_avg: validation: 0.800400
09/05 05:02:22 PM: micro_avg: validation: 0.820000
09/05 05:02:22 PM: copa_accuracy: validation: 0.780000
09/05 05:02:22 PM: mnli_accuracy: training: 0.861157 validation: 0.820800
09/05 05:02:22 PM: Global learning rate: 5e-06
09/05 05:02:22 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 05:02:29 PM: Update 34023: task mnli, batch 23 (33973): accuracy: 0.8696, mnli_loss: 0.3294
09/05 05:02:32 PM: Update 34034: task copa, batch 1 (51): accuracy: 0.6667, copa_loss: 0.5992
09/05 05:02:39 PM: Update 34055: task mnli, batch 54 (34004): accuracy: 0.8734, mnli_loss: 0.3245
09/05 05:02:48 PM: Update 34089: task copa, batch 2 (52): accuracy: 0.6750, copa_loss: 0.5813
09/05 05:02:49 PM: Update 34093: task mnli, batch 91 (34041): accuracy: 0.8653, mnli_loss: 0.3522
09/05 05:03:00 PM: Update 34131: task mnli, batch 129 (34079): accuracy: 0.8685, mnli_loss: 0.3498
09/05 05:03:10 PM: Update 34169: task mnli, batch 167 (34117): accuracy: 0.8682, mnli_loss: 0.3576
09/05 05:03:20 PM: Update 34209: task mnli, batch 207 (34157): accuracy: 0.8677, mnli_loss: 0.3539
09/05 05:03:30 PM: Update 34246: task mnli, batch 244 (34194): accuracy: 0.8661, mnli_loss: 0.3599
09/05 05:03:40 PM: Update 34285: task mnli, batch 283 (34233): accuracy: 0.8673, mnli_loss: 0.3545
09/05 05:03:50 PM: Update 34325: task mnli, batch 323 (34273): accuracy: 0.8657, mnli_loss: 0.3561
09/05 05:04:00 PM: Update 34364: task mnli, batch 362 (34312): accuracy: 0.8676, mnli_loss: 0.3534
09/05 05:04:10 PM: Update 34403: task mnli, batch 401 (34351): accuracy: 0.8683, mnli_loss: 0.3516
09/05 05:04:21 PM: Update 34440: task mnli, batch 438 (34388): accuracy: 0.8669, mnli_loss: 0.3544
09/05 05:04:31 PM: Update 34473: task mnli, batch 471 (34421): accuracy: 0.8660, mnli_loss: 0.3546
09/05 05:04:42 PM: Update 34512: task mnli, batch 510 (34460): accuracy: 0.8653, mnli_loss: 0.3556
09/05 05:04:52 PM: Update 34551: task mnli, batch 549 (34499): accuracy: 0.8644, mnli_loss: 0.3571
09/05 05:05:02 PM: Update 34589: task mnli, batch 587 (34537): accuracy: 0.8652, mnli_loss: 0.3567
09/05 05:05:12 PM: Update 34627: task mnli, batch 625 (34575): accuracy: 0.8647, mnli_loss: 0.3563
09/05 05:05:22 PM: Update 34665: task mnli, batch 663 (34613): accuracy: 0.8642, mnli_loss: 0.3571
09/05 05:05:28 PM: Update 34686: task copa, batch 3 (53): accuracy: 0.7031, copa_loss: 0.5319
09/05 05:05:32 PM: Update 34702: task mnli, batch 699 (34649): accuracy: 0.8640, mnli_loss: 0.3576
09/05 05:05:42 PM: Update 34742: task mnli, batch 739 (34689): accuracy: 0.8630, mnli_loss: 0.3600
09/05 05:05:52 PM: Update 34781: task mnli, batch 778 (34728): accuracy: 0.8628, mnli_loss: 0.3607
09/05 05:06:02 PM: Update 34817: task mnli, batch 814 (34764): accuracy: 0.8629, mnli_loss: 0.3610
09/05 05:06:13 PM: Update 34857: task mnli, batch 854 (34804): accuracy: 0.8633, mnli_loss: 0.3590
09/05 05:06:24 PM: Update 34891: task mnli, batch 888 (34838): accuracy: 0.8627, mnli_loss: 0.3604
09/05 05:06:29 PM: Update 34910: task copa, batch 4 (54): accuracy: 0.7273, copa_loss: 0.5166
09/05 05:06:34 PM: Update 34930: task mnli, batch 926 (34876): accuracy: 0.8631, mnli_loss: 0.3596
09/05 05:06:44 PM: Update 34968: task mnli, batch 964 (34914): accuracy: 0.8631, mnli_loss: 0.3591
09/05 05:06:52 PM: ***** Step 35000 / Validation 35 *****
09/05 05:06:52 PM: copa: trained on 4 batches, 0.235 epochs
09/05 05:06:52 PM: mnli: trained on 996 batches, 0.061 epochs
09/05 05:06:52 PM: Validating...
09/05 05:06:52 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8333, copa_loss: 0.4180
09/05 05:06:54 PM: Evaluate: task mnli, batch 19 (209): accuracy: 0.8136, mnli_loss: 0.4757
09/05 05:07:04 PM: Evaluate: task mnli, batch 156 (209): accuracy: 0.8235, mnli_loss: 0.4822
09/05 05:07:08 PM: Best result seen so far for macro.
09/05 05:07:08 PM: Updating LR scheduler:
09/05 05:07:08 PM: 	Best result seen so far for macro_avg: 0.801
09/05 05:07:08 PM: 	# validation passes without improvement: 0
09/05 05:07:08 PM: copa_loss: training: 0.516596 validation: 0.604701
09/05 05:07:08 PM: mnli_loss: training: 0.358753 validation: 0.485312
09/05 05:07:08 PM: macro_avg: validation: 0.801000
09/05 05:07:08 PM: micro_avg: validation: 0.821176
09/05 05:07:08 PM: copa_accuracy: training: 0.727273 validation: 0.780000
09/05 05:07:08 PM: mnli_accuracy: training: 0.863819 validation: 0.822000
09/05 05:07:08 PM: Global learning rate: 5e-06
09/05 05:07:08 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 05:07:15 PM: Update 35022: task mnli, batch 22 (34968): accuracy: 0.8561, mnli_loss: 0.3816
09/05 05:07:25 PM: Update 35062: task mnli, batch 62 (35008): accuracy: 0.8669, mnli_loss: 0.3665
09/05 05:07:35 PM: Update 35100: task mnli, batch 100 (35046): accuracy: 0.8633, mnli_loss: 0.3733
09/05 05:07:45 PM: Update 35138: task mnli, batch 138 (35084): accuracy: 0.8699, mnli_loss: 0.3568
09/05 05:07:55 PM: Update 35178: task mnli, batch 178 (35124): accuracy: 0.8720, mnli_loss: 0.3526
09/05 05:08:05 PM: Update 35215: task mnli, batch 215 (35161): accuracy: 0.8686, mnli_loss: 0.3599
09/05 05:08:15 PM: Update 35251: task mnli, batch 251 (35197): accuracy: 0.8737, mnli_loss: 0.3480
09/05 05:08:25 PM: Update 35290: task mnli, batch 290 (35236): accuracy: 0.8724, mnli_loss: 0.3506
09/05 05:08:35 PM: Update 35321: task mnli, batch 321 (35267): accuracy: 0.8698, mnli_loss: 0.3531
09/05 05:08:46 PM: Update 35360: task mnli, batch 360 (35306): accuracy: 0.8698, mnli_loss: 0.3539
09/05 05:08:56 PM: Update 35398: task mnli, batch 398 (35344): accuracy: 0.8692, mnli_loss: 0.3555
09/05 05:09:06 PM: Update 35437: task mnli, batch 437 (35383): accuracy: 0.8691, mnli_loss: 0.3558
09/05 05:09:16 PM: Update 35475: task mnli, batch 475 (35421): accuracy: 0.8676, mnli_loss: 0.3582
09/05 05:09:26 PM: Update 35513: task mnli, batch 513 (35459): accuracy: 0.8684, mnli_loss: 0.3554
09/05 05:09:36 PM: Update 35551: task mnli, batch 551 (35497): accuracy: 0.8686, mnli_loss: 0.3540
09/05 05:09:46 PM: Update 35588: task mnli, batch 588 (35534): accuracy: 0.8688, mnli_loss: 0.3538
09/05 05:09:57 PM: Update 35626: task mnli, batch 626 (35572): accuracy: 0.8688, mnli_loss: 0.3540
09/05 05:10:07 PM: Update 35665: task mnli, batch 665 (35611): accuracy: 0.8700, mnli_loss: 0.3517
09/05 05:10:17 PM: Update 35703: task mnli, batch 703 (35649): accuracy: 0.8699, mnli_loss: 0.3528
09/05 05:10:27 PM: Update 35732: task mnli, batch 732 (35678): accuracy: 0.8700, mnli_loss: 0.3516
09/05 05:10:37 PM: Update 35772: task mnli, batch 772 (35718): accuracy: 0.8703, mnli_loss: 0.3501
09/05 05:10:48 PM: Update 35810: task mnli, batch 810 (35756): accuracy: 0.8692, mnli_loss: 0.3512
09/05 05:10:58 PM: Update 35849: task mnli, batch 849 (35795): accuracy: 0.8693, mnli_loss: 0.3499
09/05 05:11:08 PM: Update 35887: task mnli, batch 887 (35833): accuracy: 0.8695, mnli_loss: 0.3493
09/05 05:11:18 PM: Update 35926: task mnli, batch 926 (35872): accuracy: 0.8694, mnli_loss: 0.3498
09/05 05:11:28 PM: Update 35963: task mnli, batch 963 (35909): accuracy: 0.8687, mnli_loss: 0.3505
09/05 05:11:38 PM: Update 36000: task mnli, batch 1000 (35946): accuracy: 0.8696, mnli_loss: 0.3491
09/05 05:11:38 PM: ***** Step 36000 / Validation 36 *****
09/05 05:11:38 PM: copa: trained on 0 batches, 0.000 epochs
09/05 05:11:38 PM: mnli: trained on 1000 batches, 0.061 epochs
09/05 05:11:38 PM: Validating...
09/05 05:11:38 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8333, copa_loss: 0.4176
09/05 05:11:48 PM: Evaluate: task mnli, batch 132 (209): accuracy: 0.8235, mnli_loss: 0.4931
09/05 05:11:54 PM: Best result seen so far for micro.
09/05 05:11:54 PM: Updating LR scheduler:
09/05 05:11:54 PM: 	Best result seen so far for macro_avg: 0.801
09/05 05:11:54 PM: 	# validation passes without improvement: 1
09/05 05:11:54 PM: copa_loss: training: 0.000000 validation: 0.597420
09/05 05:11:54 PM: mnli_loss: training: 0.349075 validation: 0.500765
09/05 05:11:54 PM: macro_avg: validation: 0.796200
09/05 05:11:54 PM: micro_avg: validation: 0.821373
09/05 05:11:54 PM: copa_accuracy: validation: 0.770000
09/05 05:11:54 PM: mnli_accuracy: training: 0.869621 validation: 0.822400
09/05 05:11:54 PM: Global learning rate: 5e-06
09/05 05:11:54 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 05:11:58 PM: Update 36014: task mnli, batch 14 (35960): accuracy: 0.8631, mnli_loss: 0.3375
09/05 05:12:08 PM: Update 36054: task mnli, batch 54 (36000): accuracy: 0.8789, mnli_loss: 0.3179
09/05 05:12:19 PM: Update 36093: task mnli, batch 93 (36039): accuracy: 0.8804, mnli_loss: 0.3187
09/05 05:12:29 PM: Update 36131: task mnli, batch 131 (36077): accuracy: 0.8744, mnli_loss: 0.3330
09/05 05:12:39 PM: Update 36162: task mnli, batch 162 (36108): accuracy: 0.8711, mnli_loss: 0.3331
09/05 05:12:49 PM: Update 36202: task mnli, batch 202 (36148): accuracy: 0.8742, mnli_loss: 0.3304
09/05 05:12:59 PM: Update 36240: task mnli, batch 240 (36186): accuracy: 0.8700, mnli_loss: 0.3423
09/05 05:13:09 PM: Update 36277: task mnli, batch 277 (36223): accuracy: 0.8702, mnli_loss: 0.3419
09/05 05:13:20 PM: Update 36315: task mnli, batch 315 (36261): accuracy: 0.8692, mnli_loss: 0.3435
09/05 05:13:30 PM: Update 36353: task mnli, batch 353 (36299): accuracy: 0.8679, mnli_loss: 0.3475
09/05 05:13:40 PM: Update 36392: task mnli, batch 392 (36338): accuracy: 0.8681, mnli_loss: 0.3468
09/05 05:13:50 PM: Update 36431: task mnli, batch 431 (36377): accuracy: 0.8684, mnli_loss: 0.3472
09/05 05:14:00 PM: Update 36469: task mnli, batch 469 (36415): accuracy: 0.8673, mnli_loss: 0.3477
09/05 05:14:10 PM: Update 36507: task mnli, batch 507 (36453): accuracy: 0.8657, mnli_loss: 0.3496
09/05 05:14:20 PM: Update 36545: task mnli, batch 545 (36491): accuracy: 0.8660, mnli_loss: 0.3504
09/05 05:14:31 PM: Update 36578: task mnli, batch 578 (36524): accuracy: 0.8666, mnli_loss: 0.3491
09/05 05:14:41 PM: Update 36616: task mnli, batch 616 (36562): accuracy: 0.8682, mnli_loss: 0.3464
09/05 05:14:51 PM: Update 36654: task mnli, batch 654 (36600): accuracy: 0.8698, mnli_loss: 0.3440
09/05 05:15:01 PM: Update 36694: task mnli, batch 694 (36640): accuracy: 0.8709, mnli_loss: 0.3418
09/05 05:15:11 PM: Update 36733: task mnli, batch 733 (36679): accuracy: 0.8716, mnli_loss: 0.3404
09/05 05:15:22 PM: Update 36771: task mnli, batch 771 (36717): accuracy: 0.8727, mnli_loss: 0.3373
09/05 05:15:32 PM: Update 36810: task mnli, batch 810 (36756): accuracy: 0.8737, mnli_loss: 0.3345
09/05 05:15:42 PM: Update 36849: task mnli, batch 849 (36795): accuracy: 0.8738, mnli_loss: 0.3361
09/05 05:15:52 PM: Update 36888: task mnli, batch 888 (36834): accuracy: 0.8738, mnli_loss: 0.3371
09/05 05:16:02 PM: Update 36926: task mnli, batch 926 (36872): accuracy: 0.8738, mnli_loss: 0.3366
09/05 05:16:12 PM: Update 36962: task mnli, batch 962 (36908): accuracy: 0.8732, mnli_loss: 0.3381
09/05 05:16:22 PM: Update 36995: task mnli, batch 995 (36941): accuracy: 0.8732, mnli_loss: 0.3386
09/05 05:16:24 PM: ***** Step 37000 / Validation 37 *****
09/05 05:16:24 PM: copa: trained on 0 batches, 0.000 epochs
09/05 05:16:24 PM: mnli: trained on 1000 batches, 0.061 epochs
09/05 05:16:24 PM: Validating...
09/05 05:16:24 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8333, copa_loss: 0.4179
09/05 05:16:32 PM: Evaluate: task mnli, batch 107 (209): accuracy: 0.8244, mnli_loss: 0.4841
09/05 05:16:40 PM: Updating LR scheduler:
09/05 05:16:40 PM: 	Best result seen so far for macro_avg: 0.801
09/05 05:16:40 PM: 	# validation passes without improvement: 2
09/05 05:16:40 PM: copa_loss: training: 0.000000 validation: 0.596971
09/05 05:16:40 PM: mnli_loss: training: 0.338523 validation: 0.497602
09/05 05:16:40 PM: macro_avg: validation: 0.795700
09/05 05:16:40 PM: micro_avg: validation: 0.820392
09/05 05:16:40 PM: copa_accuracy: validation: 0.770000
09/05 05:16:40 PM: mnli_accuracy: training: 0.873123 validation: 0.821400
09/05 05:16:40 PM: Global learning rate: 5e-06
09/05 05:16:40 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 05:16:42 PM: Update 37006: task mnli, batch 6 (36952): accuracy: 0.8611, mnli_loss: 0.3281
09/05 05:16:53 PM: Update 37043: task mnli, batch 43 (36989): accuracy: 0.8721, mnli_loss: 0.3362
09/05 05:17:03 PM: Update 37081: task mnli, batch 81 (37027): accuracy: 0.8729, mnli_loss: 0.3434
09/05 05:17:13 PM: Update 37119: task mnli, batch 119 (37065): accuracy: 0.8743, mnli_loss: 0.3410
09/05 05:17:23 PM: Update 37159: task mnli, batch 159 (37105): accuracy: 0.8766, mnli_loss: 0.3355
09/05 05:17:33 PM: Update 37196: task mnli, batch 196 (37142): accuracy: 0.8799, mnli_loss: 0.3283
09/05 05:17:43 PM: Update 37233: task mnli, batch 233 (37179): accuracy: 0.8795, mnli_loss: 0.3302
09/05 05:17:54 PM: Update 37272: task mnli, batch 272 (37218): accuracy: 0.8816, mnli_loss: 0.3255
09/05 05:18:04 PM: Update 37311: task mnli, batch 311 (37257): accuracy: 0.8821, mnli_loss: 0.3203
09/05 05:18:14 PM: Update 37351: task mnli, batch 351 (37297): accuracy: 0.8834, mnli_loss: 0.3184
09/05 05:18:24 PM: Update 37389: task mnli, batch 389 (37335): accuracy: 0.8809, mnli_loss: 0.3204
09/05 05:18:34 PM: Update 37420: task mnli, batch 420 (37366): accuracy: 0.8807, mnli_loss: 0.3208
09/05 05:18:44 PM: Update 37459: task mnli, batch 459 (37405): accuracy: 0.8802, mnli_loss: 0.3225
09/05 05:18:54 PM: Update 37496: task mnli, batch 496 (37442): accuracy: 0.8794, mnli_loss: 0.3245
09/05 05:19:04 PM: Update 37535: task mnli, batch 535 (37481): accuracy: 0.8822, mnli_loss: 0.3195
09/05 05:19:14 PM: Update 37573: task mnli, batch 573 (37519): accuracy: 0.8817, mnli_loss: 0.3204
09/05 05:19:24 PM: Update 37611: task mnli, batch 611 (37557): accuracy: 0.8813, mnli_loss: 0.3183
09/05 05:19:34 PM: Update 37648: task mnli, batch 648 (37594): accuracy: 0.8805, mnli_loss: 0.3193
09/05 05:19:44 PM: Update 37686: task mnli, batch 686 (37632): accuracy: 0.8803, mnli_loss: 0.3190
09/05 05:19:54 PM: Update 37722: task mnli, batch 722 (37668): accuracy: 0.8795, mnli_loss: 0.3206
09/05 05:20:05 PM: Update 37763: task mnli, batch 763 (37709): accuracy: 0.8789, mnli_loss: 0.3223
09/05 05:20:15 PM: Update 37801: task mnli, batch 801 (37747): accuracy: 0.8792, mnli_loss: 0.3216
09/05 05:20:25 PM: Update 37833: task mnli, batch 833 (37779): accuracy: 0.8791, mnli_loss: 0.3217
09/05 05:20:35 PM: Update 37871: task mnli, batch 871 (37817): accuracy: 0.8789, mnli_loss: 0.3221
09/05 05:20:45 PM: Update 37909: task mnli, batch 909 (37855): accuracy: 0.8796, mnli_loss: 0.3211
09/05 05:20:55 PM: Update 37948: task mnli, batch 948 (37894): accuracy: 0.8798, mnli_loss: 0.3209
09/05 05:21:05 PM: Update 37987: task mnli, batch 987 (37933): accuracy: 0.8803, mnli_loss: 0.3192
09/05 05:21:09 PM: ***** Step 38000 / Validation 38 *****
09/05 05:21:09 PM: copa: trained on 0 batches, 0.000 epochs
09/05 05:21:09 PM: mnli: trained on 1000 batches, 0.061 epochs
09/05 05:21:09 PM: Validating...
09/05 05:21:09 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8333, copa_loss: 0.4092
09/05 05:21:16 PM: Evaluate: task mnli, batch 87 (209): accuracy: 0.8305, mnli_loss: 0.4845
09/05 05:21:24 PM: Updating LR scheduler:
09/05 05:21:24 PM: 	Best result seen so far for macro_avg: 0.801
09/05 05:21:24 PM: 	# validation passes without improvement: 3
09/05 05:21:24 PM: copa_loss: training: 0.000000 validation: 0.598616
09/05 05:21:24 PM: mnli_loss: training: 0.318462 validation: 0.512810
09/05 05:21:24 PM: macro_avg: validation: 0.800700
09/05 05:21:24 PM: micro_avg: validation: 0.820588
09/05 05:21:24 PM: copa_accuracy: validation: 0.780000
09/05 05:21:24 PM: mnli_accuracy: training: 0.880420 validation: 0.821400
09/05 05:21:24 PM: Global learning rate: 5e-06
09/05 05:21:24 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 05:21:26 PM: Update 38001: task mnli, batch 1 (37947): accuracy: 0.8750, mnli_loss: 0.2554
09/05 05:21:36 PM: Update 38038: task mnli, batch 38 (37984): accuracy: 0.8618, mnli_loss: 0.3622
09/05 05:21:46 PM: Update 38077: task mnli, batch 77 (38023): accuracy: 0.8766, mnli_loss: 0.3290
09/05 05:21:56 PM: Update 38115: task mnli, batch 115 (38061): accuracy: 0.8797, mnli_loss: 0.3151
09/05 05:22:06 PM: Update 38153: task mnli, batch 153 (38099): accuracy: 0.8826, mnli_loss: 0.3096
09/05 05:22:16 PM: Update 38192: task mnli, batch 192 (38138): accuracy: 0.8776, mnli_loss: 0.3201
09/05 05:22:28 PM: Update 38228: task mnli, batch 228 (38174): accuracy: 0.8809, mnli_loss: 0.3172
09/05 05:22:38 PM: Update 38266: task mnli, batch 266 (38212): accuracy: 0.8811, mnli_loss: 0.3194
09/05 05:22:48 PM: Update 38306: task mnli, batch 306 (38252): accuracy: 0.8836, mnli_loss: 0.3122
09/05 05:22:58 PM: Update 38343: task mnli, batch 343 (38289): accuracy: 0.8834, mnli_loss: 0.3121
09/05 05:23:08 PM: Update 38383: task mnli, batch 383 (38329): accuracy: 0.8848, mnli_loss: 0.3082
09/05 05:23:18 PM: Update 38420: task mnli, batch 420 (38366): accuracy: 0.8825, mnli_loss: 0.3109
09/05 05:23:29 PM: Update 38457: task mnli, batch 457 (38403): accuracy: 0.8829, mnli_loss: 0.3102
09/05 05:23:39 PM: Update 38496: task mnli, batch 496 (38442): accuracy: 0.8832, mnli_loss: 0.3099
09/05 05:23:49 PM: Update 38534: task mnli, batch 534 (38480): accuracy: 0.8837, mnli_loss: 0.3096
09/05 05:23:59 PM: Update 38573: task mnli, batch 573 (38519): accuracy: 0.8839, mnli_loss: 0.3099
09/05 05:24:09 PM: Update 38612: task mnli, batch 612 (38558): accuracy: 0.8846, mnli_loss: 0.3079
09/05 05:24:20 PM: Update 38645: task mnli, batch 645 (38591): accuracy: 0.8849, mnli_loss: 0.3087
09/05 05:24:30 PM: Update 38684: task mnli, batch 684 (38630): accuracy: 0.8846, mnli_loss: 0.3080
09/05 05:24:40 PM: Update 38722: task mnli, batch 722 (38668): accuracy: 0.8848, mnli_loss: 0.3083
09/05 05:24:50 PM: Update 38760: task mnli, batch 760 (38706): accuracy: 0.8850, mnli_loss: 0.3085
09/05 05:25:00 PM: Update 38797: task mnli, batch 797 (38743): accuracy: 0.8853, mnli_loss: 0.3091
09/05 05:25:10 PM: Update 38835: task mnli, batch 835 (38781): accuracy: 0.8863, mnli_loss: 0.3078
09/05 05:25:21 PM: Update 38875: task mnli, batch 875 (38821): accuracy: 0.8862, mnli_loss: 0.3084
09/05 05:25:31 PM: Update 38913: task mnli, batch 913 (38859): accuracy: 0.8863, mnli_loss: 0.3078
09/05 05:25:41 PM: Update 38952: task mnli, batch 952 (38898): accuracy: 0.8866, mnli_loss: 0.3069
09/05 05:25:51 PM: Update 38990: task mnli, batch 990 (38936): accuracy: 0.8865, mnli_loss: 0.3067
09/05 05:25:54 PM: ***** Step 39000 / Validation 39 *****
09/05 05:25:54 PM: copa: trained on 0 batches, 0.000 epochs
09/05 05:25:54 PM: mnli: trained on 1000 batches, 0.061 epochs
09/05 05:25:54 PM: Validating...
09/05 05:25:54 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8333, copa_loss: 0.4009
09/05 05:26:01 PM: Evaluate: task mnli, batch 98 (209): accuracy: 0.8257, mnli_loss: 0.5023
09/05 05:26:09 PM: Updating LR scheduler:
09/05 05:26:09 PM: 	Best result seen so far for macro_avg: 0.801
09/05 05:26:09 PM: 	# validation passes without improvement: 4
09/05 05:26:09 PM: copa_loss: training: 0.000000 validation: 0.593798
09/05 05:26:09 PM: mnli_loss: training: 0.305823 validation: 0.517436
09/05 05:26:09 PM: macro_avg: validation: 0.800900
09/05 05:26:09 PM: micro_avg: validation: 0.820980
09/05 05:26:09 PM: copa_accuracy: validation: 0.780000
09/05 05:26:09 PM: mnli_accuracy: training: 0.886800 validation: 0.821800
09/05 05:26:09 PM: Global learning rate: 5e-06
09/05 05:26:09 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 05:26:11 PM: Update 39004: task mnli, batch 4 (38950): accuracy: 0.8750, mnli_loss: 0.3967
09/05 05:26:21 PM: Update 39042: task mnli, batch 42 (38988): accuracy: 0.8929, mnli_loss: 0.2896
09/05 05:26:32 PM: Update 39072: task mnli, batch 72 (39018): accuracy: 0.8808, mnli_loss: 0.3204
09/05 05:26:42 PM: Update 39110: task mnli, batch 110 (39056): accuracy: 0.8841, mnli_loss: 0.3115
09/05 05:26:52 PM: Update 39149: task mnli, batch 149 (39095): accuracy: 0.8848, mnli_loss: 0.3102
09/05 05:27:02 PM: Update 39185: task mnli, batch 185 (39131): accuracy: 0.8856, mnli_loss: 0.3092
09/05 05:27:12 PM: Update 39223: task mnli, batch 223 (39169): accuracy: 0.8875, mnli_loss: 0.3059
09/05 05:27:22 PM: Update 39260: task mnli, batch 260 (39206): accuracy: 0.8882, mnli_loss: 0.3062
09/05 05:27:32 PM: Update 39298: task mnli, batch 298 (39244): accuracy: 0.8872, mnli_loss: 0.3089
09/05 05:27:42 PM: Update 39337: task mnli, batch 337 (39283): accuracy: 0.8891, mnli_loss: 0.3072
09/05 05:27:52 PM: Update 39375: task mnli, batch 375 (39321): accuracy: 0.8892, mnli_loss: 0.3075
09/05 05:28:02 PM: Update 39414: task mnli, batch 414 (39360): accuracy: 0.8886, mnli_loss: 0.3103
09/05 05:28:13 PM: Update 39453: task mnli, batch 453 (39399): accuracy: 0.8869, mnli_loss: 0.3130
09/05 05:28:23 PM: Update 39485: task mnli, batch 485 (39431): accuracy: 0.8863, mnli_loss: 0.3171
09/05 05:28:33 PM: Update 39525: task mnli, batch 525 (39471): accuracy: 0.8873, mnli_loss: 0.3149
09/05 05:28:43 PM: Update 39565: task mnli, batch 565 (39511): accuracy: 0.8892, mnli_loss: 0.3115
09/05 05:28:53 PM: Update 39603: task mnli, batch 603 (39549): accuracy: 0.8908, mnli_loss: 0.3070
09/05 05:29:03 PM: Update 39642: task mnli, batch 642 (39588): accuracy: 0.8905, mnli_loss: 0.3068
09/05 05:29:13 PM: Update 39680: task mnli, batch 680 (39626): accuracy: 0.8898, mnli_loss: 0.3078
09/05 05:29:24 PM: Update 39720: task mnli, batch 720 (39666): accuracy: 0.8899, mnli_loss: 0.3083
09/05 05:29:34 PM: Update 39757: task mnli, batch 757 (39703): accuracy: 0.8901, mnli_loss: 0.3081
09/05 05:29:44 PM: Update 39795: task mnli, batch 795 (39741): accuracy: 0.8901, mnli_loss: 0.3068
09/05 05:29:54 PM: Update 39835: task mnli, batch 835 (39781): accuracy: 0.8902, mnli_loss: 0.3068
09/05 05:30:04 PM: Update 39870: task mnli, batch 870 (39816): accuracy: 0.8893, mnli_loss: 0.3080
09/05 05:30:14 PM: Update 39901: task mnli, batch 901 (39847): accuracy: 0.8896, mnli_loss: 0.3070
09/05 05:30:25 PM: Update 39940: task mnli, batch 940 (39886): accuracy: 0.8892, mnli_loss: 0.3079
09/05 05:30:35 PM: Update 39980: task mnli, batch 980 (39926): accuracy: 0.8890, mnli_loss: 0.3081
09/05 05:30:40 PM: ***** Step 40000 / Validation 40 *****
09/05 05:30:40 PM: copa: trained on 0 batches, 0.000 epochs
09/05 05:30:40 PM: mnli: trained on 1000 batches, 0.061 epochs
09/05 05:30:40 PM: Validating...
09/05 05:30:40 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8333, copa_loss: 0.4022
09/05 05:30:45 PM: Evaluate: task mnli, batch 55 (209): accuracy: 0.8220, mnli_loss: 0.4980
09/05 05:30:55 PM: Evaluate: task mnli, batch 180 (209): accuracy: 0.8234, mnli_loss: 0.5129
09/05 05:30:57 PM: Updating LR scheduler:
09/05 05:30:57 PM: 	Best result seen so far for macro_avg: 0.801
09/05 05:30:57 PM: 	# validation passes without improvement: 0
09/05 05:30:57 PM: copa_loss: training: 0.000000 validation: 0.597725
09/05 05:30:57 PM: mnli_loss: training: 0.307428 validation: 0.521027
09/05 05:30:57 PM: macro_avg: validation: 0.794500
09/05 05:30:57 PM: micro_avg: validation: 0.818039
09/05 05:30:57 PM: copa_accuracy: validation: 0.770000
09/05 05:30:57 PM: mnli_accuracy: training: 0.889348 validation: 0.819000
09/05 05:30:57 PM: Global learning rate: 2.5e-06
09/05 05:30:57 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 05:31:05 PM: Update 40027: task mnli, batch 27 (39973): accuracy: 0.9105, mnli_loss: 0.2377
09/05 05:31:15 PM: Update 40065: task mnli, batch 65 (40011): accuracy: 0.8994, mnli_loss: 0.2703
09/05 05:31:25 PM: Update 40103: task mnli, batch 103 (40049): accuracy: 0.8928, mnli_loss: 0.2919
09/05 05:31:35 PM: Update 40142: task mnli, batch 142 (40088): accuracy: 0.8955, mnli_loss: 0.2859
09/05 05:31:46 PM: Update 40180: task mnli, batch 180 (40126): accuracy: 0.8935, mnli_loss: 0.2904
09/05 05:31:56 PM: Update 40218: task mnli, batch 218 (40164): accuracy: 0.8914, mnli_loss: 0.2926
09/05 05:32:06 PM: Update 40258: task mnli, batch 258 (40204): accuracy: 0.8931, mnli_loss: 0.2942
09/05 05:32:16 PM: Update 40296: task mnli, batch 296 (40242): accuracy: 0.8932, mnli_loss: 0.2944
09/05 05:32:26 PM: Update 40327: task mnli, batch 327 (40273): accuracy: 0.8950, mnli_loss: 0.2916
09/05 05:32:36 PM: Update 40365: task mnli, batch 365 (40311): accuracy: 0.8939, mnli_loss: 0.2979
09/05 05:32:46 PM: Update 40405: task mnli, batch 405 (40351): accuracy: 0.8940, mnli_loss: 0.2980
09/05 05:32:57 PM: Update 40444: task mnli, batch 444 (40390): accuracy: 0.8941, mnli_loss: 0.2981
09/05 05:33:07 PM: Update 40482: task mnli, batch 482 (40428): accuracy: 0.8933, mnli_loss: 0.2985
09/05 05:33:17 PM: Update 40521: task mnli, batch 521 (40467): accuracy: 0.8932, mnli_loss: 0.2977
09/05 05:33:27 PM: Update 40559: task mnli, batch 559 (40505): accuracy: 0.8939, mnli_loss: 0.2962
09/05 05:33:37 PM: Update 40596: task mnli, batch 596 (40542): accuracy: 0.8940, mnli_loss: 0.2969
09/05 05:33:47 PM: Update 40633: task mnli, batch 633 (40579): accuracy: 0.8930, mnli_loss: 0.2992
09/05 05:33:57 PM: Update 40672: task mnli, batch 672 (40618): accuracy: 0.8934, mnli_loss: 0.2983
09/05 05:34:07 PM: Update 40711: task mnli, batch 711 (40657): accuracy: 0.8938, mnli_loss: 0.2959
09/05 05:34:18 PM: Update 40742: task mnli, batch 742 (40688): accuracy: 0.8933, mnli_loss: 0.2967
09/05 05:34:28 PM: Update 40779: task mnli, batch 779 (40725): accuracy: 0.8920, mnli_loss: 0.2995
09/05 05:34:38 PM: Update 40818: task mnli, batch 818 (40764): accuracy: 0.8924, mnli_loss: 0.2996
09/05 05:34:48 PM: Update 40857: task mnli, batch 857 (40803): accuracy: 0.8919, mnli_loss: 0.2996
09/05 05:34:58 PM: Update 40896: task mnli, batch 896 (40842): accuracy: 0.8910, mnli_loss: 0.3011
09/05 05:35:08 PM: Update 40934: task mnli, batch 934 (40880): accuracy: 0.8904, mnli_loss: 0.3023
09/05 05:35:18 PM: Update 40971: task mnli, batch 971 (40917): accuracy: 0.8906, mnli_loss: 0.3029
09/05 05:35:26 PM: ***** Step 41000 / Validation 41 *****
09/05 05:35:26 PM: copa: trained on 0 batches, 0.000 epochs
09/05 05:35:26 PM: mnli: trained on 1000 batches, 0.061 epochs
09/05 05:35:26 PM: Validating...
09/05 05:35:26 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8333, copa_loss: 0.4035
09/05 05:35:28 PM: Evaluate: task mnli, batch 29 (209): accuracy: 0.8190, mnli_loss: 0.5193
09/05 05:35:38 PM: Evaluate: task mnli, batch 165 (209): accuracy: 0.8263, mnli_loss: 0.5110
09/05 05:35:41 PM: Best result seen so far for mnli.
09/05 05:35:41 PM: Best result seen so far for micro.
09/05 05:35:41 PM: Best result seen so far for macro.
09/05 05:35:41 PM: Updating LR scheduler:
09/05 05:35:41 PM: 	Best result seen so far for macro_avg: 0.801
09/05 05:35:41 PM: 	# validation passes without improvement: 0
09/05 05:35:41 PM: copa_loss: training: 0.000000 validation: 0.598501
09/05 05:35:41 PM: mnli_loss: training: 0.302533 validation: 0.518594
09/05 05:35:41 PM: macro_avg: validation: 0.801500
09/05 05:35:41 PM: micro_avg: validation: 0.822157
09/05 05:35:41 PM: copa_accuracy: validation: 0.780000
09/05 05:35:41 PM: mnli_accuracy: training: 0.890635 validation: 0.823000
09/05 05:35:41 PM: Global learning rate: 2.5e-06
09/05 05:35:41 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 05:35:48 PM: Update 41023: task mnli, batch 23 (40969): accuracy: 0.8822, mnli_loss: 0.2944
09/05 05:35:58 PM: Update 41061: task mnli, batch 61 (41007): accuracy: 0.8866, mnli_loss: 0.2951
09/05 05:36:08 PM: Update 41099: task mnli, batch 99 (41045): accuracy: 0.8876, mnli_loss: 0.2932
09/05 05:36:19 PM: Update 41137: task mnli, batch 137 (41083): accuracy: 0.8875, mnli_loss: 0.3052
09/05 05:36:29 PM: Update 41169: task mnli, batch 169 (41115): accuracy: 0.8888, mnli_loss: 0.3025
09/05 05:36:39 PM: Update 41208: task mnli, batch 208 (41154): accuracy: 0.8888, mnli_loss: 0.3033
09/05 05:36:49 PM: Update 41246: task mnli, batch 246 (41192): accuracy: 0.8899, mnli_loss: 0.3016
09/05 05:36:59 PM: Update 41284: task mnli, batch 284 (41230): accuracy: 0.8890, mnli_loss: 0.3036
09/05 05:37:10 PM: Update 41323: task mnli, batch 323 (41269): accuracy: 0.8891, mnli_loss: 0.3035
09/05 05:37:20 PM: Update 41360: task mnli, batch 360 (41306): accuracy: 0.8875, mnli_loss: 0.3040
09/05 05:37:30 PM: Update 41398: task mnli, batch 398 (41344): accuracy: 0.8866, mnli_loss: 0.3041
09/05 05:37:40 PM: Update 41437: task mnli, batch 437 (41383): accuracy: 0.8864, mnli_loss: 0.3044
09/05 05:37:50 PM: Update 41475: task mnli, batch 475 (41421): accuracy: 0.8853, mnli_loss: 0.3067
09/05 05:38:00 PM: Update 41516: task mnli, batch 516 (41462): accuracy: 0.8879, mnli_loss: 0.3003
09/05 05:38:10 PM: Update 41553: task mnli, batch 553 (41499): accuracy: 0.8877, mnli_loss: 0.3010
09/05 05:38:20 PM: Update 41581: task mnli, batch 581 (41527): accuracy: 0.8863, mnli_loss: 0.3033
09/05 05:38:30 PM: Update 41619: task mnli, batch 619 (41565): accuracy: 0.8861, mnli_loss: 0.3040
09/05 05:38:41 PM: Update 41658: task mnli, batch 658 (41604): accuracy: 0.8867, mnli_loss: 0.3033
09/05 05:38:51 PM: Update 41696: task mnli, batch 696 (41642): accuracy: 0.8859, mnli_loss: 0.3050
09/05 05:39:01 PM: Update 41735: task mnli, batch 735 (41681): accuracy: 0.8858, mnli_loss: 0.3038
09/05 05:39:11 PM: Update 41773: task mnli, batch 773 (41719): accuracy: 0.8845, mnli_loss: 0.3057
09/05 05:39:21 PM: Update 41811: task mnli, batch 811 (41757): accuracy: 0.8841, mnli_loss: 0.3073
09/05 05:39:31 PM: Update 41850: task mnli, batch 850 (41796): accuracy: 0.8841, mnli_loss: 0.3078
09/05 05:39:41 PM: Update 41888: task mnli, batch 888 (41834): accuracy: 0.8853, mnli_loss: 0.3054
09/05 05:39:51 PM: Update 41926: task mnli, batch 926 (41872): accuracy: 0.8851, mnli_loss: 0.3058
09/05 05:40:02 PM: Update 41965: task mnli, batch 965 (41911): accuracy: 0.8849, mnli_loss: 0.3068
09/05 05:40:12 PM: Update 41996: task mnli, batch 996 (41942): accuracy: 0.8844, mnli_loss: 0.3079
09/05 05:40:13 PM: ***** Step 42000 / Validation 42 *****
09/05 05:40:13 PM: copa: trained on 0 batches, 0.000 epochs
09/05 05:40:13 PM: mnli: trained on 1000 batches, 0.061 epochs
09/05 05:40:13 PM: Validating...
09/05 05:40:13 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8333, copa_loss: 0.3950
09/05 05:40:22 PM: Evaluate: task mnli, batch 117 (209): accuracy: 0.8273, mnli_loss: 0.5078
09/05 05:40:28 PM: Updating LR scheduler:
09/05 05:40:28 PM: 	Best result seen so far for macro_avg: 0.801
09/05 05:40:28 PM: 	# validation passes without improvement: 1
09/05 05:40:28 PM: copa_loss: training: 0.000000 validation: 0.595964
09/05 05:40:28 PM: mnli_loss: training: 0.307820 validation: 0.520204
09/05 05:40:28 PM: macro_avg: validation: 0.801100
09/05 05:40:28 PM: micro_avg: validation: 0.821373
09/05 05:40:28 PM: copa_accuracy: validation: 0.780000
09/05 05:40:28 PM: mnli_accuracy: training: 0.884426 validation: 0.822200
09/05 05:40:28 PM: Global learning rate: 2.5e-06
09/05 05:40:28 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 05:40:32 PM: Update 42010: task mnli, batch 10 (41956): accuracy: 0.9000, mnli_loss: 0.3281
09/05 05:40:42 PM: Update 42047: task mnli, batch 47 (41993): accuracy: 0.8918, mnli_loss: 0.2937
09/05 05:40:52 PM: Update 42086: task mnli, batch 86 (42032): accuracy: 0.8939, mnli_loss: 0.2879
09/05 05:41:02 PM: Update 42122: task mnli, batch 122 (42068): accuracy: 0.8900, mnli_loss: 0.2992
09/05 05:41:12 PM: Update 42161: task mnli, batch 161 (42107): accuracy: 0.8903, mnli_loss: 0.2990
09/05 05:41:22 PM: Update 42200: task mnli, batch 200 (42146): accuracy: 0.8890, mnli_loss: 0.3028
09/05 05:41:32 PM: Update 42238: task mnli, batch 238 (42184): accuracy: 0.8904, mnli_loss: 0.3014
09/05 05:41:43 PM: Update 42275: task mnli, batch 275 (42221): accuracy: 0.8864, mnli_loss: 0.3085
09/05 05:41:53 PM: Update 42313: task mnli, batch 313 (42259): accuracy: 0.8847, mnli_loss: 0.3107
09/05 05:42:03 PM: Update 42353: task mnli, batch 353 (42299): accuracy: 0.8870, mnli_loss: 0.3084
09/05 05:42:13 PM: Update 42392: task mnli, batch 392 (42338): accuracy: 0.8878, mnli_loss: 0.3074
09/05 05:42:23 PM: Update 42423: task mnli, batch 423 (42369): accuracy: 0.8875, mnli_loss: 0.3076
09/05 05:42:33 PM: Update 42462: task mnli, batch 462 (42408): accuracy: 0.8858, mnli_loss: 0.3109
09/05 05:42:43 PM: Update 42499: task mnli, batch 499 (42445): accuracy: 0.8871, mnli_loss: 0.3095
09/05 05:42:53 PM: Update 42537: task mnli, batch 537 (42483): accuracy: 0.8878, mnli_loss: 0.3098
09/05 05:43:03 PM: Update 42575: task mnli, batch 575 (42521): accuracy: 0.8870, mnli_loss: 0.3108
09/05 05:43:14 PM: Update 42614: task mnli, batch 614 (42560): accuracy: 0.8870, mnli_loss: 0.3110
09/05 05:43:24 PM: Update 42654: task mnli, batch 654 (42600): accuracy: 0.8875, mnli_loss: 0.3099
09/05 05:43:34 PM: Update 42692: task mnli, batch 692 (42638): accuracy: 0.8867, mnli_loss: 0.3100
09/05 05:43:44 PM: Update 42729: task mnli, batch 729 (42675): accuracy: 0.8870, mnli_loss: 0.3100
09/05 05:43:54 PM: Update 42768: task mnli, batch 768 (42714): accuracy: 0.8877, mnli_loss: 0.3073
09/05 05:44:04 PM: Update 42808: task mnli, batch 808 (42754): accuracy: 0.8879, mnli_loss: 0.3057
09/05 05:44:14 PM: Update 42839: task mnli, batch 839 (42785): accuracy: 0.8877, mnli_loss: 0.3057
09/05 05:44:25 PM: Update 42878: task mnli, batch 878 (42824): accuracy: 0.8874, mnli_loss: 0.3046
09/05 05:44:35 PM: Update 42916: task mnli, batch 916 (42862): accuracy: 0.8880, mnli_loss: 0.3034
09/05 05:44:45 PM: Update 42954: task mnli, batch 954 (42900): accuracy: 0.8878, mnli_loss: 0.3049
09/05 05:44:55 PM: Update 42994: task mnli, batch 994 (42940): accuracy: 0.8887, mnli_loss: 0.3030
09/05 05:44:57 PM: ***** Step 43000 / Validation 43 *****
09/05 05:44:57 PM: copa: trained on 0 batches, 0.000 epochs
09/05 05:44:57 PM: mnli: trained on 1000 batches, 0.061 epochs
09/05 05:44:57 PM: Validating...
09/05 05:44:57 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8333, copa_loss: 0.3913
09/05 05:45:05 PM: Evaluate: task mnli, batch 110 (209): accuracy: 0.8277, mnli_loss: 0.5135
09/05 05:45:12 PM: Updating LR scheduler:
09/05 05:45:12 PM: 	Best result seen so far for macro_avg: 0.801
09/05 05:45:12 PM: 	# validation passes without improvement: 2
09/05 05:45:12 PM: copa_loss: training: 0.000000 validation: 0.597783
09/05 05:45:12 PM: mnli_loss: training: 0.302338 validation: 0.523130
09/05 05:45:12 PM: macro_avg: validation: 0.801300
09/05 05:45:12 PM: micro_avg: validation: 0.821765
09/05 05:45:12 PM: copa_accuracy: validation: 0.780000
09/05 05:45:12 PM: mnli_accuracy: training: 0.889009 validation: 0.822600
09/05 05:45:12 PM: Global learning rate: 2.5e-06
09/05 05:45:12 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 05:45:15 PM: Update 43008: task mnli, batch 8 (42954): accuracy: 0.9375, mnli_loss: 0.2410
09/05 05:45:26 PM: Update 43046: task mnli, batch 46 (42992): accuracy: 0.9004, mnli_loss: 0.2932
09/05 05:45:36 PM: Update 43085: task mnli, batch 85 (43031): accuracy: 0.9034, mnli_loss: 0.2763
09/05 05:45:46 PM: Update 43123: task mnli, batch 123 (43069): accuracy: 0.9051, mnli_loss: 0.2761
09/05 05:45:56 PM: Update 43160: task mnli, batch 160 (43106): accuracy: 0.9008, mnli_loss: 0.2800
09/05 05:46:06 PM: Update 43200: task mnli, batch 200 (43146): accuracy: 0.9006, mnli_loss: 0.2772
09/05 05:46:17 PM: Update 43232: task mnli, batch 232 (43178): accuracy: 0.8950, mnli_loss: 0.2874
09/05 05:46:27 PM: Update 43272: task mnli, batch 272 (43218): accuracy: 0.8954, mnli_loss: 0.2855
09/05 05:46:37 PM: Update 43310: task mnli, batch 310 (43256): accuracy: 0.8949, mnli_loss: 0.2850
09/05 05:46:47 PM: Update 43348: task mnli, batch 348 (43294): accuracy: 0.8951, mnli_loss: 0.2848
09/05 05:46:57 PM: Update 43386: task mnli, batch 386 (43332): accuracy: 0.8942, mnli_loss: 0.2865
09/05 05:47:07 PM: Update 43423: task mnli, batch 423 (43369): accuracy: 0.8937, mnli_loss: 0.2888
09/05 05:47:17 PM: Update 43460: task mnli, batch 460 (43406): accuracy: 0.8930, mnli_loss: 0.2917
09/05 05:47:27 PM: Update 43498: task mnli, batch 498 (43444): accuracy: 0.8920, mnli_loss: 0.2918
09/05 05:47:38 PM: Update 43537: task mnli, batch 537 (43483): accuracy: 0.8929, mnli_loss: 0.2894
09/05 05:47:48 PM: Update 43577: task mnli, batch 577 (43523): accuracy: 0.8944, mnli_loss: 0.2865
09/05 05:47:58 PM: Update 43616: task mnli, batch 616 (43562): accuracy: 0.8942, mnli_loss: 0.2865
09/05 05:48:09 PM: Update 43649: task mnli, batch 649 (43595): accuracy: 0.8949, mnli_loss: 0.2845
09/05 05:48:19 PM: Update 43686: task mnli, batch 686 (43632): accuracy: 0.8935, mnli_loss: 0.2858
09/05 05:48:29 PM: Update 43725: task mnli, batch 725 (43671): accuracy: 0.8937, mnli_loss: 0.2860
09/05 05:48:39 PM: Update 43763: task mnli, batch 763 (43709): accuracy: 0.8935, mnli_loss: 0.2873
09/05 05:48:50 PM: Update 43801: task mnli, batch 801 (43747): accuracy: 0.8936, mnli_loss: 0.2874
09/05 05:49:00 PM: Update 43839: task mnli, batch 839 (43785): accuracy: 0.8919, mnli_loss: 0.2899
09/05 05:49:10 PM: Update 43877: task mnli, batch 877 (43823): accuracy: 0.8920, mnli_loss: 0.2900
09/05 05:49:20 PM: Update 43916: task mnli, batch 916 (43862): accuracy: 0.8923, mnli_loss: 0.2896
09/05 05:49:30 PM: Update 43955: task mnli, batch 955 (43901): accuracy: 0.8924, mnli_loss: 0.2893
09/05 05:49:40 PM: Update 43992: task mnli, batch 992 (43938): accuracy: 0.8916, mnli_loss: 0.2902
09/05 05:49:42 PM: ***** Step 44000 / Validation 44 *****
09/05 05:49:42 PM: copa: trained on 0 batches, 0.000 epochs
09/05 05:49:42 PM: mnli: trained on 1000 batches, 0.061 epochs
09/05 05:49:42 PM: Validating...
09/05 05:49:42 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8333, copa_loss: 0.3926
09/05 05:49:50 PM: Evaluate: task mnli, batch 103 (209): accuracy: 0.8273, mnli_loss: 0.5072
09/05 05:49:58 PM: Updating LR scheduler:
09/05 05:49:58 PM: 	Best result seen so far for macro_avg: 0.801
09/05 05:49:58 PM: 	# validation passes without improvement: 3
09/05 05:49:58 PM: copa_loss: training: 0.000000 validation: 0.599432
09/05 05:49:58 PM: mnli_loss: training: 0.289910 validation: 0.525391
09/05 05:49:58 PM: macro_avg: validation: 0.796100
09/05 05:49:58 PM: micro_avg: validation: 0.821176
09/05 05:49:58 PM: copa_accuracy: validation: 0.770000
09/05 05:49:58 PM: mnli_accuracy: training: 0.891719 validation: 0.822200
09/05 05:49:58 PM: Global learning rate: 2.5e-06
09/05 05:49:58 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 05:50:00 PM: Update 44006: task mnli, batch 6 (43952): accuracy: 0.8681, mnli_loss: 0.2772
09/05 05:50:10 PM: Update 44045: task mnli, batch 45 (43991): accuracy: 0.8806, mnli_loss: 0.2951
09/05 05:50:21 PM: Update 44076: task mnli, batch 76 (44022): accuracy: 0.8871, mnli_loss: 0.2921
09/05 05:50:31 PM: Update 44116: task mnli, batch 116 (44062): accuracy: 0.8923, mnli_loss: 0.2763
09/05 05:50:41 PM: Update 44154: task mnli, batch 154 (44100): accuracy: 0.8899, mnli_loss: 0.2859
09/05 05:50:51 PM: Update 44194: task mnli, batch 194 (44140): accuracy: 0.8901, mnli_loss: 0.2859
09/05 05:51:01 PM: Update 44233: task mnli, batch 233 (44179): accuracy: 0.8902, mnli_loss: 0.2862
09/05 05:51:12 PM: Update 44270: task mnli, batch 270 (44216): accuracy: 0.8860, mnli_loss: 0.2967
09/05 05:51:22 PM: Update 44307: task mnli, batch 307 (44253): accuracy: 0.8879, mnli_loss: 0.2927
09/05 05:51:32 PM: Update 44347: task mnli, batch 347 (44293): accuracy: 0.8875, mnli_loss: 0.2941
09/05 05:51:42 PM: Update 44386: task mnli, batch 386 (44332): accuracy: 0.8875, mnli_loss: 0.2959
09/05 05:51:52 PM: Update 44424: task mnli, batch 424 (44370): accuracy: 0.8868, mnli_loss: 0.2992
09/05 05:52:02 PM: Update 44463: task mnli, batch 463 (44409): accuracy: 0.8890, mnli_loss: 0.2967
09/05 05:52:12 PM: Update 44492: task mnli, batch 492 (44438): accuracy: 0.8886, mnli_loss: 0.2974
09/05 05:52:22 PM: Update 44530: task mnli, batch 530 (44476): accuracy: 0.8895, mnli_loss: 0.2974
09/05 05:52:33 PM: Update 44568: task mnli, batch 568 (44514): accuracy: 0.8892, mnli_loss: 0.2980
09/05 05:52:43 PM: Update 44606: task mnli, batch 606 (44552): accuracy: 0.8891, mnli_loss: 0.2983
09/05 05:52:53 PM: Update 44644: task mnli, batch 644 (44590): accuracy: 0.8894, mnli_loss: 0.2973
09/05 05:53:03 PM: Update 44682: task mnli, batch 682 (44628): accuracy: 0.8899, mnli_loss: 0.2957
09/05 05:53:13 PM: Update 44722: task mnli, batch 722 (44668): accuracy: 0.8898, mnli_loss: 0.2952
09/05 05:53:24 PM: Update 44763: task mnli, batch 763 (44709): accuracy: 0.8911, mnli_loss: 0.2924
09/05 05:53:34 PM: Update 44801: task mnli, batch 801 (44747): accuracy: 0.8911, mnli_loss: 0.2921
09/05 05:53:44 PM: Update 44839: task mnli, batch 839 (44785): accuracy: 0.8911, mnli_loss: 0.2925
09/05 05:53:54 PM: Update 44878: task mnli, batch 878 (44824): accuracy: 0.8907, mnli_loss: 0.2932
09/05 05:54:04 PM: Update 44908: task mnli, batch 908 (44854): accuracy: 0.8901, mnli_loss: 0.2957
09/05 05:54:14 PM: Update 44946: task mnli, batch 946 (44892): accuracy: 0.8899, mnli_loss: 0.2966
09/05 05:54:24 PM: Update 44985: task mnli, batch 985 (44931): accuracy: 0.8901, mnli_loss: 0.2960
09/05 05:54:28 PM: ***** Step 45000 / Validation 45 *****
09/05 05:54:28 PM: copa: trained on 0 batches, 0.000 epochs
09/05 05:54:28 PM: mnli: trained on 1000 batches, 0.061 epochs
09/05 05:54:28 PM: Validating...
09/05 05:54:28 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8333, copa_loss: 0.3887
09/05 05:54:34 PM: Evaluate: task mnli, batch 78 (209): accuracy: 0.8301, mnli_loss: 0.5007
09/05 05:54:44 PM: Updating LR scheduler:
09/05 05:54:44 PM: 	Best result seen so far for macro_avg: 0.801
09/05 05:54:44 PM: 	# validation passes without improvement: 4
09/05 05:54:44 PM: copa_loss: training: 0.000000 validation: 0.597743
09/05 05:54:44 PM: mnli_loss: training: 0.294923 validation: 0.529978
09/05 05:54:44 PM: macro_avg: validation: 0.795900
09/05 05:54:44 PM: micro_avg: validation: 0.820784
09/05 05:54:44 PM: copa_accuracy: validation: 0.770000
09/05 05:54:44 PM: mnli_accuracy: training: 0.890557 validation: 0.821800
09/05 05:54:44 PM: Global learning rate: 2.5e-06
09/05 05:54:44 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 05:54:45 PM: Update 45001: task mnli, batch 1 (44947): accuracy: 0.8750, mnli_loss: 0.3090
09/05 05:54:55 PM: Update 45041: task mnli, batch 41 (44987): accuracy: 0.8984, mnli_loss: 0.2714
09/05 05:55:05 PM: Update 45080: task mnli, batch 80 (45026): accuracy: 0.8984, mnli_loss: 0.2729
09/05 05:55:15 PM: Update 45117: task mnli, batch 117 (45063): accuracy: 0.8992, mnli_loss: 0.2690
09/05 05:55:25 PM: Update 45155: task mnli, batch 155 (45101): accuracy: 0.8944, mnli_loss: 0.2797
09/05 05:55:35 PM: Update 45193: task mnli, batch 193 (45139): accuracy: 0.8942, mnli_loss: 0.2812
09/05 05:55:46 PM: Update 45232: task mnli, batch 232 (45178): accuracy: 0.8931, mnli_loss: 0.2840
09/05 05:55:56 PM: Update 45269: task mnli, batch 269 (45215): accuracy: 0.8900, mnli_loss: 0.2890
09/05 05:56:06 PM: Update 45307: task mnli, batch 307 (45253): accuracy: 0.8905, mnli_loss: 0.2910
09/05 05:56:16 PM: Update 45335: task mnli, batch 335 (45281): accuracy: 0.8908, mnli_loss: 0.2896
09/05 05:56:26 PM: Update 45373: task mnli, batch 373 (45319): accuracy: 0.8936, mnli_loss: 0.2874
09/05 05:56:36 PM: Update 45412: task mnli, batch 412 (45358): accuracy: 0.8937, mnli_loss: 0.2865
09/05 05:56:46 PM: Update 45451: task mnli, batch 451 (45397): accuracy: 0.8923, mnli_loss: 0.2879
09/05 05:56:56 PM: Update 45490: task mnli, batch 490 (45436): accuracy: 0.8925, mnli_loss: 0.2896
09/05 05:57:07 PM: Update 45528: task mnli, batch 528 (45474): accuracy: 0.8931, mnli_loss: 0.2891
09/05 05:57:17 PM: Update 45567: task mnli, batch 567 (45513): accuracy: 0.8929, mnli_loss: 0.2905
09/05 05:57:27 PM: Update 45606: task mnli, batch 606 (45552): accuracy: 0.8930, mnli_loss: 0.2902
09/05 05:57:37 PM: Update 45642: task mnli, batch 642 (45588): accuracy: 0.8934, mnli_loss: 0.2888
09/05 05:57:47 PM: Update 45680: task mnli, batch 680 (45626): accuracy: 0.8931, mnli_loss: 0.2885
09/05 05:57:57 PM: Update 45718: task mnli, batch 718 (45664): accuracy: 0.8931, mnli_loss: 0.2876
09/05 05:58:07 PM: Update 45749: task mnli, batch 749 (45695): accuracy: 0.8924, mnli_loss: 0.2884
09/05 05:58:17 PM: Update 45788: task mnli, batch 788 (45734): accuracy: 0.8927, mnli_loss: 0.2884
09/05 05:58:28 PM: Update 45827: task mnli, batch 827 (45773): accuracy: 0.8924, mnli_loss: 0.2900
09/05 05:58:38 PM: Update 45865: task mnli, batch 865 (45811): accuracy: 0.8915, mnli_loss: 0.2910
09/05 05:58:48 PM: Update 45903: task mnli, batch 903 (45849): accuracy: 0.8911, mnli_loss: 0.2918
09/05 05:58:58 PM: Update 45941: task mnli, batch 941 (45887): accuracy: 0.8906, mnli_loss: 0.2933
09/05 05:59:08 PM: Update 45981: task mnli, batch 981 (45927): accuracy: 0.8906, mnli_loss: 0.2939
09/05 05:59:13 PM: ***** Step 46000 / Validation 46 *****
09/05 05:59:13 PM: copa: trained on 0 batches, 0.000 epochs
09/05 05:59:13 PM: mnli: trained on 1000 batches, 0.061 epochs
09/05 05:59:13 PM: Validating...
09/05 05:59:13 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8333, copa_loss: 0.3905
09/05 05:59:18 PM: Evaluate: task mnli, batch 58 (209): accuracy: 0.8218, mnli_loss: 0.5185
09/05 05:59:28 PM: Evaluate: task mnli, batch 194 (209): accuracy: 0.8245, mnli_loss: 0.5237
09/05 05:59:29 PM: Updating LR scheduler:
09/05 05:59:29 PM: 	Best result seen so far for macro_avg: 0.801
09/05 05:59:29 PM: 	# validation passes without improvement: 0
09/05 05:59:29 PM: copa_loss: training: 0.000000 validation: 0.600408
09/05 05:59:29 PM: mnli_loss: training: 0.294726 validation: 0.534666
09/05 05:59:29 PM: macro_avg: validation: 0.795100
09/05 05:59:29 PM: micro_avg: validation: 0.819216
09/05 05:59:29 PM: copa_accuracy: validation: 0.770000
09/05 05:59:29 PM: mnli_accuracy: training: 0.890344 validation: 0.820200
09/05 05:59:29 PM: Global learning rate: 1.25e-06
09/05 05:59:29 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 05:59:38 PM: Update 46032: task mnli, batch 32 (45978): accuracy: 0.8893, mnli_loss: 0.2870
09/05 05:59:48 PM: Update 46070: task mnli, batch 70 (46016): accuracy: 0.8845, mnli_loss: 0.3108
09/05 05:59:59 PM: Update 46107: task mnli, batch 107 (46053): accuracy: 0.8914, mnli_loss: 0.3085
09/05 06:00:09 PM: Update 46145: task mnli, batch 145 (46091): accuracy: 0.8894, mnli_loss: 0.3109
09/05 06:00:19 PM: Update 46176: task mnli, batch 176 (46122): accuracy: 0.8861, mnli_loss: 0.3137
09/05 06:00:29 PM: Update 46216: task mnli, batch 216 (46162): accuracy: 0.8883, mnli_loss: 0.3095
09/05 06:00:39 PM: Update 46254: task mnli, batch 254 (46200): accuracy: 0.8883, mnli_loss: 0.3089
09/05 06:00:49 PM: Update 46292: task mnli, batch 292 (46238): accuracy: 0.8886, mnli_loss: 0.3076
09/05 06:00:59 PM: Update 46331: task mnli, batch 331 (46277): accuracy: 0.8876, mnli_loss: 0.3064
09/05 06:01:10 PM: Update 46370: task mnli, batch 370 (46316): accuracy: 0.8881, mnli_loss: 0.3067
09/05 06:01:20 PM: Update 46408: task mnli, batch 408 (46354): accuracy: 0.8883, mnli_loss: 0.3069
09/05 06:01:30 PM: Update 46446: task mnli, batch 446 (46392): accuracy: 0.8848, mnli_loss: 0.3141
09/05 06:01:40 PM: Update 46483: task mnli, batch 483 (46429): accuracy: 0.8849, mnli_loss: 0.3151
09/05 06:01:50 PM: Update 46520: task mnli, batch 520 (46466): accuracy: 0.8849, mnli_loss: 0.3158
09/05 06:02:00 PM: Update 46559: task mnli, batch 559 (46505): accuracy: 0.8859, mnli_loss: 0.3121
09/05 06:02:10 PM: Update 46590: task mnli, batch 590 (46536): accuracy: 0.8843, mnli_loss: 0.3167
09/05 06:02:21 PM: Update 46629: task mnli, batch 629 (46575): accuracy: 0.8840, mnli_loss: 0.3179
09/05 06:02:31 PM: Update 46667: task mnli, batch 667 (46613): accuracy: 0.8831, mnli_loss: 0.3207
09/05 06:02:41 PM: Update 46703: task mnli, batch 703 (46649): accuracy: 0.8830, mnli_loss: 0.3218
09/05 06:02:51 PM: Update 46739: task mnli, batch 739 (46685): accuracy: 0.8828, mnli_loss: 0.3225
09/05 06:03:01 PM: Update 46778: task mnli, batch 778 (46724): accuracy: 0.8813, mnli_loss: 0.3258
09/05 06:03:11 PM: Update 46817: task mnli, batch 817 (46763): accuracy: 0.8803, mnli_loss: 0.3285
09/05 06:03:21 PM: Update 46858: task mnli, batch 858 (46804): accuracy: 0.8802, mnli_loss: 0.3292
09/05 06:03:31 PM: Update 46895: task mnli, batch 895 (46841): accuracy: 0.8795, mnli_loss: 0.3305
09/05 06:03:42 PM: Update 46934: task mnli, batch 934 (46880): accuracy: 0.8789, mnli_loss: 0.3330
09/05 06:03:52 PM: Update 46973: task mnli, batch 973 (46919): accuracy: 0.8790, mnli_loss: 0.3328
09/05 06:04:01 PM: ***** Step 47000 / Validation 47 *****
09/05 06:04:01 PM: copa: trained on 0 batches, 0.000 epochs
09/05 06:04:01 PM: mnli: trained on 1000 batches, 0.061 epochs
09/05 06:04:01 PM: Validating...
09/05 06:04:01 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8333, copa_loss: 0.3922
09/05 06:04:02 PM: Evaluate: task mnli, batch 5 (209): accuracy: 0.7583, mnli_loss: 0.5977
09/05 06:04:12 PM: Evaluate: task mnli, batch 142 (209): accuracy: 0.8190, mnli_loss: 0.5292
09/05 06:04:17 PM: Updating LR scheduler:
09/05 06:04:17 PM: 	Best result seen so far for macro_avg: 0.801
09/05 06:04:17 PM: 	# validation passes without improvement: 1
09/05 06:04:17 PM: copa_loss: training: 0.000000 validation: 0.599677
09/05 06:04:17 PM: mnli_loss: training: 0.332446 validation: 0.532990
09/05 06:04:17 PM: macro_avg: validation: 0.794000
09/05 06:04:17 PM: micro_avg: validation: 0.817059
09/05 06:04:17 PM: copa_accuracy: validation: 0.770000
09/05 06:04:17 PM: mnli_accuracy: training: 0.879296 validation: 0.818000
09/05 06:04:17 PM: Global learning rate: 1.25e-06
09/05 06:04:17 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 06:04:22 PM: Update 47016: task mnli, batch 16 (46962): accuracy: 0.8750, mnli_loss: 0.3527
09/05 06:04:32 PM: Update 47056: task mnli, batch 56 (47002): accuracy: 0.8832, mnli_loss: 0.3345
09/05 06:04:43 PM: Update 47095: task mnli, batch 95 (47041): accuracy: 0.8816, mnli_loss: 0.3323
09/05 06:04:53 PM: Update 47133: task mnli, batch 133 (47079): accuracy: 0.8797, mnli_loss: 0.3364
09/05 06:05:03 PM: Update 47172: task mnli, batch 172 (47118): accuracy: 0.8808, mnli_loss: 0.3300
09/05 06:05:13 PM: Update 47210: task mnli, batch 210 (47156): accuracy: 0.8772, mnli_loss: 0.3339
09/05 06:05:23 PM: Update 47250: task mnli, batch 250 (47196): accuracy: 0.8748, mnli_loss: 0.3407
09/05 06:05:34 PM: Update 47287: task mnli, batch 287 (47233): accuracy: 0.8722, mnli_loss: 0.3442
09/05 06:05:44 PM: Update 47325: task mnli, batch 325 (47271): accuracy: 0.8735, mnli_loss: 0.3395
09/05 06:05:54 PM: Update 47363: task mnli, batch 363 (47309): accuracy: 0.8728, mnli_loss: 0.3396
09/05 06:06:06 PM: Update 47402: task mnli, batch 402 (47348): accuracy: 0.8745, mnli_loss: 0.3351
09/05 06:06:17 PM: Update 47439: task mnli, batch 439 (47385): accuracy: 0.8734, mnli_loss: 0.3368
09/05 06:06:27 PM: Update 47478: task mnli, batch 478 (47424): accuracy: 0.8721, mnli_loss: 0.3398
09/05 06:06:37 PM: Update 47517: task mnli, batch 517 (47463): accuracy: 0.8725, mnli_loss: 0.3395
09/05 06:06:47 PM: Update 47555: task mnli, batch 555 (47501): accuracy: 0.8745, mnli_loss: 0.3362
09/05 06:06:57 PM: Update 47593: task mnli, batch 593 (47539): accuracy: 0.8737, mnli_loss: 0.3371
09/05 06:07:08 PM: Update 47632: task mnli, batch 632 (47578): accuracy: 0.8728, mnli_loss: 0.3389
09/05 06:07:18 PM: Update 47670: task mnli, batch 670 (47616): accuracy: 0.8729, mnli_loss: 0.3391
09/05 06:07:28 PM: Update 47708: task mnli, batch 708 (47654): accuracy: 0.8729, mnli_loss: 0.3392
09/05 06:07:38 PM: Update 47748: task mnli, batch 748 (47694): accuracy: 0.8730, mnli_loss: 0.3381
09/05 06:07:48 PM: Update 47787: task mnli, batch 787 (47733): accuracy: 0.8732, mnli_loss: 0.3374
09/05 06:07:58 PM: Update 47819: task mnli, batch 819 (47765): accuracy: 0.8734, mnli_loss: 0.3371
09/05 06:08:09 PM: Update 47857: task mnli, batch 857 (47803): accuracy: 0.8735, mnli_loss: 0.3367
09/05 06:08:19 PM: Update 47895: task mnli, batch 895 (47841): accuracy: 0.8734, mnli_loss: 0.3389
09/05 06:08:29 PM: Update 47933: task mnli, batch 933 (47879): accuracy: 0.8730, mnli_loss: 0.3396
09/05 06:08:39 PM: Update 47970: task mnli, batch 970 (47916): accuracy: 0.8732, mnli_loss: 0.3394
09/05 06:08:47 PM: ***** Step 48000 / Validation 48 *****
09/05 06:08:47 PM: copa: trained on 0 batches, 0.000 epochs
09/05 06:08:47 PM: mnli: trained on 1000 batches, 0.061 epochs
09/05 06:08:47 PM: Validating...
09/05 06:08:47 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8333, copa_loss: 0.3944
09/05 06:08:49 PM: Evaluate: task mnli, batch 19 (209): accuracy: 0.8136, mnli_loss: 0.5170
09/05 06:08:59 PM: Evaluate: task mnli, batch 156 (209): accuracy: 0.8197, mnli_loss: 0.5262
09/05 06:09:03 PM: Updating LR scheduler:
09/05 06:09:03 PM: 	Best result seen so far for macro_avg: 0.801
09/05 06:09:03 PM: 	# validation passes without improvement: 2
09/05 06:09:03 PM: copa_loss: training: 0.000000 validation: 0.599425
09/05 06:09:03 PM: mnli_loss: training: 0.339252 validation: 0.531696
09/05 06:09:03 PM: macro_avg: validation: 0.788900
09/05 06:09:03 PM: micro_avg: validation: 0.816667
09/05 06:09:03 PM: copa_accuracy: validation: 0.760000
09/05 06:09:03 PM: mnli_accuracy: training: 0.872874 validation: 0.817800
09/05 06:09:03 PM: Global learning rate: 1.25e-06
09/05 06:09:03 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 06:09:09 PM: Update 48020: task mnli, batch 20 (47966): accuracy: 0.8583, mnli_loss: 0.3450
09/05 06:09:19 PM: Update 48058: task mnli, batch 58 (48004): accuracy: 0.8642, mnli_loss: 0.3518
09/05 06:09:29 PM: Update 48098: task mnli, batch 98 (48044): accuracy: 0.8648, mnli_loss: 0.3510
09/05 06:09:39 PM: Update 48136: task mnli, batch 136 (48082): accuracy: 0.8658, mnli_loss: 0.3497
09/05 06:09:50 PM: Update 48175: task mnli, batch 175 (48121): accuracy: 0.8686, mnli_loss: 0.3483
09/05 06:10:00 PM: Update 48213: task mnli, batch 213 (48159): accuracy: 0.8713, mnli_loss: 0.3430
09/05 06:10:10 PM: Update 48246: task mnli, batch 246 (48192): accuracy: 0.8696, mnli_loss: 0.3459
09/05 06:10:20 PM: Update 48286: task mnli, batch 286 (48232): accuracy: 0.8676, mnli_loss: 0.3532
09/05 06:10:30 PM: Update 48323: task mnli, batch 323 (48269): accuracy: 0.8685, mnli_loss: 0.3525
09/05 06:10:40 PM: Update 48361: task mnli, batch 361 (48307): accuracy: 0.8684, mnli_loss: 0.3504
09/05 06:10:50 PM: Update 48400: task mnli, batch 400 (48346): accuracy: 0.8688, mnli_loss: 0.3492
09/05 06:11:00 PM: Update 48439: task mnli, batch 439 (48385): accuracy: 0.8695, mnli_loss: 0.3484
09/05 06:11:10 PM: Update 48477: task mnli, batch 477 (48423): accuracy: 0.8701, mnli_loss: 0.3476
09/05 06:11:20 PM: Update 48514: task mnli, batch 514 (48460): accuracy: 0.8705, mnli_loss: 0.3467
09/05 06:11:31 PM: Update 48551: task mnli, batch 551 (48497): accuracy: 0.8702, mnli_loss: 0.3475
09/05 06:11:41 PM: Update 48589: task mnli, batch 589 (48535): accuracy: 0.8725, mnli_loss: 0.3428
09/05 06:11:51 PM: Update 48627: task mnli, batch 627 (48573): accuracy: 0.8724, mnli_loss: 0.3427
09/05 06:12:01 PM: Update 48658: task mnli, batch 658 (48604): accuracy: 0.8722, mnli_loss: 0.3414
09/05 06:12:11 PM: Update 48697: task mnli, batch 697 (48643): accuracy: 0.8725, mnli_loss: 0.3411
09/05 06:12:21 PM: Update 48737: task mnli, batch 737 (48683): accuracy: 0.8722, mnli_loss: 0.3417
09/05 06:12:31 PM: Update 48774: task mnli, batch 774 (48720): accuracy: 0.8721, mnli_loss: 0.3427
09/05 06:12:41 PM: Update 48811: task mnli, batch 811 (48757): accuracy: 0.8722, mnli_loss: 0.3444
09/05 06:12:52 PM: Update 48848: task mnli, batch 848 (48794): accuracy: 0.8720, mnli_loss: 0.3463
09/05 06:13:02 PM: Update 48888: task mnli, batch 888 (48834): accuracy: 0.8714, mnli_loss: 0.3483
09/05 06:13:12 PM: Update 48928: task mnli, batch 928 (48874): accuracy: 0.8723, mnli_loss: 0.3458
09/05 06:13:22 PM: Update 48966: task mnli, batch 966 (48912): accuracy: 0.8712, mnli_loss: 0.3476
09/05 06:13:31 PM: ***** Step 49000 / Validation 49 *****
09/05 06:13:31 PM: copa: trained on 0 batches, 0.000 epochs
09/05 06:13:31 PM: mnli: trained on 1000 batches, 0.061 epochs
09/05 06:13:31 PM: Validating...
09/05 06:13:31 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8333, copa_loss: 0.3957
09/05 06:13:32 PM: Evaluate: task mnli, batch 9 (209): accuracy: 0.8009, mnli_loss: 0.5431
09/05 06:13:42 PM: Evaluate: task mnli, batch 145 (209): accuracy: 0.8207, mnli_loss: 0.5197
09/05 06:13:47 PM: Updating LR scheduler:
09/05 06:13:47 PM: 	Best result seen so far for macro_avg: 0.801
09/05 06:13:47 PM: 	# validation passes without improvement: 3
09/05 06:13:47 PM: copa_loss: training: 0.000000 validation: 0.599098
09/05 06:13:47 PM: mnli_loss: training: 0.346625 validation: 0.527009
09/05 06:13:47 PM: macro_avg: validation: 0.794000
09/05 06:13:47 PM: micro_avg: validation: 0.817059
09/05 06:13:47 PM: copa_accuracy: validation: 0.770000
09/05 06:13:47 PM: mnli_accuracy: training: 0.871373 validation: 0.818000
09/05 06:13:47 PM: Global learning rate: 1.25e-06
09/05 06:13:47 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 06:13:52 PM: Update 49018: task mnli, batch 18 (48964): accuracy: 0.8727, mnli_loss: 0.2989
09/05 06:14:02 PM: Update 49056: task mnli, batch 56 (49002): accuracy: 0.8676, mnli_loss: 0.3094
09/05 06:14:13 PM: Update 49088: task mnli, batch 88 (49034): accuracy: 0.8625, mnli_loss: 0.3260
09/05 06:14:23 PM: Update 49127: task mnli, batch 127 (49073): accuracy: 0.8611, mnli_loss: 0.3395
09/05 06:14:33 PM: Update 49166: task mnli, batch 166 (49112): accuracy: 0.8644, mnli_loss: 0.3328
09/05 06:14:43 PM: Update 49198: task mnli, batch 198 (49144): accuracy: 0.8638, mnli_loss: 0.3376
09/05 06:14:53 PM: Update 49236: task mnli, batch 236 (49182): accuracy: 0.8654, mnli_loss: 0.3376
09/05 06:15:04 PM: Update 49276: task mnli, batch 276 (49222): accuracy: 0.8653, mnli_loss: 0.3392
09/05 06:15:14 PM: Update 49316: task mnli, batch 316 (49262): accuracy: 0.8681, mnli_loss: 0.3340
09/05 06:15:24 PM: Update 49355: task mnli, batch 355 (49301): accuracy: 0.8692, mnli_loss: 0.3349
09/05 06:15:34 PM: Update 49393: task mnli, batch 393 (49339): accuracy: 0.8717, mnli_loss: 0.3313
09/05 06:15:44 PM: Update 49431: task mnli, batch 431 (49377): accuracy: 0.8725, mnli_loss: 0.3303
09/05 06:15:54 PM: Update 49469: task mnli, batch 469 (49415): accuracy: 0.8732, mnli_loss: 0.3280
09/05 06:16:04 PM: Update 49507: task mnli, batch 507 (49453): accuracy: 0.8728, mnli_loss: 0.3291
09/05 06:16:14 PM: Update 49545: task mnli, batch 545 (49491): accuracy: 0.8712, mnli_loss: 0.3317
09/05 06:16:24 PM: Update 49582: task mnli, batch 582 (49528): accuracy: 0.8713, mnli_loss: 0.3352
09/05 06:16:35 PM: Update 49613: task mnli, batch 613 (49559): accuracy: 0.8719, mnli_loss: 0.3354
09/05 06:16:45 PM: Update 49654: task mnli, batch 654 (49600): accuracy: 0.8728, mnli_loss: 0.3334
09/05 06:16:55 PM: Update 49692: task mnli, batch 692 (49638): accuracy: 0.8729, mnli_loss: 0.3331
09/05 06:17:05 PM: Update 49732: task mnli, batch 732 (49678): accuracy: 0.8732, mnli_loss: 0.3322
09/05 06:17:15 PM: Update 49769: task mnli, batch 769 (49715): accuracy: 0.8730, mnli_loss: 0.3328
09/05 06:17:25 PM: Update 49807: task mnli, batch 807 (49753): accuracy: 0.8727, mnli_loss: 0.3326
09/05 06:17:35 PM: Update 49844: task mnli, batch 844 (49790): accuracy: 0.8726, mnli_loss: 0.3320
09/05 06:17:45 PM: Update 49882: task mnli, batch 882 (49828): accuracy: 0.8733, mnli_loss: 0.3311
09/05 06:17:55 PM: Update 49921: task mnli, batch 921 (49867): accuracy: 0.8736, mnli_loss: 0.3313
09/05 06:18:05 PM: Update 49960: task mnli, batch 960 (49906): accuracy: 0.8730, mnli_loss: 0.3322
09/05 06:18:16 PM: Update 49999: task mnli, batch 999 (49945): accuracy: 0.8735, mnli_loss: 0.3320
09/05 06:18:16 PM: ***** Step 50000 / Validation 50 *****
09/05 06:18:16 PM: copa: trained on 0 batches, 0.000 epochs
09/05 06:18:16 PM: mnli: trained on 1000 batches, 0.061 epochs
09/05 06:18:16 PM: Validating...
09/05 06:18:16 PM: Evaluate: task copa, batch 1 (5): accuracy: 0.8333, copa_loss: 0.3948
09/05 06:18:26 PM: Evaluate: task mnli, batch 119 (209): accuracy: 0.8200, mnli_loss: 0.5158
09/05 06:18:32 PM: Updating LR scheduler:
09/05 06:18:32 PM: 	Best result seen so far for macro_avg: 0.801
09/05 06:18:32 PM: 	# validation passes without improvement: 4
09/05 06:18:32 PM: Reached max_epochs limit on all tasks. Stopping training.
09/05 06:18:32 PM: copa_loss: training: 0.000000 validation: 0.598777
09/05 06:18:32 PM: mnli_loss: training: 0.331854 validation: 0.525381
09/05 06:18:32 PM: macro_avg: validation: 0.794500
09/05 06:18:32 PM: micro_avg: validation: 0.818039
09/05 06:18:32 PM: copa_accuracy: validation: 0.770000
09/05 06:18:32 PM: mnli_accuracy: training: 0.873571 validation: 0.819000
09/05 06:18:32 PM: Global learning rate: 1.25e-06
09/05 06:18:32 PM: Saving checkpoints to: diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 06:18:33 PM: Stopped training after 50 validation checks
09/05 06:18:33 PM: Trained copa for 54 batches or 3.176 epochs
09/05 06:18:33 PM: Trained mnli for 49946 batches or 3.052 epochs
09/05 06:18:33 PM: ***** VALIDATION RESULTS *****
09/05 06:18:33 PM: copa_accuracy (for best val pass 25): copa_loss: 0.60432, mnli_loss: 0.48028, macro_avg: 0.79970, micro_avg: 0.81863, copa_accuracy: 0.78000, mnli_accuracy: 0.81940
09/05 06:18:33 PM: mnli_accuracy (for best val pass 41): copa_loss: 0.59850, mnli_loss: 0.51859, macro_avg: 0.80150, micro_avg: 0.82216, copa_accuracy: 0.78000, mnli_accuracy: 0.82300
09/05 06:18:33 PM: micro_avg (for best val pass 41): copa_loss: 0.59850, mnli_loss: 0.51859, macro_avg: 0.80150, micro_avg: 0.82216, copa_accuracy: 0.78000, mnli_accuracy: 0.82300
09/05 06:18:33 PM: macro_avg (for best val pass 41): copa_loss: 0.59850, mnli_loss: 0.51859, macro_avg: 0.80150, micro_avg: 0.82216, copa_accuracy: 0.78000, mnli_accuracy: 0.82300
09/05 06:18:33 PM: Evaluating...
09/05 06:18:34 PM: Loaded model state from diagnostic_run_2/my-experiment/mnli_copa_diagnostic/model_state_pretrain_val_41.best.th
09/05 06:18:34 PM: Evaluating on: glue-diagnostic, split: val
09/05 06:18:39 PM: Task 'glue-diagnostic': sorting predictions by 'idx'
09/05 06:18:39 PM: Finished evaluating on: glue-diagnostic
09/05 06:18:39 PM: Wrote predictions for task: glue-diagnostic
09/05 06:18:39 PM: Task 'glue-diagnostic': Wrote predictions to diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 06:18:39 PM: Wrote all preds for split 'val' to diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 06:18:39 PM: Evaluating on: glue-diagnostic, split: test
09/05 06:18:43 PM: Task 'glue-diagnostic': sorting predictions by 'idx'
09/05 06:18:43 PM: Finished evaluating on: glue-diagnostic
09/05 06:18:43 PM: Wrote predictions for task: glue-diagnostic
09/05 06:18:43 PM: Task 'glue-diagnostic': Wrote predictions to diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 06:18:43 PM: Wrote all preds for split 'test' to diagnostic_run_2/my-experiment/mnli_copa_diagnostic
09/05 06:18:43 PM: Writing results for split 'val' to diagnostic_run_2/my-experiment/results.tsv
09/05 06:18:43 PM: micro_avg: 0.000, macro_avg: 0.000, glue-diagnostic_lex_sem: 0.385, glue-diagnostic_lex_sem__Factivity;Quantifiers: 0.000, glue-diagnostic_lex_sem__Factivity: 0.165, glue-diagnostic_lex_sem__Quantifiers: 0.586, glue-diagnostic_lex_sem__Named entities: 0.217, glue-diagnostic_lex_sem__Lexical entailment;Factivity: 0.000, glue-diagnostic_lex_sem__Symmetry/Collectivity: 0.000, glue-diagnostic_lex_sem__Redundancy: 0.677, glue-diagnostic_lex_sem__Morphological negation: 0.365, glue-diagnostic_lex_sem__Lexical entailment: 0.328, glue-diagnostic_lex_sem__Lexical entailment;Quantifiers: 1.000, glue-diagnostic_pr_ar_str: 0.429, glue-diagnostic_pr_ar_str__Active/Passive: 0.365, glue-diagnostic_pr_ar_str__Nominalization: 0.307, glue-diagnostic_pr_ar_str__Relative clauses;Anaphora/Coreference: 1.000, glue-diagnostic_pr_ar_str__Restrictivity: -0.250, glue-diagnostic_pr_ar_str__Coordination scope;Prepositional phrases: 0.333, glue-diagnostic_pr_ar_str__Ellipsis/Implicits;Anaphora/Coreference: 0.671, glue-diagnostic_pr_ar_str__Anaphora/Coreference: 0.306, glue-diagnostic_pr_ar_str__Core args;Anaphora/Coreference: 0.447, glue-diagnostic_pr_ar_str__Active/Passive;Prepositional phrases: 1.000, glue-diagnostic_pr_ar_str__Coordination scope: 0.433, glue-diagnostic_pr_ar_str__Nominalization;Genitives/Partitives: 0.000, glue-diagnostic_pr_ar_str__Intersectivity: 0.211, glue-diagnostic_pr_ar_str__Genitives/Partitives: 0.685, glue-diagnostic_pr_ar_str__Intersectivity;Ellipsis/Implicits: 0.000, glue-diagnostic_pr_ar_str__Datives: 0.673, glue-diagnostic_pr_ar_str__Restrictivity;Relative clauses: -1.000, glue-diagnostic_pr_ar_str__Anaphora/Coreference;Prepositional phrases: 1.000, glue-diagnostic_pr_ar_str__Relative clauses;Restrictivity: 0.577, glue-diagnostic_pr_ar_str__Core args: 0.415, glue-diagnostic_pr_ar_str__Restrictivity;Anaphora/Coreference: 0.000, glue-diagnostic_pr_ar_str__Ellipsis/Implicits: 0.442, glue-diagnostic_pr_ar_str__Relative clauses: 0.331, glue-diagnostic_pr_ar_str__Prepositional phrases: 0.707, glue-diagnostic_logic: 0.215, glue-diagnostic_logic__Existential;Negation: 0.000, glue-diagnostic_logic__Existential;Upward monotone: 1.000, glue-diagnostic_logic__Universal;Negation: 0.000, glue-diagnostic_logic__Downward monotone;Conditionals: -1.000, glue-diagnostic_logic__Negation: 0.177, glue-diagnostic_logic__Intervals/Numbers: -0.221, glue-diagnostic_logic__Disjunction: -0.337, glue-diagnostic_logic__Universal: 0.703, glue-diagnostic_logic__Temporal;Intervals/Numbers: 0.000, glue-diagnostic_logic__Disjunction;Conjunction: 0.000, glue-diagnostic_logic__Conjunction;Upward monotone: 1.000, glue-diagnostic_logic__Temporal: 0.178, glue-diagnostic_logic__Upward monotone: 0.330, glue-diagnostic_logic__Disjunction;Negation: -0.316, glue-diagnostic_logic__Non-monotone: 0.059, glue-diagnostic_logic__Universal;Conjunction: 0.000, glue-diagnostic_logic__Conjunction;Negation: 0.632, glue-diagnostic_logic__Conjunction: 0.354, glue-diagnostic_logic__Existential: 0.400, glue-diagnostic_logic__Downward monotone: -0.544, glue-diagnostic_logic__Disjunction;Conditionals;Negation: 0.289, glue-diagnostic_logic__Double negation;Negation: 0.000, glue-diagnostic_logic__Downward monotone;Existential;Negation: -1.000, glue-diagnostic_logic__Conditionals: 0.247, glue-diagnostic_logic__Disjunction;Non-monotone: 0.000, glue-diagnostic_logic__Double negation: 0.313, glue-diagnostic_logic__Negation;Conditionals: 0.000, glue-diagnostic_logic__Intervals/Numbers;Non-monotone: 0.707, glue-diagnostic_logic__Temporal;Conjunction: 0.000, glue-diagnostic_knowledge: 0.183, glue-diagnostic_knowledge__Common sense: 0.199, glue-diagnostic_knowledge__World knowledge: 0.161, glue-diagnostic_all_mcc: 0.338, glue-diagnostic_accuracy: 0.572
09/05 06:18:43 PM: Done!
